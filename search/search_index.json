{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Medical Artificial Intelligence (AI) has the potential to revolutionize healthcare by advancing evidence-based medicine, personalizing patient care, and reducing costs. Unlocking this potential requires reliable methods for evaluating the efficacy of medical machine learning (ML) models on large-scale heterogeneous data while maintaining patient privacy.</p>"},{"location":"#what-is-medperf","title":"What is MedPerf?","text":"<p>MedPerf is an open-source framework for benchmarking ML models to deliver clinical efficacy while prioritizing patient privacy and mitigating legal and regulatory risks. It enables federated evaluation in which models are securely distributed to different facilities for evaluation. The goal of federated evaluation is to make it simple and reliable to share models with many data providers, evaluate those models against their data in controlled settings, then aggregate and analyze the findings.</p> <p>The MedPerf approach empowers healthcare organizations to assess and verify the performance of ML models in an efficient and human-supervised process without sharing any patient data across facilities during the process.</p>"},{"location":"#why-medperf","title":"Why MedPerf?","text":"<p>MedPerf reduces the risks and costs associated with data sharing, maximizing medical and patient outcomes. The platform leads to an effective, broader, and cost-effective adoption of medical ML and improves patient outcomes.</p> <p>Anyone who joins our platform can get several benefits, regardless of the role they will assume.</p> <p>Benefits if you are a Data Provider:</p> <ul> <li>Evaluate how well machine learning models perform on your population\u2019s data.</li> <li>Connect to Model Owners to help them improve medical ML in a specific domain.</li> <li>Help define impactful medical ML benchmarks.</li> </ul> <p>Benefits if you are a Model Owner:</p> <ul> <li>Measure model performance on private datasets that you would never have access to.</li> <li>Connect to specific Data Providers that can help you increase the performance of your model.</li> </ul> <p>Benefits if you own a benchmark (Benchmark Committee):</p> <ul> <li>Hold a leading role in the MedPerf ecosystem by defining specifications of a benchmark for a particular medical ML task.</li> <li>Get help to create a strong community around a specific area.</li> <li>Rule point on creating the guidelines to generate impactful ML for a specific area.</li> <li>Help improve best practices in your area of interest.</li> <li>Ensure the choice of the metrics as well as the proper reference implementations.</li> </ul> <p>Benefits to the Broad Community:</p> <ul> <li>Provide consistent and rigorous approaches for evaluating the accuracy of ML models for real-world use in a standardized manner.</li> <li>Enable model usability measurement across institutions while maintaining data privacy and model reliability.</li> <li>Connect with a community of expert groups to employ scientific evaluation methodologies and technical approaches to operate benchmarks that not only have well-defined clinical aspects, such as clinical impact, clinical workflow integration and patient outcome, but also support robust technical aspects, including proper metrics, data preprocessing and reference implementation.</li> </ul>"},{"location":"#what-is-a-benchmark-in-the-medperf-perspective","title":"What is a benchmark in the MedPerf perspective?","text":"<p>A benchmark is a collection of assets used by the platform to test the performance of ML models for a specific clinical problem. The primary components of a benchmark are:</p> <ol> <li>Specifications: precise definition of the clinical setting (e.g., problem or task and specific patient population) on which trained ML models are to be evaluated. It also includes the labeling (annotation) methodology as well as the choice of evaluation metrics.</li> <li>Dataset Preparation: a process that prepares datasets for use in evaluation, and can also test the prepared datasets for quality and compatibility. This is implemented as an MLCube (see Data Preparator MLCube).</li> <li>Registered Datasets: a list of registered datasets prepared according to the benchmark criteria and approved for evaluation use by their owners (e.g. patient data from multiple facilities representing (as a whole) a diverse patient population).</li> <li>Evaluation: a consistent implementation of the testing pipelines and evaluation metrics.</li> <li>Reference Implementation: a detailed example of a benchmark submission consisting of example model code, the evaluation component, and de-identified or synthetic publicly available sample data.</li> <li>Registered Models: a list of registered models to run in this benchmark.</li> <li>Documentation: documents for understanding and using the benchmark.</li> </ol>"},{"location":"cli_reference/","title":"In Progress","text":""},{"location":"medperf_components/","title":"MedPerf Components","text":""},{"location":"medperf_components/#medperf-server","title":"MedPerf Server","text":"<p>The server contains all the metadata necessary to coordinate and execute experiments. No code assets or datasets are stored on the server.</p> <p>The backend server is implemented in Django, and it can be found in the server folder in the MedPerf Github repository.</p>"},{"location":"medperf_components/#medperf-client","title":"MedPerf Client","text":"<p>The MedPerf client contains all the necessary tools to interact with the server, preparing datasets for benchmarks and running experiments on the local machine. It can be found in this folder in the MedPerf Github repository.</p> <p>The client communicates to the server through the API to, for example, authenticate a user, retrieve benchmarks/MLcubes and send results.</p> <p>The client is currently available to the user through a command-line interface (CLI). See the CLI reference.</p>"},{"location":"medperf_components/#auth-provider","title":"Auth Provider","text":"<p>The auth provider manages MedPerf users identities, authentication, and authorization to access the MedPerf server. Users will authenticate with the auth provider and authorize their MedPerf client to access the MedPerf server. Upon authorization, the MedPerf client will use access tokens issued by the auth provider in every request to the MedPerf server. The MedPerf server is configured to processes only requests authorized by the auth provider.</p> <p>Currently, MedPerf uses Auth0 as the auth provider.</p>"},{"location":"roles/","title":"User Roles and Responsibilities","text":"<p>There are different types of users/roles that can be created in MedPerf:</p>"},{"location":"roles/#benchmark-committee","title":"Benchmark Committee","text":"<p>Include regulatory bodies, groups of experts (e.g., clinicians, patient representative groups), and data or Model Owners wishing to drive the evaluation of their model or data. Benchmark Committee do not have admin privileges on the system, but they have elevated permissions regarding the benchmark they define, such as deciding which model and Data Providers will participate.</p> <p></p>"},{"location":"roles/#data-providers","title":"Data Providers","text":"<p>Include hospitals, medical practices, research organizations, and healthcare insurance providers that own medical data, register medical data, and execute benchmark requests.</p> <p></p>"},{"location":"roles/#model-owners","title":"Model Owners","text":"<p>Include ML researchers and software vendors that own a trained medical ML model and want to evaluate its performance.</p> <p></p>"},{"location":"roles/#platform-providers","title":"Platform Providers","text":"<p>Organizations like MLCommons that operate a platform that enables a benchmark committee to run benchmarks by connecting Data Providers with Model Owners.</p> <p></p>"},{"location":"workflow/","title":"MedPerf Benchmarking Workflow","text":""},{"location":"workflow/#creating-a-user","title":"Creating a User","text":"<p>Currently, the MedPerf administration is the only one able to create users, controlling access to the system and permissions to own a benchmark. For example, if a hospital (Data Provider), a model owner, or a benchmark committee wants to have access to MedPerf, they need to contact the MedPerf administrator to add a user.</p>"},{"location":"workflow/#establishing-a-benchmark-committee","title":"Establishing a Benchmark Committee","text":"<p>The benchmarking process starts with establishing a benchmark committee of healthcare stakeholders (experts, committee), which will identify a clinical problem where an effective ML-based solution can have a significant clinical impact.</p>"},{"location":"workflow/#recruiting-data-and-model-owners","title":"Recruiting Data and Model Owners","text":"<p>The benchmark committee recruits Data Providers and Model Owners either by inviting trusted parties or by making an open call for participation. A higher number of dataset providers recruited can maximize diversity on a global scale.</p>"},{"location":"workflow/#benchmark-submission","title":"Benchmark Submission","text":"<p>MLCubes are the building blocks of an experiment and are required in order to create a benchmark. Three MLCubes (Data Preparator MLCube, Reference Model MLCube, and Metrics MLCube) need to be submitted. After submitting the three MLCubes, alongside with a sample reference dataset, the Benchmark Committee is capable of creating a benchmark. Once the benchmark is submitted, the Medperf admin must approve it before it can be seen by other users. Follow our Hands-on Tutorial for detailed step-by-step guidelines.</p>"},{"location":"workflow/#submitting-and-associating-additional-models","title":"Submitting and Associating Additional Models","text":"<p>Once a benchmark is submitted by the Benchmark Committee, any user can submit their own Model MLCubes and request an association with the benchmark. This association request executes the benchmark locally with the given model on the benchmark's reference dataset to ensure workflow validity and compatibility. If the model successfully passes the compatibility test, and its association is approved by the Benchmark Committee, it becomes a part of the benchmark.</p> <p></p>"},{"location":"workflow/#dataset-preparation-and-association","title":"Dataset Preparation and Association","text":"<p>Data Providers that want to be part of the benchmark can prepare their own datasets, register them, and associate them with the benchmark. A dataset will be prepared using the benchmark's Data Preparator MLCube. Then, the prepared dataset's metadata is registered within the MedPerf server.</p> <p></p> <p>The data provider then can request to participate in the benchmark with their dataset. Requesting the association will run the benchmark's reference workflow to assure the compatibility of the prepared dataset structure with the workflow. Once the association request is approved by the Benchmark Committee, then the dataset becomes a part of the benchmark.</p> <p></p>"},{"location":"workflow/#executing-the-benchmark","title":"Executing the Benchmark","text":"<p>The Benchmark Committee may notify Data Providers that models are available for benchmarking. Data Providers can then run the benchmark models locally on their data.</p> <p>This procedure retrieves the model MLCubes associated with the benchmark and runs them on the indicated prepared dataset to generate predictions. The Metrics MLCube of the benchmark is then retrieved to evaluate the predictions. Once the evaluation results are generated, the data provider can submit them to the platform.</p> <p></p>"},{"location":"workflow/#release-results-to-participants","title":"Release Results to Participants","text":"<p>The benchmarking platform aggregates the results of running the models against the datasets and shares them according to the Benchmark Committee's policy.</p> <p>The sharing policy controls how much of the data is shared, ranging from a single aggregated metric to a more detailed model-data cross product. A public leaderboard is available to Model Owners who produce the best performances.</p>"},{"location":"concepts/associations/","title":"In Progress","text":""},{"location":"concepts/auth/","title":"Authentication","text":"<p>This guide helps you learn how to login and logout using the MedPerf client to access the main production MedPerf server. MedPerf uses passwordless authentication. This means that login will only require you to access your email in order complete the login process.</p>"},{"location":"concepts/auth/#login","title":"Login","text":"<p>Follow the steps below to login:</p> <ul> <li>Step1 Run the following command:</li> </ul> <pre><code>medperf auth login\n</code></pre> <p>You will be prompted to enter your email address.</p> <p>After entering your email address, you will be provided with a verification URL and a code. A text similar to the following will be printed in your terminal:</p> <p></p> <p>Tip</p> <p>If you are running the MedPerf client on a machine with no graphical interface, you can use the link on any other device, e.g. your cellphone. Make sure that you trust that device.</p> <ul> <li>Step2 Open the verification URL and confirm the code:</li> </ul> <p>Open the printed URL in your browser. You will be presented with a code and you will be asked to confirm if that code is the same one printed in your terminal.</p> <p></p> <ul> <li>Step3 After confirmation, you will be asked to enter your email address. Enter your email address and press \"Continue\". You will see the following screen:</li> </ul> <p></p> <ul> <li>Step4 Check your inbox. You should recieve an email similar to the following:</li> </ul> <p></p> <p>Enter the recieved code in the previous screen.</p> <ul> <li>Step5 If there is no problem with your account, the login will be successful and you will see a screen similar to the following:</li> </ul> <p></p>"},{"location":"concepts/auth/#logout","title":"Logout","text":"<p>To disconnect the MedPerf client, simply run the following command:</p> <pre><code>medperf auth logout\n</code></pre>"},{"location":"concepts/auth/#checking-the-authentication-status","title":"Checking the authentication status","text":"<p>Note that when you login, the MedPerf client will remember you as long as you are using the same <code>profile</code>. If you switch to another profile by running <code>medperf profile activate &lt;other-profile&gt;</code>, you may have to login again. If you switch back again to a profile where you previously logged in, your login state will be restored. Read more about profiles here.</p> <p>You can always check the current login status by the running the following command:</p> <pre><code>medperf auth status\n</code></pre>"},{"location":"concepts/hosting_files/","title":"Hosting Files","text":"<p>MedPerf requires some files to be hosted on the cloud when running machine learning pipelines. Submitting MLCubes to the MedPerf server means submitting their metadata, and not, for example, model weights or parameters files. MLCube files such as model weights need to be hosted on the cloud, and the submitted MLCube metadata will only contain URLs (or certain identifiers) for these files. Another example would be benchmark submission, where demo datasets need to be hosted.</p> <p>The MedPerf client expects files to be hosted in certain ways. Below are options of how files can be hosted and how MedPerf identitfies them (e.g. a URL).</p>"},{"location":"concepts/hosting_files/#file-hosting","title":"File hosting","text":"<p>This can be done with any cloud hosting tool/provider you desire (such as GCP, AWS, Dropbox, Google Drive, Github). As long as your file can be accessed through a direct download link, it should work with medperf. Generating a direct download link for your hosted file can be straight-forward when using some providers (e.g. Amazon Web Services, Google Cloud Platform, Microsoft Azure) and can be a bit tricky when using others (e.g. Dropbox, GitHub, Google Drive).</p> <p>Note</p> <p>Direct download links must be permanent</p> <p>Tip</p> <p>You can make sure if a URL is a direct download link or not using tools like <code>wget</code> or <code>curl</code>. Running <code>wget &lt;URL&gt;</code> will download the file if the URL is a direct download link. Running <code>wget &lt;URL&gt;</code> may fail or may download an HTML page if the URL is not a direct download link.</p> <p>When your file is hosted with a direct download link, MedPerf will be able to identify this file using that direct download link. So for example, when you are submitting an MLCube, you would pass your hosted MLCube manifest file as follows:</p> <pre><code>--mlcube-file &lt;the-direct-download-link-to-the-file&gt;\n</code></pre> <p>Warning</p> <p>Files in this case are supposed to have anonymous public read access permission.</p>"},{"location":"concepts/hosting_files/#direct-download-links-of-files-on-github","title":"Direct download links of files on GitHub","text":"<p>It was a common practice by the current MedPerf users to host files on GitHub. You can learn below how to find the direct download link of a file hosted on GitHub. You can check online for other storage providers.</p> <p>It's important though to make sure the files won't be modified after being submitted to medperf, which could happen due to future commits. Because of this, the URLs of the files hosted on GitHub must contain a reference to the current commit hash. Below are the steps to get this URL for a specific file:</p> <ol> <li>Open the GitHub repository and ensure you are in the correct branch</li> <li>Click on \u201cCommits\u201d at the right top corner of the repository explorer.</li> <li>Locate the latest commit, it is the top most commit.</li> <li>If you are targeting previous versions of your file, make sure to consider the right commit.</li> <li>Click on this button \u201c&lt;&gt;\u201d corresponding to the commit (Browse the repository at this point in the history).</li> <li>Navigate to the file of interest.</li> <li>Click on \u201cRaw\u201d.</li> <li>Copy the url from your browser. It should be a UserContent GitHub URLs (domain raw.githubusercontent.com).</li> </ol>"},{"location":"concepts/hosting_files/#synapse-hosting","title":"Synapse hosting","text":"<p>You can choose the option of hosting with Synapse in cases where privacy is a concern. Please refer to this link for hosting files on the Synapse platform.</p> <p>When your file is hosted on Synapse, MedPerf will be able to identify this file using the Synapse ID corresponding to that file. So for example, when you are submitting an MLCube, you would pass your hosted MLCube manifest file as follows (note the prefix):</p> <pre><code>--mlcube-file synapse:&lt;the-synapse-id-of-the-file&gt;\n</code></pre> <p>Note that you need to authenticate with your Synapse credentials if you plan to use a Synaspe file with MedPerf. To do so run <code>medperf auth synapse_login</code>.</p> <p>Note</p> <p>You must authenticate if using files on Synapse. If this is not necessary, this means the file has anonymous public access read permission. In this case, Synapse allows you to generate a permanent direct download link for your file and you can follow the previous section.</p>"},{"location":"concepts/mlcube_files/","title":"MLCube Components: What to Host?","text":"<p>Once you have built an MLCube ready for MedPerf, you need to host it somewhere on the cloud so that it can be identified and retrieved by the MedPerf client on other machines. This requires hosting the MLCube components somewhere on the cloud. The following is a description of what needs to be hosted.</p>"},{"location":"concepts/mlcube_files/#hosting-your-container-image","title":"Hosting Your Container Image","text":"<p>MLCubes execute a container image behind the scenes. This container image is usually hosted on a container registry, like Docker Hub. In cases where this is not possible, medperf provides the option of passing the image file directly (i.e. having the image file hosted somewhere and providing MedPerf with the download link). MLCubes that work with images outside of the docker registry usually store the image inside the <code>&lt;path_to_mlcube&gt;/workspace/.image</code> folder. MedPerf supports using direct container image files for Singularity only.</p> <p>Note</p> <p>While there is the option of hosting the singularity image directly, it is highly recommended to use a container registry for accessability and usability purposes. MLCube also has mechanisms for converting containers to other runners, like Docker to Singularity.</p> <p>Note</p> <p>Docker Images can be on any docker container registry, not necessarily on Docker Hub.</p>"},{"location":"concepts/mlcube_files/#files-to-be-hosted","title":"Files to be hosted","text":"<p>The following is the list of files that must be hosted separately so they can be used by MedPerf:</p>"},{"location":"concepts/mlcube_files/#mlcubeyaml","title":"<code>mlcube.yaml</code>","text":"<p>Every MLCube is defined by its <code>mlcube.yaml</code> manifest file. As such, Medperf needs to have access to this file to recreate the MLCube. This file can be found inside your MLCube at <code>&lt;path_to_mlcube&gt;/mlcube.yaml</code>.</p>"},{"location":"concepts/mlcube_files/#parametersyaml-optional","title":"<code>parameters.yaml</code> (Optional)","text":"<p>The <code>parameters.yaml</code> file specify additional ways to parametrize your model MLCube using the same container image it is built with. This file can be found inside your MLCube at <code>&lt;path_to_mlcube&gt;/workspace/parameters.yaml</code>.</p>"},{"location":"concepts/mlcube_files/#additional_filestargz-optional","title":"<code>additional_files.tar.gz</code> (Optional)","text":"<p>MLCubes may require additional files that may be desired to keep separate from the model architecture and hosted image. For example, model weights. This allows for testing multiple implementations of the same model, without requiring a separate container image for each. If additional images are being used by your MLCube, they need to be compressed into a <code>.tar.gz</code> file and hosted separately. You can create this tarball file with the following command</p> <pre><code>tar -czf additional_files.tar.gz -C &lt;path_to_mlcube&gt;/workspace/additional_files .\n</code></pre>"},{"location":"concepts/mlcube_files/#preparing-an-mlcube-for-hosting","title":"Preparing an MLCube for hosting","text":"<p>To facilitate hosting and interface compatibility validation, MedPerf provides a script that finds all the required assets, compresses them if necessary, and places them in a single location for easy access. To run the script, make sure you have medperf installed and you are in medperf's root directory:</p> <pre><code>python scripts/package-mlcube.py \\\n--mlcube path/to/mlcube \\\n--mlcube-types &lt;list-of-comma-separated-strings&gt; \\\n--output path/to/file.tar.gz\n</code></pre> <p>where:</p> <ul> <li><code>path/to/mlcube</code> is the path to the MLCube folder containing the manifest file (<code>mlcube.yaml</code>)</li> <li><code>--mlcube-types</code> specifies a comma-separated list of MLCube types ('data-preparator' for a data preparation MLCube, 'model' for a model MLCube, and 'metrics' for a metrics MLCube.)</li> <li><code>path/to/file.tar.gz</code> is a path to the output file where you want to store the compressed version of all assets.</li> </ul> <p>See <code>python scripts/package-mlcube.py --help</code> for more information.</p> <p>Once executed, you should be able to find all prepared assets at <code>./mlcube/assets</code>, as well as a compressed version of the <code>assets</code> folder at the output path provided.</p> <p>Note</p> <p>The <code>--output</code> parameter is optional. The compressed version of the <code>assets</code> folder can be useful in cases where you don't directly interact with the MedPerf server, but instead you do so through a third party. This is usually the case for challenges and competitions.</p>"},{"location":"concepts/mlcube_files/#see-also","title":"See Also","text":"<ul> <li>File Hosting</li> </ul>"},{"location":"concepts/priorities/","title":"In Progress","text":""},{"location":"concepts/profiles/","title":"In Progress","text":""},{"location":"concepts/single_run/","title":"In Progress","text":""},{"location":"getting_started/benchmark_owner_demo/","title":"Hands-on Tutorial for Bechmark Committee","text":""},{"location":"getting_started/benchmark_owner_demo/#overview","title":"Overview","text":"<p>In this guide, you will learn how a user can use MedPerf to create a benchmark. The key tasks can be summarized as follows:</p> <ol> <li>Implement a valid workflow.</li> <li>Develop a demo dataset.</li> <li>Test your workflow.</li> <li>Submitting the MLCubes to the MedPerf server.</li> <li>Host the demo dataset.</li> <li>Submit the benchmark to the MedPerf server.</li> </ol> <p>It's assumed that you have already set up the general testing environment as explained in the setup guide.</p>"},{"location":"getting_started/benchmark_owner_demo/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/benchmark_owner_demo/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo benchmark\ncd ..\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_benchmark_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded.</p>"},{"location":"getting_started/benchmark_owner_demo/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should not do anything.</p> <p>You are now ready to start!</p>"},{"location":"getting_started/benchmark_owner_demo/#1-implement-a-valid-workflow","title":"1. Implement a Valid Workflow","text":"<p>The implementation of a valid workflow is accomplished by implementing three MLCubes:</p> <ol> <li> <p>Data Preparator MLCube: This MLCube will transform raw data into a dataset ready for the AI model execution. All data owners willing to participate in this benchmark will have their data prepared using this MLCube. A guide on how to implement data preparation MLCubes can be found here.</p> </li> <li> <p>Reference Model MLCube: This MLCube will contain an example model implementation for the desired AI task. It should be compatible with the data preparation MLCube (i.e., the outputs of the data preparation MLCube can be directly fed as inputs to this MLCube). A guide on how to implement model MLCubes can be found here.</p> </li> <li> <p>Metrics MLCube: This MLCube will be responsible for evaluating the performance of a model. It should be compatible with the reference model MLCube (i.e., the outputs of the reference model MLCube can be directly fed as inputs to this MLCube). A guide on how to implement metrics MLCubes can be found here.</p> </li> </ol> <p>For this tutorial, you are provided with following three already implemented mlcubes for the task of chest X-ray classification. The implementations can be found in the following links: Data Preparator, Reference Model, Metrics. These mlcubes are setup locally for you and can be found in your workspace folder under <code>data_preparator</code>, <code>model_custom_cnn</code>, and <code>metrics</code>.</p>"},{"location":"getting_started/benchmark_owner_demo/#2-develop-a-demo-dataset","title":"2. Develop a Demo Dataset","text":"<p>A demo dataset is a small reference dataset. It contains a few data records and their labels, which will be used to test the benchmark's workflow in two scenarios:</p> <ol> <li> <p>It is used for testing the benchmark's default workflow. The MedPerf client automatically runs a compatibility test of the benchmark's three mlcubes prior to its submission. The test is run using the benchmark's demo dataset as input.</p> </li> <li> <p>When a model owner wants to participate in the benchmark, the MedPerf client tests the compatibility of their model with the benchmark's data preparation cube and metrics cube. The test is run using the benchmark's demo dataset as input.</p> </li> </ol> <p>For this tutorial, you are provided with a demo dataset for the chest X-ray classification workflow. The dataset can be found in your workspace folder under <code>demo_data</code>. It is a small dataset comprising two chest X-ray images and corresponding thoracic disease labels.</p> <p>You can test the workflow now that you have the three MLCubes and the demo data. Testing the workflow before submitting any asset to the MedPerf server is usually recommended.</p>"},{"location":"getting_started/benchmark_owner_demo/#3-test-your-workflow","title":"3. Test your Workflow","text":"<p>MedPerf provides a single command to test an inference workflow. To test your workflow with local MLCubes and local data, the following need to be passed to the command:</p> <ol> <li>Path to the data preparation MLCube manifest file: <code>medperf_tutorial/data_preparator/mlcube/mlcube.yaml</code>.</li> <li>Path to the model MLCube manifest file: <code>medperf_tutorial/model_custom_cnn/mlcube/mlcube.yaml</code>.</li> <li>Path to the metrics MLCube manifest file: <code>medperf_tutorial/metrics/mlcube/mlcube.yaml</code>.</li> <li>Path to the demo dataset data records: <code>medperf_tutorial/demo_data/images</code>.</li> <li>Path to the demo dataset data labels. <code>medperf_tutorial/demo_data/labels</code>.</li> </ol> <p>Run the following command to execute the test ensuring you are in MedPerf's root folder:</p> <pre><code>medperf test run \\\n--data_preparation \"medperf_tutorial/data_preparator/mlcube/mlcube.yaml\" \\\n--model \"medperf_tutorial/model_custom_cnn/mlcube/mlcube.yaml\" \\\n--evaluator \"medperf_tutorial/metrics/mlcube/mlcube.yaml\" \\\n--data_path \"medperf_tutorial/demo_data/images\" \\\n--labels_path \"medperf_tutorial/demo_data/labels\"\n</code></pre> <p>Assuming the test passes successfully, you are ready to submit the MLCubes to the MedPerf server.</p>"},{"location":"getting_started/benchmark_owner_demo/#4-submitting-the-mlcubes","title":"4. Submitting the MLCubes","text":""},{"location":"getting_started/benchmark_owner_demo/#how-does-medperf-recognize-an-mlcube","title":"How does MedPerf Recognize an MLCube?","text":"<p>The MedPerf server registers an MLCube as metadata comprised of a set of files that can be retrieved from the internet. This means that before submitting an MLCube you have to host its files on the internet. The MedPerf client provides a utility to prepare the files of an MLCube that need to be hosted. You can refer to this page if you want to understand what the files are, but using the utility script is enough.</p> <p>To prepare the files of the three MLCubes, run the following command ensuring you are in MedPerf's root folder:</p> <pre><code>python scripts/package-mlcube.py --mlcube medperf_tutorial/data_preparator/mlcube --mlcube-types data-preparator\npython scripts/package-mlcube.py --mlcube medperf_tutorial/model_custom_cnn/mlcube --mlcube-types model\npython scripts/package-mlcube.py --mlcube medperf_tutorial/metrics/mlcube --mlcube-types metrics\n</code></pre> <p>For each MLCube, this script will create a new folder named <code>assets</code> in the MLCube directory. This folder will contain all the files that should be hosted separately.</p>"},{"location":"getting_started/benchmark_owner_demo/#host-the-files","title":"Host the Files","text":"<p>For the tutorial to run smoothly, the files are already hosted. If you wish to host them by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"getting_started/benchmark_owner_demo/#submit-the-mlcubes","title":"Submit the MLCubes","text":""},{"location":"getting_started/benchmark_owner_demo/#data-preparator-mlcube","title":"Data Preparator MLCube","text":"<p>For the Data Preparator MLCube, the submission should include:</p> <ul> <li> <p>The URL to the hosted mlcube manifest file, which is:</p> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/data_preparator/mlcube/mlcube.yaml\n</code></pre> </li> <li> <p>The URL to the hosted mlcube parameters file, which is:</p> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/data_preparator/mlcube/workspace/parameters.yaml\n</code></pre> </li> </ul> <p>Use the following command to submit:</p> <pre><code>medperf mlcube submit \\\n--name my-prep-cube \\\n--mlcube-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/data_preparator/mlcube/mlcube.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/data_preparator/mlcube/workspace/parameters.yaml\" \\\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#reference-model-mlcube","title":"Reference Model MLCube","text":"<p>For the Reference Model MLCube, the submission should include:</p> <ul> <li> <p>The URL to the hosted mlcube manifest file:</p> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_custom_cnn/mlcube/mlcube.yaml\n</code></pre> </li> <li> <p>The URL to the hosted mlcube parameters file:</p> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_custom_cnn/mlcube/workspace/parameters.yaml\n</code></pre> </li> <li> <p>The URL to the hosted additional files tarball file:</p> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/cnn_weights.tar.gz\n</code></pre> </li> </ul> <p>Use the following command to submit:</p> <pre><code>medperf mlcube submit \\\n--name my-modelref-cube \\\n--mlcube-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_custom_cnn/mlcube/mlcube.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_custom_cnn/mlcube/workspace/parameters.yaml\" \\\n--additional-file \"https://storage.googleapis.com/medperf-storage/chestxray_tutorial/cnn_weights.tar.gz\"\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#metrics-mlcube","title":"Metrics MLCube","text":"<p>For the Metrics MLCube, the submission should include:</p> <ul> <li> <p>The URL to the hosted mlcube manifest file:</p> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/metrics/mlcube/mlcube.yaml\n</code></pre> </li> <li> <p>The URL to the hosted mlcube parameters file:</p> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/metrics/mlcube/workspace/parameters.yaml\n</code></pre> </li> </ul> <p>Use the following command to submit:</p> <pre><code>medperf mlcube submit \\\n--name my-metrics-cube \\\n--mlcube-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/metrics/mlcube/mlcube.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/metrics/mlcube/workspace/parameters.yaml\" \\\n</code></pre> <p>Each of the three MLCubes will be assigned by a server UID. You can check the server UID for each MLCube by running:</p> <pre><code>medperf mlcube ls --mine\n</code></pre> <p>Next, you will learn how to host the demo dataset.</p>"},{"location":"getting_started/benchmark_owner_demo/#5-host-the-demo-dataset","title":"5. Host the Demo Dataset","text":"<p>The demo dataset should be packaged in a specific way as a compressed tarball file. The folder stucture in the workspace currently looks like the following:</p> <pre><code>.\n\u2514\u2500\u2500 medperf_tutorial\n    \u251c\u2500\u2500 demo_data\n    \u2502   \u251c\u2500\u2500 images\n    \u2502   \u2514\u2500\u2500 labels\n    \u2502\n    ...\n</code></pre> <p>The goal is to package the folder <code>demo_data</code>. You must first create a file called <code>paths.yaml</code>. This file will provide instructions on how to locate the data records path and the labels path. The <code>paths.yaml</code> file should specify both the data records path and the labels path.</p> <p>In your workspace directory (<code>medperf_tutorial</code>), create a file <code>paths.yaml</code> and fill it with the following:</p> <pre><code>data_path: demo_data/images\nlabels_path: demo_data/labels\n</code></pre> <p>Note</p> <p>The paths are determined by the Data Preparator MLCube's expected input path.</p> <p>After that, the workspace should look like the following:</p> <pre><code>.\n\u2514\u2500\u2500 medperf_tutorial\n    \u251c\u2500\u2500 demo_data\n    \u2502   \u251c\u2500\u2500 images\n    \u2502   \u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 paths.yaml\n    \u2502\n    ...\n</code></pre> <p>Finally, compress the required assets (<code>demo_data</code> and <code>paths.yaml</code>) into a tarball file by running the following command in your workspace directory:</p> <pre><code>tar -czf demo_data.tar.gz demo_data paths.yaml\n</code></pre> <p>And that's it! Now you have to host the tarball file (<code>demo_data.tar.gz</code>) on the internet.</p> <p>For the tutorial to run smoothly, the file is already hosted at the following URL:</p> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/demo_data.tar.gz\n</code></pre> <p>If you wish to host it by yourself, you can find the list of supported options and details about hosting files in this page.</p> <p>Finally, now after having the MLCubes submitted and the demo dataset hosted, you can submit the benchmark to the MedPerf server.</p>"},{"location":"getting_started/benchmark_owner_demo/#6-submit-your-benchmark","title":"6. Submit your Benchmark","text":"<p>You need to keep at hand the following information:</p> <ul> <li>The Demo Dataset URL. Here, the URL will be:</li> </ul> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/demo_data.tar.gz\n</code></pre> <ul> <li>The server UIDs of the three MLCubes:<ul> <li>Data preparator UID: <code>1</code></li> <li>Reference model UID: <code>2</code></li> <li>Evaluator UID: <code>3</code></li> </ul> </li> </ul> <p>You can create and submit your benchmark using the following command:</p> <pre><code>medperf benchmark submit \\\n--name tutorial_bmk \\\n--description \"MedPerf demo bmk\" \\\n--demo-url \"https://storage.googleapis.com/medperf-storage/chestxray_tutorial/demo_data.tar.gz\" \\\n--data-preparation-mlcube 1 \\\n--reference-model-mlcube 2 \\\n--evaluator-mlcube 3\n</code></pre> <p>The MedPerf client will first automatically run a compatibility test between the MLCubes using the demo dataset. If the test is successful, the benchmark will be submitted along with the compatibility test results.</p> <p>Note</p> <p>The benchmark will stay inactive until the MedPerf server admin approves your submission.</p> <p>That's it! You can check your benchmark's server UID by running:</p> <pre><code>medperf benchmark ls --mine\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#see-also","title":"See Also","text":"<ul> <li>Benchmark Associations.</li> <li>Models Priorities</li> </ul>"},{"location":"getting_started/data_owner_demo/","title":"Hands-on Tutorial for Data Owners","text":""},{"location":"getting_started/data_owner_demo/#overview","title":"Overview","text":"<p>This guide provides you with the necessary steps to use MedPerf as a Data Owner. The key tasks can be summarized as follows:</p> <ol> <li>Prepare your data.</li> <li>Submit your data information.</li> <li>Request participation in a benchmark.</li> <li>Execute the benchmark models on your dataset.</li> <li>Submit a result.</li> </ol> <p>It is assumed that you have the general testing environment set up.</p>"},{"location":"getting_started/data_owner_demo/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/data_owner_demo/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo data\ncd ..\n</code></pre>"},{"location":"getting_started/data_owner_demo/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_data_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded.</p>"},{"location":"getting_started/data_owner_demo/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should not do anything.</p> <p>You are now ready to start!</p>"},{"location":"getting_started/data_owner_demo/#1-prepare-your-data","title":"1. Prepare your Data","text":"<p>To prepare your data, you need to collect the following information:</p> <ul> <li>A name you wish to have for your dataset.</li> <li>A small description of the dataset.</li> <li>The source location of your data (e.g., hospital name).</li> <li>The path to the data records (here, it is <code>medperf_tutorial/sample_raw_data/images</code>).</li> <li>The path to the labels of the data (here, it is <code>medperf_tutorial/sample_raw_data/labels</code>)</li> <li>The benchmark ID that you wish to participate in. This ensures your data is prepared using the benchmark's data preparation MLCube.</li> </ul> <p>Note</p> <p>The <code>data_path</code> and <code>labels_path</code> are determined according to the input path requirements of the data preparation MLCube. To ensure that your data is structured correctly, it is recommended to check with the Benchmark Committee for specific details or instructions.</p> <p>In order to find the benchmark ID, you can execute the following command to view the list of available benchmarks.</p> <pre><code>medperf benchmark ls\n</code></pre> <p>The target benchmark ID here is <code>1</code>.</p> <p>Run the following command to prepare your data (make sure you are in MedPerf's root folder):</p> <pre><code>medperf dataset create \\\n--name \"mytestdata\" \\\n--description \"A tutorial dataset\" \\\n--location \"My machine\" \\\n--data_path \"medperf_tutorial/sample_raw_data/images\" \\\n--labels_path \"medperf_tutorial/sample_raw_data/labels\" \\\n--benchmark 1\n</code></pre> <p>After that, your dataset will be successfully prepared! This command will also calculate some statistics on your dataset. These statistics, along with general information about the data, are going to be submitted to the MedPerf server in the next step.</p>"},{"location":"getting_started/data_owner_demo/#2-submit-your-data-information","title":"2. Submit your Data Information","text":"<p>To submit your data information, you need to know the generated UID of your prepared dataset. Normally, you will see it in the output of the previous command. You can always check local datasets information by running:</p> <pre><code>medperf dataset ls --local\n</code></pre> <p>Note</p> <p>You will be submitting general information about the data, not the data itself. The data never leaves your machine.</p> <p>The unique identifier for your generated data is <code>9d56e799a9e63a6c3ced056ebd67eb6381483381</code>.</p> <p>Run the following command to submit your dataset information to the MedPerf server:</p> <pre><code>medperf dataset submit --data_uid 9d56e799a9e63a6c3ced056ebd67eb6381483381\n</code></pre> <p>Once you run this command, the information to be submitted will be displayed on the screen and you will be asked to confirm your submission.</p> <p>After successfully submitting your dataset, you can proceed to request participation in the benchmark by initiating an association request.</p>"},{"location":"getting_started/data_owner_demo/#3-request-participation","title":"3. Request Participation","text":"<p>For submitting the results of executing the benchmark models on your data in the future, you must associate your data with the benchmark.</p> <p>Once you have submitted your dataset to the MedPerf server, it will be assigned a server UID, which you can find by running <code>medperf dataset ls --mine</code>. Your dataset's server UID is also <code>1</code>.</p> <p>Run the following command to request associating your dataset with the benchmark:</p> <pre><code>medperf dataset associate --benchmark_uid 1 --data_uid 1\n</code></pre> <p>This command will first run the benchmark's reference model on your dataset to ensure your dataset is compatible with the benchmark workflow. Then, the association request information is printed on the screen, which includes an executive summary of the test mentioned. You will be prompted to confirm sending this information and initiating this association request.</p>"},{"location":"getting_started/data_owner_demo/#how-to-proceed-after-requesting-association","title":"How to proceed after requesting association","text":"<p>When participating with a real benchmark, you must wait for the Benchmark Committee to approve the association request. You can check the status of your association requests by running <code>medperf association ls</code>. The association is identified by the server UIDs of your dataset and the benchmark with which you are requesting association.</p> <p>For the sake of continuing the tutorial only, run the following to simulate the benchmark committee approving your association (make sure you are in the MedPerf's root directory):</p> <pre><code>sh tutorials_scripts/simulate_data_association_approval.sh\n</code></pre> <p>You can verify if your association request has been approved by running <code>medperf association ls</code>.</p>"},{"location":"getting_started/data_owner_demo/#4-execute-the-benchmark","title":"4. Execute the Benchmark","text":"<p>MedPerf provides a command that runs all the models of a benchmark effortlessly. You only need to provide two parameters:</p> <ul> <li>The benchmark ID you want to run, which is <code>1</code>.</li> <li>The server UID of your data, which is <code>1</code>.</li> </ul> <p>For that, run the following command:</p> <pre><code>medperf benchmark run --benchmark 1 --data_uid 1\n</code></pre> <p>After running the command, you will receive a summary of the executions. You will see something similar to the following:</p> <pre><code>  model  local result UID    partial result    from cache    error\n-------  ------------------  ----------------  ------------  -------\n      2  b1m2d1              False             True\n      4  b1m4d1              False             False\nTotal number of models: 2\n        1 were skipped (already executed), of which 0 have partial results\n        0 failed\n        1 ran successfully, of which 0 have partial results\n\n\u2705 Done!\n</code></pre> <p>This means that the benchmark has two models:</p> <ul> <li>A model that you already ran when you requested the association. This explains why it was skipped.</li> <li>Another model that ran successfully. Its result generated UID is <code>b1m4d1</code>.</li> </ul> <p>You can view the results by running the following command with the specific local result UID. For example:</p> <pre><code>medperf result view b1m4d1\n</code></pre> <p>For now, your results are only local. Next, you will learn how to submit the results.</p>"},{"location":"getting_started/data_owner_demo/#5-submit-a-result","title":"5. Submit a Result","text":"<p>After executing the benchmark, you will submit a result to the MedPerf server. To do so, you have to find the target result generated UID.</p> <p>As an example, you will be submitting the result of UID <code>b1m4d1</code>. To do this, run the following command:</p> <pre><code>medperf result submit --result b1m4d1\n</code></pre> <p>The information that is going to be submitted will be printed to the screen and you will be prompted to confirm that you want to submit.</p>"},{"location":"getting_started/data_owner_demo/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/data_owner_demo/#see-also","title":"See Also","text":"<ul> <li>Running a Single Model.</li> </ul>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting_started/installation/#python","title":"Python","text":"<p>Make sure you have Python 3.9 installed along with pip. To check if they are installed, run:</p> <pre><code>python --version\npip --version\n</code></pre> <p>or, depending on you machine configuration:</p> <pre><code>python3 --version\npip3 --version\n</code></pre> <p>We will assume the commands' names are <code>pip</code> and <code>python</code>. Use <code>pip3</code> and <code>python3</code> if your machine is configured differently.</p>"},{"location":"getting_started/installation/#docker-or-singularity","title":"Docker or Singularity","text":"<p>Warning</p> <p>Singularity is temporarily not supported.</p> <p>Make sure you have the latest version of Docker or Singularity 3.10 installed.</p> <p>To verify docker is installed, run:</p> <pre><code>docker --version\n</code></pre> <p>To verify singularity is installed, run:</p> <pre><code>singularity --version\n</code></pre> <p>If using Docker, make sure you can run Docker as a non-root user.</p>"},{"location":"getting_started/installation/#install-medperf","title":"Install MedPerf","text":"<ol> <li> <p>(Optional) MedPerf is better to be installed in a virtual environment. We recommend using Anaconda. Having anaconda installed, create a virtual environment <code>medperf-env</code> with the following command:</p> <pre><code>conda create -n medperf-env python=3.9\n</code></pre> <p>Then, activate your environment:</p> <pre><code>conda activate medperf-env\n</code></pre> </li> <li> <p>Clone the MedPerf repository:</p> <pre><code>git clone https://github.com/mlcommons/medperf.git\ncd medperf\n</code></pre> </li> <li> <p>Install MedPerf from source:</p> <pre><code>pip install -e ./cli\n</code></pre> </li> <li> <p>Verify the installation:</p> <pre><code>medperf --version\n</code></pre> </li> </ol>"},{"location":"getting_started/installation/#whats-next","title":"What's Next?","text":"<ul> <li>Get familiar with the MedPerf client by following the hands-on tutorials.</li> <li>Understand and learn how to build MedPerf MLCubes.</li> </ul>"},{"location":"getting_started/model_owner_demo/","title":"Hands-on Tutorial for Model Owners","text":""},{"location":"getting_started/model_owner_demo/#overview","title":"Overview","text":"<p>In this guide, you will learn how a Model Owner can use MedPerf to take part in a benchmark. It's highly recommend that you follow this or this guide first to implement your own model MLCube and use it throughout this tutorial. However, this guide provides an already implemented MLCube if you want to directly proceed to learn how to interact with MedPerf.</p> <p>The main tasks of this guide are:</p> <ol> <li>Testing MLCube compatibility with the benchmark.</li> <li>Submitting the MLCube.</li> <li>Requesting participation in a benchmark.</li> </ol> <p>It's assumed that you have already set up the general testing environment as explained in the setup guide.</p>"},{"location":"getting_started/model_owner_demo/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/model_owner_demo/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo model\ncd ..\n</code></pre>"},{"location":"getting_started/model_owner_demo/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_model_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded.</p>"},{"location":"getting_started/model_owner_demo/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should not do anything.</p> <p>You are now ready to start!</p>"},{"location":"getting_started/model_owner_demo/#1-test-your-mlcube-compatibility","title":"1. Test your MLCube Compatibility","text":"<p>Before submitting your MLCube, it is highly recommended that you test your MLCube compatibility with the benchmarks of interest to avoid later edits and multiple submissions. Your MLCube should be compatible with the benchmark workflow in two main ways:</p> <ol> <li>It should expect a specific data input structure</li> <li>Its outputs should follow a particular structure expected by the benchmark's metrics evaluator MLCube</li> </ol> <p>These details should usually be acquired by contacting the Benchmark Committee and following their instructions.</p> <p>To test your MLCube validity with the benchmark, first run <code>medperf benchmark ls</code> to identify the benchmark's server UID. In this case, it is going to be <code>1</code>.</p> <p>Next, locate the MLCube. Unless you implemented your own MLCube, the MLCube provided for this tutorial is located in your workspace: <code>medperf_tutorial/model_mobilenetv2/mlcube/mlcube.yaml</code>.</p> <p>After that, run the compatibility test:</p> <pre><code>medperf test run \\\n--benchmark 1 \\\n--model \"medperf_tutorial/model_mobilenetv2/mlcube/mlcube.yaml\"\n</code></pre> <p>Assuming the test passes successfuly, you are ready to submit the MLCube to the MedPerf server.</p>"},{"location":"getting_started/model_owner_demo/#2-submit-the-mlcube","title":"2. Submit the MLCube","text":""},{"location":"getting_started/model_owner_demo/#how-does-medperf-recognize-an-mlcube","title":"How does MedPerf Recognize an MLCube?","text":"<p>The MedPerf server registers an MLCube as metadata comprised of a set of files that can be retrieved from the internet. This means that before submitting an MLCube you have to host its files on the internet. The MedPerf client provides a utility to prepare the files of an MLCube that need to be hosted. You can refer to this page if you want to understand what the files are, but using the utility script is enough.</p> <p>To prepare the files of the MLCube, run the following command ensuring you are in MedPerf's root folder:</p> <pre><code>python scripts/package-mlcube.py --mlcube medperf_tutorial/model_mobilenetv2/mlcube --mlcube-types model\n</code></pre> <p>This script will create a new folder in the MLCube directory, named <code>assets</code>, containing all the files that should be hosted separately.</p>"},{"location":"getting_started/model_owner_demo/#host-the-files","title":"Host the Files","text":"<p>For the tutorial to run smoothly, the files are already hosted. If you wish to host them by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"getting_started/model_owner_demo/#submit-the-mlcube","title":"Submit the MLCube","text":"<p>The submission should include the URLs of all the hosted files. For the MLCube provided for the tutorial:</p> <ul> <li>The URL to the hosted mlcube manifest file is</li> </ul> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_mobilenetv2/mlcube/mlcube.yaml\n</code></pre> <ul> <li>The URL to the hosted mlcube parameters file is</li> </ul> <pre><code>https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_mobilenetv2/mlcube/workspace/parameters.yaml\n</code></pre> <ul> <li>The URL to the hosted additional files tarball file is</li> </ul> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/mobilenetv2_weights.tar.gz\n</code></pre> <p>Use the following command to submit:</p> <pre><code>medperf mlcube submit \\\n--name my-model-cube \\\n--mlcube-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_mobilenetv2/mlcube/mlcube.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/hasan7n/medperf/99b0d84bc107415d9fc6f69c4ea3fcdfbf22315d/examples/chestxray_tutorial/model_mobilenetv2/mlcube/workspace/parameters.yaml\" \\\n--additional-file \"https://storage.googleapis.com/medperf-storage/chestxray_tutorial/mobilenetv2_weights.tar.gz\"\n</code></pre> <p>The MLCube will be assigned by a server UID. You can check it by running:</p> <pre><code>medperf mlcube ls --mine\n</code></pre>"},{"location":"getting_started/model_owner_demo/#3-request-participation","title":"3. Request Participation","text":"<p>Benchmark workflows are run by Data Owners, who will get notified when a new model is added to a benchmark. You must request the association for your model to be part of the benchmark.</p> <p>To initiate an association request, you need to collect the following information:</p> <ul> <li>The target benchmark ID, which is <code>1</code></li> <li>The server UID of your MLCube, which is <code>4</code>.</li> </ul> <p>Run the following command to request associating your MLCube with the benchmark:</p> <pre><code>medperf mlcube associate --benchmark 1 --model_uid 4\n</code></pre> <p>This command will first run the benchmark's workflow on your model to ensure your model is compatible with the benchmark workflow. Then, the association request information is printed on the screen, which includes an executive summary of the test mentioned. You will be prompted to confirm sending this information and initiating this association request.</p>"},{"location":"getting_started/model_owner_demo/#what-happens-after-requesting-the-association","title":"What Happens After Requesting the Association?","text":"<p>When participating with a real benchmark, you must wait for the Benchmark Committee to approve the association request. You can check the status of your association requests by running <code>medperf association ls</code>. The association is identified by the server UIDs of your MLCube and the benchmark with which you are requesting association.</p>"},{"location":"getting_started/model_owner_demo/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/overview/","title":"Overview","text":"<p>The MedPerf client provides all the necessary tools to run a complete benchmark experiment. Below, you will find a comprehensive breakdown of user roles and the corresponding functionalities they can access and perform using the MedPerf client:</p> <ul> <li>Benchmark Committee: The Benchmark Commitee can define and create a benchmark, as well as manage experiments (e.g., approving which datasets and models will be allowed to participate)</li> <li>Model Owner: The Model Owner can submit a model to the MedPerf server and request participation in a benchmark.</li> <li>Data Owner: The Data Owner can prepare their raw medical data, register the metadata of their prepared dataset, request participation in a benchmark, execute a benchmark's models on their data, and submit the results of the execution.</li> </ul>"},{"location":"getting_started/overview/#whats-next","title":"What's Next?","text":"<ul> <li>Create your MedPerf account</li> <li>Install the MedPerf client</li> <li>Get familiar with the MedPerf client by following the hands-on tutorials</li> <li>Understand and learn how to build MedPerf MLCubes</li> </ul>"},{"location":"getting_started/setup/","title":"Setup","text":"<p>This setup is only for running the tutorials. If you are using MedPerf with a real benchmark and real experiments, skip to this section to optionally change your container runner. Then, follow the tutorials as a general guidance for your real experiments.</p>"},{"location":"getting_started/setup/#run-a-local-medperf-server","title":"Run a Local MedPerf Server","text":"<p>For this tutorial, you should spawn a local MedPerf server for the MedPerf client to communicate with. Note that this server will be hosted on your <code>localhost</code> and not on the internet.</p> <ol> <li> <p>Install the server requirements ensuring you are in MedPerf's root folder:</p> <pre><code>pip install -r server/requirements.txt\npip install -r server/test-requirements.txt\n</code></pre> </li> <li> <p>Run the local MedPerf server using the following command:</p> <pre><code>cd server\ncp .env.local.local-auth .env\nsh setup-dev-server.sh\n</code></pre> </li> </ol> <p>The local MedPerf server now is ready to recieve requests. You can always stop the server by pressing <code>CTRL</code>+<code>C</code> in the terminal where you ran the server.</p> <p>After that, you will be configuring the MedPerf client to communicate with the local MedPerf server. Make sure you continue following the instructions in a new terminal.</p>"},{"location":"getting_started/setup/#configure-the-medperf-client","title":"Configure the MedPerf Client","text":"<p>The MedPerf client can be configured by creating or modifying \"<code>profiles</code>\". A profile is a set of configuration parameters used by the client during runtime. By default, the profile named <code>default</code> will be active.</p> <p>The <code>default</code> profile is preconfigured so that the client communicates with the main MedPerf server (api.medperf.org). For the purposes of the tutorial, you will be using the <code>local</code> profile as it is preconfigured so that the client communicates with the local MedPerf server.</p> <p>To activate the <code>local</code> profile, run the following command:</p> <pre><code>medperf profile activate local\n</code></pre> <p>You can always check which profile is active by running:</p> <pre><code>medperf profile ls\n</code></pre> <p>To view the current active profile's configured parameters, you can run the following:</p> <pre><code>medperf profile view\n</code></pre>"},{"location":"getting_started/setup/#choose-the-container-runner","title":"Choose the Container Runner","text":"<p>Warning</p> <p>Singularity is temporarily not supported.</p> <p>You can configure the MedPerf client to use either Docker or Singularity. The <code>local</code> profile is configured to use Docker. If you want to use MedPerf with Singularity, modify the <code>local</code> profile configured parameters by running the following:</p> <pre><code>medperf profile set --platform singularity\n</code></pre> <p>This command will modify the <code>platform</code> parameter of the currently activated profile.</p>"},{"location":"getting_started/setup/#whats-next","title":"What's Next?","text":"<p>The local MedPerf server now is ready to recieve requests, and the MedPerf client is ready to communicate. Depending on your role, you can follow these hands-on tutorials:</p> <ul> <li> <p>How a benchmark committee can create and submit a benchmark.</p> </li> <li> <p>How a model owner can submit a model.</p> </li> <li> <p>How a data owner can prepare their data and execute a benchmark.</p> </li> </ul>"},{"location":"getting_started/signup/","title":"Create your MedPerf Account","text":"<p>MedPerf uses passwordless authentication. This means that there will be no need for a password, and you have to access your email in order complete the signup process.</p> <p>Automatic signups are currently disabled. Please contact the MedPerf team in order to provision an account.</p> <p>Tip</p> <p>You don't need an account to run the tutorials and learn how to use the MedPerf client.</p>"},{"location":"getting_started/signup/#whats-next","title":"What's Next?","text":"<ul> <li>Install the MedPerf client</li> </ul>"},{"location":"getting_started/tutorials_overview/","title":"Overview","text":"<p>To ensure users have the best experience in learning the fundamentals of MedPerf and utilizing the MedPerf client, the following set of tutorials are provided:</p> <ul> <li> <p>How a Benchmark Committee can create and submit a benchmark.</p> </li> <li> <p>How a Model Owner can submit a model.</p> </li> <li> <p>How a Data Owner can prepare their data and execute a benchmark.</p> </li> </ul> <p>The tutorials simulate a benchmarking example for the task of detecting thoracic diseases from chest X-ray scans. You can find the description of the used data here. Throughout the tutorials, you will be interacting with a temporary local MedPerf server as described in the setup page. This allows you to freely experiment with the MedPerf client and rerun the tutorials as many times as you want, providing you with an immersive learning experience. Please note that these tutorials also serve as a general guidance to be followed when using the MedPerf client in a real scenario.</p> <p>Before proceeding to the tutorials, make sure you have the general tutorial environment set up.</p> <p>For a detailed reference on the commands used throughout the tutorials, you can always refer to the command line interface documentation.</p>"},{"location":"getting_started/shared/before_we_start/","title":"Before we start","text":""},{"location":"getting_started/shared/before_we_start/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/shared/before_we_start/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo {{ no such element: dict object['tutorial_id'] }}\ncd ..\n</code></pre>"},{"location":"getting_started/shared/before_we_start/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_{{ no such element: dict object['tutorial_id'] }}_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded.</p>"},{"location":"getting_started/shared/before_we_start/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should not do anything.</p> <p>You are now ready to start!</p>"},{"location":"getting_started/shared/cleanup/","title":"Cleanup","text":""},{"location":"getting_started/shared/cleanup/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/shared/mlcube_submission_overview/","title":"Mlcube submission overview","text":"<p>The MedPerf server registers an MLCube as metadata comprised of a set of files that can be retrieved from the internet. This means that before submitting an MLCube you have to host its files on the internet. The MedPerf client provides a utility to prepare the files of an MLCube that need to be hosted. You can refer to this page if you want to understand what the files are, but using the utility script is enough.</p>"},{"location":"getting_started/shared/redirect_to_hosting_files/","title":"Redirect to hosting files","text":""},{"location":"getting_started/shared/redirect_to_hosting_files/#host-the-files","title":"Host the Files","text":"<p>For the tutorial to run smoothly, the files are already hosted. If you wish to host them by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"mlcubes/gandlf_mlcube/","title":"Creating a GaNDLF MLCube","text":""},{"location":"mlcubes/gandlf_mlcube/#overview","title":"Overview","text":"<p>This guide will walk you through how to wrap a model trained using GaNDLF as a MedPerf-compatible MLCube ready to be used for inference (i.e. as a Model MLCube). The steps can be summarized as follows:</p> <ol> <li>Train a GaNDLF model</li> <li>Create the MLCube file</li> <li>(Optional) Create a custom entrypoint</li> <li>Deploy the GaNDLF model as an MLCube</li> </ol> <p>Before proceeding, make sure you have medperf installed and GaNDLF installed.</p>"},{"location":"mlcubes/gandlf_mlcube/#before-we-start","title":"Before We Start","text":""},{"location":"mlcubes/gandlf_mlcube/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_GaNDLF_mlcube_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded. Run <code>cd medperf_tutorial</code> to switch to this folder.</p>"},{"location":"mlcubes/gandlf_mlcube/#1-train-a-gandlf-model","title":"1. Train a GaNDLF Model","text":"<p>Train a small GaNDLF model to use for this guide. You can skip this step if you already have a trained model.</p> <p>Make sure you are in the workspace folder <code>medperf_tutorial</code>. Run:</p> <pre><code>gandlf_run \\\n-c ./config_getting_started_segmentation_rad3d.yaml \\\n-i ./data.csv \\\n-m ./trained_model_output \\\n-t True \\\n-d cpu\n</code></pre> <p>Note that if you want to train on GPU you can use <code>-d cuda</code>, but the example used here should take only few seconds using the CPU.</p> <p>Warning</p> <p>This tutorial assumes the user is using the latest GaNDLF version. The configuration file <code>config_getting_started_segmentation_rad3d.yaml</code> will cause problems if you are using a different version, make sure you do the necessary changes.</p> <p>You will now have your trained model and its related files in the folder <code>trained_model_output</code>. Next, you will start learning how to wrap this trained model within an MLCube.</p>"},{"location":"mlcubes/gandlf_mlcube/#2-create-the-mlcube-file","title":"2. Create the MLCube File","text":"<p>MedPerf provides a cookiecutter to create an MLCube file that is ready to be consumed by <code>gandlf_deploy</code> and produces an MLCube ready to be used by MedPerf. To create the MLCube, run: (make sure you are in the workspace folder <code>medperf_tutorial</code>)</p> <pre><code>medperf mlcube create gandlf\n</code></pre> <p>Note</p> <p>MedPerf is running CookieCutter under the hood. This medperf command provides additional arguments for handling different scenarios. You can see more information on this by running <code>medperf mlcube create --help</code></p> <p>You will be prompted to customize the MLCube creation. Below is an example of how your response might look like:</p> <pre><code>project_name [GaNDLF MLCube]: My GaNDLF MLCube # (1)!\nproject_slug [my_gandlf_mlcube]: my_gandlf_mlcube # (2)!\ndescription [GaNDLF MLCube Template. Provided by MLCommons]: GaNDLF MLCube implementation # (3)!\nauthor_name [John Smith]: John Smith # (4)!\naccelerator_count [1]: 0 # (5)!\ndocker_build_file [Dockerfile-CUDA11.6]: Dockerfile-CPU # (6)!\ndocker_image_name [docker/image:latest]: johnsmith/gandlf_model:0.0.1 # (7)!\n</code></pre> <ol> <li>Gives a Human-readable name to the MLCube Project.</li> <li>Determines how the MLCube root folder will be named.</li> <li>Gives a Human-readable description to the MLCube Project.</li> <li>Documents the MLCube implementation by specifying the author. Please use your own name here.</li> <li>Indicates how many GPUs should be visible by the MLCube.</li> <li>Indicates the Dockerfile name from GaNDLF that should be used for building your docker image. Use the name of the Dockerfile that aligns with your model's dependencies. Any \"Dockerfile-*\" in the GaNDLF source repository is valid.</li> <li>MLCubes use containers under the hood. Medperf supports both Docker and Singularity. Here, you can provide an image tag to the image that will be created by this MLCube. It's recommended to use a naming convention that allows you to upload it to Docker Hub.</li> </ol> <p>Assuming you chose <code>my_gandlf_mlcube</code> as the project slug, you will find your MLCube created under the folder <code>my_gandlf_mlcube</code>. Next, you will use a <code>GaNDLF</code> utility to build the MLCube.</p> <p>Note</p> <p>You might need to specify additional configurations in the <code>mlcube.yaml</code> file if you are using a GPU. Check the generated <code>mlcube.yaml</code> file for more info, as well as the MLCube documentation.</p>"},{"location":"mlcubes/gandlf_mlcube/#3-optional-create-a-custom-entrypoint","title":"3. (Optional) Create a Custom Entrypoint","text":"<p>When deploying the GaNDLF model directly as a model MLCube, the default entrypoint will be <code>gandlf_run ...</code>. You can override the entrypoint with a custom python script. One of the usecases is described below.</p> <p><code>gandlf_run</code> expects a <code>data.csv</code> file in the input data folder, which describes the inference test cases and their associated paths (Read more about GaNDLF's csv file conventions here). In case your MLCube will expect a data folder with a predefined data input structure but without this csv file, you can use a custom script that prepares this csv file as an entrypoint. You can find the recommended template and an example here.</p>"},{"location":"mlcubes/gandlf_mlcube/#4-deploy-the-gandlf-model-as-an-mlcube","title":"4. Deploy the GaNDLF Model as an MLCube","text":"<p>To deploy the GaNDLF model as an MLCube, run the following: (make sure you are in the workspace folder <code>medperf_tutorial</code>)</p> <pre><code>gandlf_deploy \\\n-c ./config_getting_started_segmentation_rad3d.yaml \\\n-m ./trained_model_output \\\n--target docker \\\n--mlcube-root ./my_gandlf_mlcube \\\n-o ./built_gandlf_mlcube \\\n--mlcube-type model \\\n--entrypoint &lt;(optional) path to your custom entrypoint script&gt; \\ # (1)!\n-g False # (2)!\n</code></pre> <ol> <li>If you are not using a custom entrypoint, ignore this option.</li> <li>Change to <code>True</code> if you want the resulting MLCube to use a GPU for inference.</li> </ol> <p>GaNDLF will use your initial MLCube configuration <code>my_gandlf_mlcube</code>, the GaNDLF experiment configuration file <code>config_classification.yaml</code>, and the trained model <code>trained_model_output</code> to create a ready MLCube <code>built_gandlf_mlcube</code> and build the docker image that will be used by the MLCube. The docker image will have the model weights and the GaNDLF experiment configuration file embedded. You can check that your image was built by running <code>docker image ls</code>. You will see <code>johnsmith/gandlf_model:0.0.1</code> (or whatever image name that was used) created moments ago.</p>"},{"location":"mlcubes/gandlf_mlcube/#5-next-steps","title":"5. Next Steps","text":"<p>That's it! You have built a MedPerf-compatible MLCube with GaNDLF. You may want to submit your MLCube to MedPerf, you can follow this tutorial.</p> <p>Tip</p> <p>MLCubes created by GaNDLF have the model weights and configuration file embedded in the docker image. When you want to deploy your MLCube for MedPerf, all you need to do is pushing the docker image and hosting the mlcube.yaml file.</p>"},{"location":"mlcubes/gandlf_mlcube/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre>"},{"location":"mlcubes/gandlf_mlcube/#see-also","title":"See Also","text":"<ul> <li>Creating a Model MLCube from scratch.</li> </ul>"},{"location":"mlcubes/mlcube_data/","title":"In Progress","text":""},{"location":"mlcubes/mlcube_metrics/","title":"In Progress","text":""},{"location":"mlcubes/mlcube_models/","title":"Model MLCube","text":""},{"location":"mlcubes/mlcube_models/#introduction","title":"Introduction","text":"<p>This is one of the three guides that help the user build MedPerf-compatible MLCubes. The other two guides are for building a Data Preparator MLCube and a Metrics MLCube. Together, the three MLCubes examples constitute a complete benchmark workflow for the task of thoracic disease detection from Chest X-rays.</p>"},{"location":"mlcubes/mlcube_models/#about-this-guide","title":"About this Guide","text":"<p>This guide will help users familiarize themselves with the expected interface of the Model MLCube and gain a comprehensive understanding of its components. By following this walkthrough, users will gain insights into the structure and organization of a Model MLCube, allowing them at the end to be able to implement their own MedPerf-compatible Model MLCube.</p> <p>The guide will start by providing general advice, steps, and hints on building these MLCubes. Then, an example will be presented through which the provided guidance will be applied step-by-step to build a Chest X-ray classifier MLCube. The final MLCube code can be found here.</p>"},{"location":"mlcubes/mlcube_models/#before-building-the-mlcube","title":"Before Building the MLCube","text":"<p>It is assumed that you already have a working code that runs inference on data and generates predictions, and what you want to accomplish through this guide is to wrap your inference code within an MLCube.</p> <ul> <li>Make sure you decouple your inference logic from the other machine learning common pipelines (e.g.; training, metrics, ...).</li> <li>Your inference logic can be written in any structure, can be split into any number of files, can represent any number of inference stages, etc..., as long as the following hold:<ul> <li>The whole inference flow can be invoked by a single command/function.</li> <li>This command/function has at least the following arguments:<ul> <li>A string representing a path that points to all input data records</li> <li>A string representing a path that points to the desired output directory where the predictions will be stored.</li> </ul> </li> </ul> </li> <li>Your inference logic should not alter the input files and folders.</li> <li>Your inference logic should expect the input data in a certain structure. This is usually determined by following the specifications of the benchmark you want to participate in.</li> <li>Your inference logic should save the predictions in the output directory in a certain structure. This is usually determined by following the specifications of the benchmark you want to participate in.</li> </ul>"},{"location":"mlcubes/mlcube_models/#use-an-mlcube-template","title":"Use an MLCube Template","text":"<p>MedPerf provides MLCube templates. You should start from a template for faster implementation and to build MLCubes that are compatible with MedPerf.</p> <p>First, make sure you have MedPerf installed. You can create a model MLCube template by running the following command:</p> <pre><code>medperf mlcube create model\n</code></pre> <p>You will be prompted to fill in some configuration options through the CLI. Below are the options and their default values:</p> <pre><code>project_name [Model MLCube]: # (1)!\nproject_slug [model_mlcube]: # (2)!\ndescription [Model MLCube Template. Provided by MLCommons]: # (3)!\nauthor_name [John Smith]: # (4)!\naccelerator_count [0]: # (5)!\ndocker_image_name [docker/image:latest]: # (6)!\n</code></pre> <ol> <li>Gives a Human-readable name to the MLCube Project.</li> <li>Determines how the MLCube root folder will be named.</li> <li>Gives a Human-readable description to the MLCube Project.</li> <li>Documents the MLCube implementation by specifying the author.</li> <li>Indicates how many GPUs should be visible by the MLCube.</li> <li>MLCubes use Docker containers under the hood. Here, you can provide an image tag to the image that will be created by this MLCube. You should use a valid name that allows you to upload it to a Docker registry.</li> </ol> <p>After filling the configuration options, the following directory structure will be generated:</p> <pre><code>.\n\u2514\u2500\u2500 model_mlcube\n    \u251c\u2500\u2500 mlcube\n    \u2502   \u251c\u2500\u2500 mlcube.yaml\n    \u2502   \u2514\u2500\u2500 workspace\n    \u2502       \u2514\u2500\u2500 parameters.yaml\n    \u2514\u2500\u2500 project\n        \u251c\u2500\u2500 Dockerfile\n        \u251c\u2500\u2500 mlcube.py\n        \u2514\u2500\u2500 requirements.txt\n</code></pre> <p>The next sections will go through the contents of this directory in details and customize it.</p>"},{"location":"mlcubes/mlcube_models/#the-project-folder","title":"The <code>project</code> folder","text":"<p>This is where your inference logic will live. This folder initially contains three files as shown above. The upcoming sections will cover their use in details.</p> <p>The first thing to do is put your code files in this folder.</p>"},{"location":"mlcubes/mlcube_models/#how-will-the-mlcube-identify-your-code","title":"How will the MLCube identify your code?","text":"<p>This is done through the <code>mlcube.py</code> file. This file defines the interface of the MLCube and should be linked to your inference logic.</p> mlcube.py<pre><code>\"\"\"MLCube handler file\"\"\"\nimport typer\napp = typer.Typer()\n@app.command(\"infer\")\ndef prepare(\ndata_path: str = typer.Option(..., \"--data_path\"),\nparameters_file: str = typer.Option(..., \"--parameters_file\"),\noutput_path: str = typer.Option(..., \"--output_path\"),\n# Provide additional parameters as described in the mlcube.yaml file\n# e.g. model weights:\n# weights: str = typer.Option(..., \"--weights\"),\n):\n# Modify the prepare command as needed\nraise NotImplementedError(\"The evaluate method is not yet implemented\")\n@app.command(\"hotfix\")\ndef hotfix():\n# NOOP command for typer to behave correctly. DO NOT REMOVE OR MODIFY\npass\nif __name__ == \"__main__\":\napp()\n</code></pre> <p>As shown above, this file exposes a command <code>infer</code>. It's basic arguments are the input data path, the output predictions path, and a parameters file path.</p> <p>The parameters file, as will be explained in the upcoming sections, gives flexibility to your MLCube. For example, instead of hardcoding the inference batch size in the code, it can be configured by passing a parameters file to your MLCube which contains its value. This way, your same MLCube can be reused with multiple batch sizes by just changing the input parameters file.</p> <p>You should ignore the <code>hotfix</code> command as described in the file.</p> <p>The <code>infer</code> command will be automatically called by the MLCube when it's built and run. This command should call your inference logic. Make sure you replace its contents with a code that calls your inference logic. This could be by importing a function from your code files and calling it with the necessary arguments.</p>"},{"location":"mlcubes/mlcube_models/#prepare-your-dockerfile","title":"Prepare your Dockerfile","text":"<p>The MLCube will execute a docker image whose entrypoint is <code>mlcube.py</code>. The MLCube will first build this image from the <code>Dockerfile</code> specified in the <code>project</code> folder. You can customize the Dockerfile however you want as long as the entrypoint is runs the <code>mlcube.py</code> file</p> <p>Make sure you include in your Dockerfile any system dependency your code depends on. It is also common to have <code>pip</code> dependencies, make sure you install them in the Dockerfile as well.</p> <p>Below is the docker file provided in the template:</p> Dockerfile<pre><code>FROM python:3.9.16-slim\nCOPY ./requirements.txt /mlcube_project/requirements.txt RUN pip3 install --no-cache-dir -r /mlcube_project/requirements.txt\n\nENV LANG C.UTF-8\n\nCOPY . /mlcube_project\n\nENTRYPOINT [\"python3\", \"/mlcube_project/mlcube.py\"]\n</code></pre> <p>As shown above, this docker file makes sure <code>python</code> is available by using the python base image, installs <code>pip</code> dependencies using the <code>requirements.txt</code> file, and sets the entrypoint to run <code>mlcube.py</code>. Note that the MLCube tool will invoke the Docker <code>build</code> command from the <code>project</code> folder, so it will copy all your files found in the <code>project</code> to the Docker image.</p>"},{"location":"mlcubes/mlcube_models/#the-mlcube-folder","title":"The <code>mlcube</code> folder","text":"<p>This folder is mainly for configuring your MLCube and providing additional files the MLCube may interact with, such as parameters or model weights.</p>"},{"location":"mlcubes/mlcube_models/#include-additional-input-files","title":"Include additional input files","text":""},{"location":"mlcubes/mlcube_models/#parameters","title":"parameters","text":"<p>Your inference logic may depend on some parameters (e.g. inference batch size). It is usually a more favorable design to not hardcode such parameters, but instead pass them when running the MLCube. This can be done by having a <code>parameters.yaml</code> file as an input to the MLCube. This file will be available to the <code>infer</code> command described before. You can parse this file in the <code>mlcube.py</code> file and pass its contents to your code.</p> <p>This file should be placed in the <code>mlcube/workspace</code> folder.</p>"},{"location":"mlcubes/mlcube_models/#model-weights","title":"model weights","text":"<p>It is a good practice not to ship your model weights within the docker image to reduce the image size and provide flexibility of running the MLCube with different model weights. To do this, model weights path should be provided as a separate parameter to the MLCube. You should place your model weights in a folder named <code>additional_files</code> inside the <code>mlcube/workspace</code> folder. This is how MedPerf expects any additional input to your MLCube beside the data path and the paramters file.</p> <p>After placing your model weights in <code>mlcube/workspace/additional_files</code>, you have to modify two files:</p> <ul> <li><code>mlcube.py</code>: add an argument to the <code>infer</code> command which will correspond to the path of your model weights. Remember also to pass this argument to your inference logic.</li> <li><code>mlcube.yaml</code>: The next section introduces this file and describes it in details. You should add your extra input arguments to this file as well, as described below.</li> </ul>"},{"location":"mlcubes/mlcube_models/#configure-your-mlcube","title":"Configure your MLCube","text":"<p>The <code>mlcube.yaml</code> file contains metadata and configuration of your mlcube. This file was already populated with the configuration you provided during the step of creating the template. There is no need to edit anything in this file except if you are specifying extra parameters to the <code>infer</code> command (e.g., model weights as described in the previous section).</p> <p>You will be modifying the <code>tasks</code> section of the <code>mlcube.yaml</code> file in order to account for extra additional inputs:</p> mlcube.yaml<pre><code>tasks:\ninfer:\n# Computes predictions on input data\nparameters:\ninputs: {\n          data_path: data/,\n          parameters_file: parameters.yaml,\n# Feel free to include other files required for inference.\n# These files MUST go inside the additional_files path.\n# e.g. model weights\n          # weights: additional_files/weights.pt,\n}\noutputs: { output_path: { type: directory, default: predictions } }\n</code></pre> <p>As hinted by the comments as well, you can add the additional parameters by specifying an extra key-value pair in the <code>inputs</code> dictionary of the <code>infer</code> task.</p>"},{"location":"mlcubes/mlcube_models/#build-your-mlcube","title":"Build your MLCube","text":"<p>After you follow the previous sections, the MLCube is ready to be built and run. Run the command below to build the MLCube. Make sure you are in the folder <code>model_mlcube/mlcube</code>.</p> <pre><code>mlcube configure -Pdocker.build_strategy=always\n</code></pre> <p>This command will build your docker image and make the MLCube ready to use.</p>"},{"location":"mlcubes/mlcube_models/#run-your-mlcube","title":"Run your MLCube","text":"<p>MedPerf will take care of running your MLCube. However, it's recommended to test the MLCube alone before using it with MedPerf for better debugging.</p> <p>Use the command below to run the MLCube. Make sure you are in the folder <code>model_mlcube/mlcube</code>.</p> <pre><code>mlcube run --task infer data_path=&lt;absolute path to input data&gt; output_path=&lt;absolute path to a folder where predictions will be saved&gt;\n</code></pre>"},{"location":"mlcubes/mlcube_models/#a-working-example","title":"A Working Example","text":"<p>Assume you have the codebase below. This code can be used to predict thoracic diseases based on Chest X-ray data. The classification task is modeled as a multi-label classification class.</p> models.py <pre><code>\"\"\"\nTaken from MedMNIST/MedMNIST.\n\"\"\"\nimport torch.nn as nn\nclass SimpleCNN(nn.Module):\ndef __init__(self, in_channels, num_classes):\nsuper(SimpleCNN, self).__init__()\nself.layer1 = nn.Sequential(\nnn.Conv2d(in_channels, 16, kernel_size=3), nn.BatchNorm2d(16), nn.ReLU()\n)\nself.layer2 = nn.Sequential(\nnn.Conv2d(16, 16, kernel_size=3),\nnn.BatchNorm2d(16),\nnn.ReLU(),\nnn.MaxPool2d(kernel_size=2, stride=2),\n)\nself.layer3 = nn.Sequential(\nnn.Conv2d(16, 64, kernel_size=3), nn.BatchNorm2d(64), nn.ReLU()\n)\nself.layer4 = nn.Sequential(\nnn.Conv2d(64, 64, kernel_size=3), nn.BatchNorm2d(64), nn.ReLU()\n)\nself.layer5 = nn.Sequential(\nnn.Conv2d(64, 64, kernel_size=3, padding=1),\nnn.BatchNorm2d(64),\nnn.ReLU(),\nnn.MaxPool2d(kernel_size=2, stride=2),\n)\nself.fc = nn.Sequential(\nnn.Linear(64 * 4 * 4, 128),\nnn.ReLU(),\nnn.Linear(128, 128),\nnn.ReLU(),\nnn.Linear(128, num_classes),\n)\ndef forward(self, x):\nx = self.layer1(x)\nx = self.layer2(x)\nx = self.layer3(x)\nx = self.layer4(x)\nx = self.layer5(x)\nx = x.view(x.size(0), -1)\nx = self.fc(x)\nreturn x\n</code></pre> data_loader.py <pre><code>import numpy as np\nimport torchvision.transforms as transforms\nimport os\nfrom torch.utils.data import Dataset\nclass CustomImageDataset(Dataset):\ndef __init__(self, data_path):\nself.transform = transforms.Compose(\n[transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]\n)\nself.files = os.listdir(data_path)\nself.data_path = data_path\ndef __len__(self):\nreturn len(self.files)\ndef __getitem__(self, idx):\nimg_path = os.path.join(self.data_path, self.files[idx])\nimage = np.load(img_path)\nimage = self.transform(image)\nfile_id = self.files[idx].strip(\".npy\")\nreturn image, file_id\n</code></pre> infer.py <pre><code>import torch\nfrom models import SimpleCNN\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom data_loader import CustomImageDataset\nfrom pprint import pprint\ndata_path = \"path/to/data/folder\"\nweights = \"path/to/weights.pt\"\nin_channels = 1\nnum_classes = 14\nbatch_size = 5\n# load model\nmodel = SimpleCNN(in_channels=in_channels, num_classes=num_classes)\nmodel.load_state_dict(torch.load(weights))\nmodel.eval()\n# load prepared data\ndataset = CustomImageDataset(data_path)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n# inference\npredictions_dict = {}\nwith torch.no_grad():\nfor images, files_ids in tqdm(dataloader):\noutputs = model(images)\noutputs = torch.nn.Sigmoid()(outputs)\noutputs = outputs.detach().numpy()\nfor file_id, output in zip(files_ids, outputs):\npredictions_dict[file_id] = output\npprint(predictions_dict)\n</code></pre> <p>Throughout the next sections, this code will be wrapped within an MLCube.</p>"},{"location":"mlcubes/mlcube_models/#before-building-the-mlcube_1","title":"Before Building the MLCube","text":"<p>The guidlines listed previously in this section will now be applied to the given codebase. Assume that you were instructed by the benchmark you are participating with to have your MLCube interface as follows:</p> <ul> <li>The MLCube should expect the input data folder to contain a list of images as numpy files.</li> <li>The MLCube should save the predictions in a single JSON file as key-value pairs of image file ID and its corresponding prediction. A prediction should be a vector of length 14 (number of classes) and has to be the output of the Sigmoid activation layer.</li> </ul> <p>It is important to make sure that your MLCube will output an expected predictions format and consume a defined data format, since it will be used in a benchmarking pipeline whose data input is fixed and whose metrics calculation logic expects a fixed predictions format.</p> <p>Considering the codebase above, here are the things that should be done before proceeding to build the MLCube:</p> <ul> <li><code>infer.py</code> only prints predictions but doesn't store them. This has to be changed.</li> <li><code>infer.py</code> hardcodes some parameters (<code>num_classes</code>, <code>in_channels</code>, <code>batch_size</code>) as well as the path to the trained model weights. Consider making these items configurable parameters. (This is optional but recommended)</li> <li>Consider refactoring <code>infer.py</code> to be a function so that is can be easily called by <code>mlcube.py</code>.</li> </ul> <p>The other files <code>models.py</code> and <code>data_loader.py</code> seem to be good already. The data loader expects a folder containing a list of numpy arrays, as instructed.</p> <p>Here is the modified version of <code>infer.py</code> according to the points listed above:</p> infer.py (Modified) <pre><code>import torch\nimport os\nfrom models import SimpleCNN\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom data_loader import CustomImageDataset\nimport json\ndef run_inference(data_path, parameters, output_path, weights):\nin_channels = parameters[\"in_channels\"]\nnum_classes = parameters[\"num_classes\"]\nbatch_size = parameters[\"batch_size\"]\n# load model\nmodel = SimpleCNN(in_channels=in_channels, num_classes=num_classes)\nmodel.load_state_dict(torch.load(weights))\nmodel.eval()\n# load prepared data\ndataset = CustomImageDataset(data_path)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n# inference\npredictions_dict = {}\nwith torch.no_grad():\nfor images, files_ids in tqdm(dataloader):\noutputs = model(images)\noutputs = torch.nn.Sigmoid()(outputs)\noutputs = outputs.detach().numpy().tolist()\nfor file_id, output in zip(files_ids, outputs):\npredictions_dict[file_id] = output\n# save\npreds_file = os.path.join(output_path, \"predictions.json\")\nwith open(preds_file, \"w\") as f:\njson.dump(predictions_dict, f, indent=4)\n</code></pre>"},{"location":"mlcubes/mlcube_models/#create-an-mlcube-template","title":"Create an MLCube Template","text":"<p>Assuming you installed MedPerf, run the following:</p> <pre><code>medperf mlcube create model\n</code></pre> <p>You will be prompted to fill in the configuration options. Use the following configuration as a reference:</p> <pre><code>project_name [Model MLCube]: Custom CNN Classification Model\nproject_slug [model_mlcube]: model_custom_cnn\ndescription [Model MLCube Template. Provided by MLCommons]: MedPerf Tutorial - Model MLCube.\nauthor_name [John Smith]: &lt;use your name&gt;\naccelerator_count [0]: 0\ndocker_image_name [docker/image:latest]: repository/model-tutorial:0.0.0\n</code></pre> <p>Note</p> <p>This example is built to be used with a CPU. See the last section to know how to configure this example with a GPU.</p> <p>Note that <code>docker_image_name</code> is arbitrarily chosen. Use a valid docker image.</p>"},{"location":"mlcubes/mlcube_models/#move-your-codebase","title":"Move your Codebase","text":"<p>Move the three files of the codebase to the <code>project</code> folder. The directory tree will then look like this:</p> <pre><code>.\n\u2514\u2500\u2500 model_custom_cnn\n    \u251c\u2500\u2500 mlcube\n    \u2502   \u251c\u2500\u2500 mlcube.yaml\n    \u2502   \u2514\u2500\u2500 workspace\n    \u2502       \u2514\u2500\u2500 parameters.yaml\n    \u2514\u2500\u2500 project\n        \u251c\u2500\u2500 Dockerfile\n        \u251c\u2500\u2500 mlcube.py\n        \u251c\u2500\u2500 models.py\n        \u251c\u2500\u2500 data_loader.py\n        \u251c\u2500\u2500 infer.py\n        \u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"mlcubes/mlcube_models/#add-your-parameters-and-model-weights","title":"Add your parameters and model weights","text":"<p>Since <code>num_classes</code>, <code>in_channels</code>, and <code>batch_size</code> are now parametrized, they should be defined in <code>workspace/parameters.yaml</code>. Also, the model weights should be placed inside <code>workspace/additional_files</code>.</p>"},{"location":"mlcubes/mlcube_models/#add-parameters","title":"Add parameters","text":"<p>Modify <code>parameters.yaml</code> to include the following:</p> parameters.yaml<pre><code>in_channels: 1\nnum_classes: 14\nbatch_size: 5\n</code></pre>"},{"location":"mlcubes/mlcube_models/#add-model-weights","title":"Add model weights","text":"<p>Download the following model weights to use in this example: Click here to Download</p> <p>Extract the file to <code>workspace/additional_files</code>. The directory tree should look like this:</p> <pre><code>.\n\u2514\u2500\u2500 model_custom_cnn\n    \u251c\u2500\u2500 mlcube\n    \u2502   \u251c\u2500\u2500 mlcube.yaml\n    \u2502   \u2514\u2500\u2500 workspace\n    \u2502       \u251c\u2500\u2500 additional_files\n    \u2502       \u2502   \u2514\u2500\u2500 cnn_weights.pth\n    \u2502       \u2514\u2500\u2500 parameters.yaml\n    \u2514\u2500\u2500 project\n        \u251c\u2500\u2500 Dockerfile\n        \u251c\u2500\u2500 mlcube.py\n        \u251c\u2500\u2500 models.py\n        \u251c\u2500\u2500 data_loader.py\n        \u251c\u2500\u2500 infer.py\n        \u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"mlcubes/mlcube_models/#modify-mlcubepy","title":"Modify <code>mlcube.py</code>","text":"<p>Next, the inference logic should be triggered from <code>mlcube.py</code>. The <code>parameters_file</code> will be read in <code>mlcube.py</code> and passed as a dictionary to the inference logic. Also, an extra parameter <code>weights</code> is added to the function signature which will correspond to the model weights path. See below the modified <code>mlcube.py</code> file.</p> mlcube.py (Modified) <pre><code>\"\"\"MLCube handler file\"\"\"\nimport typer\nimport yaml\nfrom infer import run_inference\napp = typer.Typer()\n@app.command(\"infer\")\ndef infer(\ndata_path: str = typer.Option(..., \"--data_path\"),\nparameters_file: str = typer.Option(..., \"--parameters_file\"),\noutput_path: str = typer.Option(..., \"--output_path\"),\nweights: str = typer.Option(..., \"--weights\"),\n):\nwith open(parameters_file) as f:\nparameters = yaml.safe_load(f)\nrun_inference(data_path, parameters, output_path, weights)\n@app.command(\"hotfix\")\ndef hotfix():\n# NOOP command for typer to behave correctly. DO NOT REMOVE OR MODIFY\npass\nif __name__ == \"__main__\":\napp()\n</code></pre>"},{"location":"mlcubes/mlcube_models/#prepare-the-dockerfile","title":"Prepare the Dockerfile","text":"<p>The provided Dockerfile in the template is enough and preconfigured to download <code>pip</code> dependencies from the <code>requirements.txt</code> file. All that is needed is to modify the <code>requirements.txt</code> file to include the project's pip dependencies.</p> requirements.txt<pre><code>typer==0.9.0\nnumpy==1.24.3\nPyYAML==6.0\ntorch==2.0.1\ntorchvision==0.15.2\ntqdm==4.65.0\n--extra-index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"mlcubes/mlcube_models/#modify-mlcubeyaml","title":"Modify <code>mlcube.yaml</code>","text":"<p>Since the extra parameter <code>weights</code> was added to the <code>infer</code> task in <code>mlcube.py</code>, this has to be reflected on the defined MLCube interface in the <code>mlcube.yaml</code> file. Modify the <code>tasks</code> section to include an extra input parameter: <code>weights: additional_files/cnn_weights.pth</code>.</p> <p>Tip</p> <p>The MLCube tool interprets these paths as relative to the <code>workspace</code>.</p> <p>The <code>tasks</code> section will then look like this:</p> mlcube.yaml<pre><code>tasks:\ninfer:\n# Computes predictions on input data\nparameters:\ninputs:\n{\n          data_path: data/,\n          parameters_file: parameters.yaml,\n          weights: additional_files/cnn_weights.pth,\n}\noutputs: { output_path: { type: directory, default: predictions } }\n</code></pre>"},{"location":"mlcubes/mlcube_models/#build-your-mlcube_1","title":"Build your MLCube","text":"<p>Run the command below to create the MLCube. Make sure you are in the folder <code>model_custom_cnn/mlcube</code>.</p> <pre><code>mlcube configure -Pdocker.build_strategy=always\n</code></pre> <p>This command will build your docker image and make the MLCube ready to use.</p> <p>Tip</p> <p>Run <code>docker image ls</code> to see your built Docker image.</p>"},{"location":"mlcubes/mlcube_models/#run-your-mlcube_1","title":"Run your MLCube","text":"<p>Download a sample data to run on: Click here to Download</p> <p>Extract the data. You will get a folder <code>sample_prepared_data</code> containing a list chest X-ray images as numpy arrays.</p> <p>Use the command below to run the MLCube. Make sure you are in the the folder <code>model_custom_cnn/mlcube</code>.</p> <pre><code>mlcube run --task infer data_path=&lt;absolute path to `sample_prepared_data`&gt; output_path=&lt;absolute path to a folder where predictions will be saved&gt;\n</code></pre>"},{"location":"mlcubes/mlcube_models/#using-the-example-with-gpus","title":"Using the Example with GPUs","text":"<p>The provided example codebase runs only on CPU. You can modify it to have <code>pytorch</code> run inference on a GPU.</p> <p>The general instructions for building an MLCube to work with a GPU are the same as the provided instructions, but with the following slight modifications:</p> <ul> <li>Use a number different than <code>0</code> for the <code>accelerator_count</code> that you will be prompted with when creating the MLCube template.</li> <li>Inside the <code>docker</code> section of the <code>mlcube.yaml</code>, add a key value pair: <code>gpu_args: --gpus=all</code>. These <code>gpu_args</code> will be passed to <code>docker run</code> under the hood by MLCube. You may add more than just <code>--gpus=all</code>.</li> <li>Make sure you install the required GPU dependencies in the docker image. For instance, this may be done by simply modifying the <code>pip</code> dependencies in the <code>requirements.txt</code> file to download <code>pytorch</code> with cuda, or by changing the base image of the dockerfile.</li> </ul>"},{"location":"mlcubes/mlcubes/","title":"MedPerf MLCubes","text":"<p>MLCube is a set of common conventions for creating Machine Learning (ML) software that can \"plug-and-play\" on many different systems. It is basically a container image with a simple interface and the correct metadata that allows researchers and developers to easily share and experiment with ML pipelines.</p> <p>You can read more about MLCubes here.</p> <p>In MedPerf, MLCubes are required for creating the three technical components of a benchmarking experiment: the data preparation flow, the model inference flow, and the evaluation flow. A Benchmark Committee will be required to create three MLCubes that these components. A Model Owner will be required to wrap their model code within an MLCube in order to submit it to the MedPerf server and participate in a benchmark.</p> <p>MLCubes are general-purpose. MedPerf defines three specific design types of MLCubes according to their purpose: The Data Preparator MLCube, the Model MLCube, and the Metrics MLCube. Each type has a specific MLCube task configuration that defines the MLCube's interface. Users need to follow these design specs when building their MLCubes to be conforming with MedPerf. We provide below a high-level description of each MLCube type and a link to a guide for building an example for each type.</p>"},{"location":"mlcubes/mlcubes/#data-preparator-mlcube","title":"Data Preparator MLCube","text":"<p>The Data Preparator MLCube is used to prepare the data for executing the benchmark. Ideally, it can receive different data standards for the task at hand, transforming them into a single, unified standard. Additionally, it ensures the quality and compatibility of the data and computes statistics and metadata for registration purposes.</p> <p>This MLCube's interface should expose the following tasks:</p> <ul> <li> <p>Prepare: Transforms the input data into the expected output data standard. It receives as input the location of the original data, as well as the location of the labels, and outputs the prepared dataset and accompanying labels.</p> </li> <li> <p>Sanity check: Ensures the integrity of the prepared data. It may check for anomalies and data corruption (e.g. blank images, empty test cases). It constitutes a set of conditions the prepared data should comply with.</p> </li> <li> <p>Statistics: Computes statistics on the prepared data.</p> </li> </ul> <p>Check this guide on how to create a Data Preparation MLCube.</p>"},{"location":"mlcubes/mlcubes/#model-mlcube","title":"Model MLCube","text":"<p>The model MLCube contains a pre-trained machine learning model that is going to be evaluated by the benchmark. It's interface should expose the following task:</p> <ul> <li>Infer: Obtains predictions on the prepared data. It receives as input the location of the prepared data and outputs the predictions.</li> </ul> <p>Check this guide on how to create a Model MLCube.</p>"},{"location":"mlcubes/mlcubes/#metricsevaluator-mlcube","title":"Metrics/Evaluator MLCube","text":"<p>The Metrics MLCube is used for computing metrics on the model predictions by comparing them against the provided labels. It's interface should expose the following task:</p> <ul> <li>Evaluate: Computes the metrics. It receives as input the location of the predictions and the location of the prepared data labels and generates a yaml file containing the metrics.</li> </ul> <p>Check this guide on how to create a Metrics MLCube.</p>"},{"location":"mlcubes/shared/build/","title":"Build","text":""},{"location":"mlcubes/shared/build/#building-a-no-such-element-dict-objectname","title":"Building a {{ no such element: dict object['name'] }}","text":"<p>The following section will describe how you can create a {{ no such element: dict object['name'] }} from scratch. This documentation goes through the set of commands provided to help during this process, as well as the contents of a {{ no such element: dict object['name'] }}.</p>"},{"location":"mlcubes/shared/build/#setup","title":"Setup","text":"<p>MedPerf provides some cookiecutter templates for all the related MLCubes. Additionally, it provides commands to easily retreive and use these templates. For that, you need to make sure MedPerf is installed. If you haven not done so, please follow the steps below:</p> <ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/mlcommons/medperf\ncd medperf\n</code></pre></p> </li> <li> <p>Install the MedPerf CLI:     <pre><code>pip install -e cli\n</code></pre></p> </li> <li> <p>If you have not done so, create a folder for keeping all MLCubes created in this tutorial:     <pre><code>mkdir tutorial\ncd tutorial\n</code></pre></p> </li> <li> <p>Create a {{ no such element: dict object['name'] }} through MedPerf:     <pre><code>medperf mlcube create {{ no such element: dict object['slug'] }}\n</code></pre>     You should be prompted to fill in some configuration options through the CLI. An example of some good options to provide for this specific task is presented below:</p> </li> </ol>"},{"location":"mlcubes/shared/contents/","title":"Contents","text":""},{"location":"mlcubes/shared/contents/#contents","title":"Contents","text":"<p>Let's have a look at what the previous command generated. First, lets look at the whole folder structure: <pre><code>tree </code></pre></p>"},{"location":"mlcubes/shared/cookiecutter/","title":"Cookiecutter","text":"<p>Note</p> <p>MedPerf is running CookieCutter under the hood. This medperf command provides additional arguments for handling different scenarios. You can see more information on this by running <code>medperf mlcube create --help</code></p>"},{"location":"mlcubes/shared/docker_file/","title":"Docker file","text":""},{"location":"mlcubes/shared/docker_file/#projectdockerfile","title":"project/Dockerfile","text":"<p>MLCubes rely on containers to work. By default, Medperf provides a functional Dockerfile, which uses <code>ubuntu:18.0.4</code> and <code>python3.6</code>. This Dockerfile handles all the required procedures to install your project and redirect commands to the <code>project/mlcube.py</code> file. You can modify as you see fit, as long as the entry point behaves as a CLI, as described before.</p> <p>Running Docker MLCubes with Singularity</p> <p>If you are building a Docker MLCube and expect it to be also run using Singularity, you need to keep in mind that Singularity containers built from Docker images ignore the <code>WORKDIR</code> instruction if used in Dockerfiles. Make sure you also follow their best practices for writing Singularity-compatible Dockerfiles.</p>"},{"location":"mlcubes/shared/execute/","title":"Execute","text":""},{"location":"mlcubes/shared/execute/#execute","title":"Execute","text":"<p>Now its time to run our own implementation. We won't go into much detail, since we covered the basics before. But, here are the commands you can run to build and run your MLCube.</p> <ol> <li>Go to the MLCube folder. For this, assuming you are in the root of the <code>{{ no such element: dict object['slug'] }}_mlcube</code>, run     <pre><code>cd mlcube\n</code></pre></li> <li> <p>Build the Docker image using the shortcuts provided by MLCubse. Here is how you can do it:     <pre><code>mlcube configure -Pdocker.build_strategy=always # (1)!\n</code></pre></p> <ol> <li>MLCube by default will look for the image on Docker hub or locally instead of building it. Providing <code>Pdocker.build_strategy=always</code> enforces MLCube to build the image from source.</li> </ol> </li> </ol>"},{"location":"mlcubes/shared/hello_world/","title":"Hello world","text":"<p>In order to provide a basic example of how Medperf MLCubes work under the hood, a toy Hello World benchmark is provided. This benchmark implements a pipeline for ingesting people's names and generating greetings for those names given some criteria. Although this is not the most scientific example, it provides a clear idea of all the pieces required to implement your MLCubes for Medperf.</p> <p>You can find the {{ no such element: dict object['name'] }} code here</p>"},{"location":"mlcubes/shared/hotfix/","title":"Hotfix","text":"<p>What is the <code>hotfix</code> function inside <code>mlcube.py</code>?</p> <p>To summarize, this issue is benign and can be safely ignored. It prevents a potential issue with the CLI and does not require further action.</p> <p>If you use the <code>typer</code>/<code>click</code> library for your command-line interface (CLI) and have only one <code>@app.command</code>, the command line may not be parsed as expected by mlcube. This is due to a known issue that can be resolved by adding more than one task to the mlcube interface.</p> <p>To avoid a potential issue with the CLI, we add a dummy typer command to our model cubes that only have one task. If you're not using <code>typer</code>/<code>click</code>, you don't need this dummy command.</p>"},{"location":"mlcubes/shared/requirements/","title":"Requirements","text":""},{"location":"mlcubes/shared/requirements/#projectrequirementstxt","title":"project/requirements.txt","text":"<p>The provided MLCube template assumes your project is python based. Because of this, it provides a <code>requirements.txt</code> file to specify the dependencies to run your project. This file is automatically used by the <code>Dockerfile</code> to install and set up your project. Since some dependencies are necessary, let's add them to the file:</p>"},{"location":"mlcubes/shared/setup/","title":"Setup","text":""},{"location":"mlcubes/shared/setup/#how-to-run","title":"How to run","text":"<p>Before digging into the code, let's try manually running the {{ no such element: dict object['name'] }}. During this process, it should be possible to see how MLCube interacts with the folders in the workspace and what is expected to happen during each step:</p>"},{"location":"mlcubes/shared/setup/#setup","title":"Setup","text":"<ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/mlcommons/medperf\ncd medperf\n</code></pre></p> </li> <li> <p>Install mlcube and mlcube-docker using pip:     <pre><code>pip install mlcube mlcube-docker\n</code></pre></p> </li> <li> <p>Navigate to the HelloWorld directory within the examples folder with     <pre><code>cd examples/HelloWorld\n</code></pre></p> </li> <li> <p>Change to the current example's <code>mlcube</code> folder with     <pre><code>cd {{ no such element: dict object['slug'] }}/mlcube\n</code></pre></p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>_version</li> <li>account_management</li> <li>commands<ul> <li>association<ul> <li>approval</li> <li>association</li> <li>list</li> <li>priority</li> </ul> </li> <li>auth<ul> <li>auth</li> <li>login</li> <li>logout</li> <li>status</li> <li>synapse_login</li> </ul> </li> <li>benchmark<ul> <li>associate</li> <li>benchmark</li> <li>submit</li> </ul> </li> <li>compatibility_test<ul> <li>compatibility_test</li> <li>run</li> <li>utils</li> <li>validate_params</li> </ul> </li> <li>dataset<ul> <li>associate</li> <li>create</li> <li>dataset</li> <li>submit</li> </ul> </li> <li>execution</li> <li>list</li> <li>mlcube<ul> <li>associate</li> <li>create</li> <li>mlcube</li> <li>submit</li> </ul> </li> <li>profile</li> <li>result<ul> <li>create</li> <li>result</li> <li>submit</li> </ul> </li> <li>view</li> </ul> </li> <li>comms<ul> <li>auth<ul> <li>auth0</li> <li>interface</li> <li>local</li> <li>token_verifier</li> </ul> </li> <li>entity_resources<ul> <li>resources</li> <li>sources<ul> <li>direct</li> <li>source</li> <li>synapse</li> </ul> </li> <li>utils</li> </ul> </li> <li>factory</li> <li>interface</li> <li>rest</li> </ul> </li> <li>config</li> <li>config_managment</li> <li>decorators</li> <li>entities<ul> <li>benchmark</li> <li>cube</li> <li>dataset</li> <li>interface</li> <li>report</li> <li>result</li> <li>schemas</li> </ul> </li> <li>enums</li> <li>exceptions</li> <li>ui<ul> <li>cli</li> <li>factory</li> <li>interface</li> <li>stdin</li> </ul> </li> <li>utils</li> </ul>"},{"location":"reference/_version/","title":"version","text":""},{"location":"reference/account_management/","title":"Account management","text":""},{"location":"reference/account_management/#account_management.get_medperf_user_data","title":"<code>get_medperf_user_data()</code>","text":"<p>Return cached medperf user data. Get from the server if not found</p> Source code in <code>cli/medperf/account_management.py</code> <pre><code>def get_medperf_user_data():\n\"\"\"Return cached medperf user data. Get from the server if not found\"\"\"\nconfig_p = read_config()\nif config.credentials_keyword not in config_p.active_profile:\nraise MedperfException(\"You are not logged in\")\nmedperf_user = config_p.active_profile[config.credentials_keyword].get(\n\"medperf_user\", None\n)\nif medperf_user is None:\nmedperf_user = set_medperf_user_data()\nreturn medperf_user\n</code></pre>"},{"location":"reference/account_management/#account_management.set_medperf_user_data","title":"<code>set_medperf_user_data()</code>","text":"<p>Get and cache user data from the MedPerf server</p> Source code in <code>cli/medperf/account_management.py</code> <pre><code>def set_medperf_user_data():\n\"\"\"Get and cache user data from the MedPerf server\"\"\"\nconfig_p = read_config()\nmedperf_user = config.comms.get_current_user()\nconfig_p.active_profile[config.credentials_keyword][\"medperf_user\"] = medperf_user\nwrite_config(config_p)\nreturn medperf_user\n</code></pre>"},{"location":"reference/config/","title":"Config","text":""},{"location":"reference/config_managment/","title":"Config managment","text":""},{"location":"reference/decorators/","title":"Decorators","text":""},{"location":"reference/decorators/#decorators.clean_except","title":"<code>clean_except(func)</code>","text":"<p>Decorator for handling errors. It allows logging and cleaning the project's directory before throwing the error.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to handle for errors</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Decorated function</p> Source code in <code>cli/medperf/decorators.py</code> <pre><code>def clean_except(func: Callable) -&gt; Callable:\n\"\"\"Decorator for handling errors. It allows logging\n    and cleaning the project's directory before throwing the error.\n    Args:\n        func (Callable): Function to handle for errors\n    Returns:\n        Callable: Decorated function\n    \"\"\"\n@functools.wraps(func)\ndef wrapper(*args, **kwargs):\ntry:\nlogging.info(f\"Running function '{func.__name__}'\")\nfunc(*args, **kwargs)\nexcept CleanExit as e:\nlogging.info(str(e))\nconfig.ui.print(str(e))\nexcept MedperfException as e:\nlogging.exception(e)\npretty_error(str(e))\nsys.exit(1)\nexcept Exception as e:\nlogging.error(\"An unexpected error occured. Terminating.\")\nlogging.exception(e)\nraise e\nfinally:\ncleanup()\nreturn wrapper\n</code></pre>"},{"location":"reference/decorators/#decorators.configurable","title":"<code>configurable(func)</code>","text":"<p>Decorator that adds common configuration options to a typer command</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function to be decorated</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>decorated function</p> Source code in <code>cli/medperf/decorators.py</code> <pre><code>def configurable(func: Callable) -&gt; Callable:\n\"\"\"Decorator that adds common configuration options to a typer command\n    Args:\n        func (Callable): function to be decorated\n    Returns:\n        Callable: decorated function\n    \"\"\"\n# initialize config if it is not yet initialized\ninit_config()\n# Set profile parameters\nconfig_p = read_config()\nset_custom_config(config_p.active_profile)\n# NOTE: changing parameters here should be accompanied\n#       by changing configurable_parameters\n@merge_args(func)\ndef wrapper(\n*args,\nserver: str = typer.Option(\nconfig.server, \"--server\", help=\"URL of a hosted MedPerf API instance\"\n),\nauth_class: str = typer.Option(\nconfig.auth_class,\n\"--auth_class\",\nhelp=\"Authentication interface to use [Auth0]\",\n),\nauth_domain: str = typer.Option(\nconfig.auth_domain, \"--auth_domain\", help=\"Auth0 domain name\"\n),\nauth_jwks_url: str = typer.Option(\nconfig.auth_jwks_url, \"--auth_jwks_url\", help=\"Auth0 Json Web Key set URL\"\n),\nauth_idtoken_issuer: str = typer.Option(\nconfig.auth_idtoken_issuer,\n\"--auth_idtoken_issuer\",\nhelp=\"Auth0 ID token issuer\",\n),\nauth_client_id: str = typer.Option(\nconfig.auth_client_id, \"--auth_client_id\", help=\"Auth0 client ID\"\n),\nauth_audience: str = typer.Option(\nconfig.auth_audience,\n\"--auth_audience\",\nhelp=\"Server's Auth0 API identifier\",\n),\ncertificate: str = typer.Option(\nconfig.certificate, \"--certificate\", help=\"path to a valid SSL certificate\"\n),\ncomms: str = typer.Option(\nconfig.comms, \"--comms\", help=\"communications interface to use. [REST]\"\n),\nui: str = typer.Option(config.ui, \"--ui\", help=\"UI interface to use. [CLI]\"),\nloglevel: str = typer.Option(\nconfig.loglevel,\n\"--loglevel\",\nhelp=\"Logging level [debug | info | warning | error]\",\n),\nprepare_timeout: int = typer.Option(\nconfig.prepare_timeout,\n\"--prepare_timeout\",\nhelp=\"Maximum time in seconds before interrupting prepare task\",\n),\nsanity_check_timeout: int = typer.Option(\nconfig.sanity_check_timeout,\n\"--sanity_check_timeout\",\nhelp=\"Maximum time in seconds before interrupting sanity_check task\",\n),\nstatistics_timeout: int = typer.Option(\nconfig.statistics_timeout,\n\"--statistics_timeout\",\nhelp=\"Maximum time in seconds before interrupting statistics task\",\n),\ninfer_timeout: int = typer.Option(\nconfig.infer_timeout,\n\"--infer_timeout\",\nhelp=\"Maximum time in seconds before interrupting infer task\",\n),\nevaluate_timeout: int = typer.Option(\nconfig.evaluate_timeout,\n\"--evaluate_timeout\",\nhelp=\"Maximum time in seconds before interrupting evaluate task\",\n),\nplatform: str = typer.Option(\nconfig.platform,\n\"--platform\",\nhelp=\"Platform to use for MLCube. [docker | singularity]\",\n),\ngpus: str = typer.Option(\nconfig.gpus,\n\"--gpus\",\nhelp=\"\"\"\n            What GPUs to expose to MLCube.\n            Accepted Values are comma separated GPU IDs (e.g \"1,2\"), or \\\"all\\\".\n            MLCubes that aren't configured to use GPUs won't be affected by this.\n            Defaults to all available GPUs\"\"\",\n),\ncleanup: bool = typer.Option(\nconfig.cleanup,\n\"--cleanup/--no-cleanup\",\nhelp=\"Wether to clean up temporary medperf storage after execution\",\n),\n**kwargs,\n):\nreturn func(*args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/enums/","title":"Enums","text":""},{"location":"reference/exceptions/","title":"Exceptions","text":""},{"location":"reference/exceptions/#exceptions.CleanExit","title":"<code>CleanExit</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when Medperf needs to stop for non erroneous reasons</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CleanExit(MedperfException):\n\"\"\"Raised when Medperf needs to stop for non erroneous reasons\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationAuthenticationError","title":"<code>CommunicationAuthenticationError</code>","text":"<p>             Bases: <code>CommunicationError</code></p> <p>Raised when the communication interface can't handle an authentication request</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationAuthenticationError(CommunicationError):\n\"\"\"Raised when the communication interface can't handle an authentication request\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationError","title":"<code>CommunicationError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an error happens due to the communication interface</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationError(MedperfException):\n\"\"\"Raised when an error happens due to the communication interface\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationRequestError","title":"<code>CommunicationRequestError</code>","text":"<p>             Bases: <code>CommunicationError</code></p> <p>Raised when the communication interface can't handle a request appropiately</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationRequestError(CommunicationError):\n\"\"\"Raised when the communication interface can't handle a request appropiately\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationRetrievalError","title":"<code>CommunicationRetrievalError</code>","text":"<p>             Bases: <code>CommunicationError</code></p> <p>Raised when the communication interface can't retrieve an element</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationRetrievalError(CommunicationError):\n\"\"\"Raised when the communication interface can't retrieve an element\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.ExecutionError","title":"<code>ExecutionError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an execution component fails</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class ExecutionError(MedperfException):\n\"\"\"Raised when an execution component fails\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.InvalidArgumentError","title":"<code>InvalidArgumentError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an argument or set of arguments are consided invalid</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class InvalidArgumentError(MedperfException):\n\"\"\"Raised when an argument or set of arguments are consided invalid\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.InvalidEntityError","title":"<code>InvalidEntityError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an entity is considered invalid</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class InvalidEntityError(MedperfException):\n\"\"\"Raised when an entity is considered invalid\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.MedperfException","title":"<code>MedperfException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Medperf base exception</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class MedperfException(Exception):\n\"\"\"Medperf base exception\"\"\"\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#utils.approval_prompt","title":"<code>approval_prompt(msg)</code>","text":"<p>Helper function for prompting the user for things they have to explicitly approve.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>What message to ask the user for approval.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Wether the user explicitly approved or not.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def approval_prompt(msg: str) -&gt; bool:\n\"\"\"Helper function for prompting the user for things they have to explicitly approve.\n    Args:\n        msg (str): What message to ask the user for approval.\n    Returns:\n        bool: Wether the user explicitly approved or not.\n    \"\"\"\nlogging.info(\"Prompting for user's approval\")\nui = config.ui\napproval = None\nwhile approval is None or approval not in \"yn\":\napproval = ui.prompt(msg.strip() + \" \").lower()\nlogging.info(f\"User answered approval with {approval}\")\nreturn approval == \"y\"\n</code></pre>"},{"location":"reference/utils/#utils.base_storage_path","title":"<code>base_storage_path(subpath)</code>","text":"<p>Helper function that converts a path to base storage-related path</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def base_storage_path(subpath: str):\n\"\"\"Helper function that converts a path to base storage-related path\"\"\"\nreturn os.path.join(config.storage, subpath)\n</code></pre>"},{"location":"reference/utils/#utils.cleanup","title":"<code>cleanup()</code>","text":"<p>Removes clutter and unused files from the medperf folder structure.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def cleanup():\n\"\"\"Removes clutter and unused files from the medperf folder structure.\"\"\"\nif not config.cleanup:\nlogging.info(\"Cleanup disabled\")\nreturn\ntmp_storage = storage_path(config.tmp_storage)\nfor path in config.tmp_paths + [tmp_storage]:\nremove_path(path)\ntrash_folder = base_storage_path(config.trash_folder)\nif os.path.exists(trash_folder):\nmsg = \"WARNING: Failed to premanently cleanup some files. Consider deleting\"\nmsg += f\" '{trash_folder}' manually to avoid unnecessary storage.\"\nconfig.ui.print_warning(msg)\n</code></pre>"},{"location":"reference/utils/#utils.combine_proc_sp_text","title":"<code>combine_proc_sp_text(proc)</code>","text":"<p>Combines the output of a process and the spinner. Joins any string captured from the process with the spinner current text. Any strings ending with any other character from the subprocess will be returned later.</p> <p>Parameters:</p> Name Type Description Default <code>proc</code> <code>spawn</code> <p>a pexpect spawned child</p> required <code>ui</code> <code>UI</code> <p>An instance of an UI implementation</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>all non-carriage-return-ending string captured from proc</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def combine_proc_sp_text(proc: spawn) -&gt; str:\n\"\"\"Combines the output of a process and the spinner.\n    Joins any string captured from the process with the\n    spinner current text. Any strings ending with any other\n    character from the subprocess will be returned later.\n    Args:\n        proc (spawn): a pexpect spawned child\n        ui (UI): An instance of an UI implementation\n    Returns:\n        str: all non-carriage-return-ending string captured from proc\n    \"\"\"\nui = config.ui\nstatic_text = ui.text\nproc_out = \"\"\nbreak_ = False\nwhile not break_:\nif not proc.isalive():\nbreak_ = True\ntry:\nline = proc.readline()\nexcept TIMEOUT:\nlogging.error(\"Process timed out\")\nraise ExecutionError(\"Process timed out\")\nline = line.decode(\"utf-8\", \"ignore\")\nif line:\nproc_out += line\nui.print(f\"{Fore.WHITE}{Style.DIM}{line.strip()}{Style.RESET_ALL}\")\nui.text = static_text\nreturn proc_out\n</code></pre>"},{"location":"reference/utils/#utils.dict_pretty_print","title":"<code>dict_pretty_print(in_dict, skip_none_values=True)</code>","text":"<p>Helper function for distinctively printing dictionaries with yaml format.</p> <p>Parameters:</p> Name Type Description Default <code>in_dict</code> <code>dict</code> <p>dictionary to print</p> required Source code in <code>cli/medperf/utils.py</code> <pre><code>def dict_pretty_print(in_dict: dict, skip_none_values: bool = True):\n\"\"\"Helper function for distinctively printing dictionaries with yaml format.\n    Args:\n        in_dict (dict): dictionary to print\n    \"\"\"\nlogging.debug(f\"Printing dictionary to the user: {in_dict}\")\nui = config.ui\nui.print()\nui.print(\"=\" * 20)\nif skip_none_values:\nin_dict = {k: v for (k, v) in in_dict.items() if v is not None}\nui.print(yaml.dump(in_dict))\nlogging.debug(f\"Dictionary printed to the user: {in_dict}\")\nui.print(\"=\" * 20)\n</code></pre>"},{"location":"reference/utils/#utils.format_errors_dict","title":"<code>format_errors_dict(errors_dict)</code>","text":"<p>Reformats the error details from a field-error(s) dictionary into a human-readable string for printing</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def format_errors_dict(errors_dict: dict):\n\"\"\"Reformats the error details from a field-error(s) dictionary into a human-readable string for printing\"\"\"\nerror_msg = \"\"\nfor field, errors in errors_dict.items():\nerror_msg += \"\\n\"\nif isinstance(field, tuple):\nfield = field[0]\nerror_msg += f\"- {field}: \"\nif isinstance(errors, str):\nerror_msg += errors\nelif len(errors) == 1:\n# If a single error for a field is given, don't create a sublist\nerror_msg += errors[0]\nelse:\n# Create a sublist otherwise\nfor e_msg in errors:\nerror_msg += \"\\n\"\nerror_msg += f\"\\t- {e_msg}\"\nreturn error_msg\n</code></pre>"},{"location":"reference/utils/#utils.generate_tmp_path","title":"<code>generate_tmp_path()</code>","text":"<p>Generates a temporary path by means of getting the current timestamp with a random salt</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>generated temporary path</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def generate_tmp_path() -&gt; str:\n\"\"\"Generates a temporary path by means of getting the current timestamp\n    with a random salt\n    Returns:\n        str: generated temporary path\n    \"\"\"\ntmp_path = os.path.join(config.tmp_storage, generate_tmp_uid())\ntmp_path = storage_path(tmp_path)\nreturn os.path.abspath(tmp_path)\n</code></pre>"},{"location":"reference/utils/#utils.generate_tmp_uid","title":"<code>generate_tmp_uid()</code>","text":"<p>Generates a temporary uid by means of getting the current timestamp with a random salt</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>generated temporary uid</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def generate_tmp_uid() -&gt; str:\n\"\"\"Generates a temporary uid by means of getting the current timestamp\n    with a random salt\n    Returns:\n        str: generated temporary uid\n    \"\"\"\ndt = datetime.utcnow()\nts_int = int(datetime.timestamp(dt))\nsalt = random.randint(-ts_int, ts_int)\nts = str(ts_int + salt)\nreturn ts\n</code></pre>"},{"location":"reference/utils/#utils.get_cube_image_name","title":"<code>get_cube_image_name(cube_path)</code>","text":"<p>Retrieves the singularity image name of the mlcube by reading its mlcube.yaml file</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_cube_image_name(cube_path: str) -&gt; str:\n\"\"\"Retrieves the singularity image name of the mlcube by reading its mlcube.yaml file\"\"\"\ncube_config_path = os.path.join(cube_path, config.cube_filename)\nwith open(cube_config_path, \"r\") as f:\ncube_config = yaml.safe_load(f)\ntry:\nreturn cube_config[\"singularity\"][\"image\"]\nexcept KeyError:\nmsg = \"The provided mlcube doesn't seem to be configured for singularity\"\nraise MedperfException(msg)\n</code></pre>"},{"location":"reference/utils/#utils.get_file_hash","title":"<code>get_file_hash(path)</code>","text":"<p>Calculates the sha256 hash for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Location of the file of interest.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Calculated hash</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_file_hash(path: str) -&gt; str:\n\"\"\"Calculates the sha256 hash for a given file.\n    Args:\n        path (str): Location of the file of interest.\n    Returns:\n        str: Calculated hash\n    \"\"\"\nlogging.debug(\"Calculating hash for file {}\".format(path))\nBUF_SIZE = 65536\nsha = hashlib.sha256()\nwith open(path, \"rb\") as f:\nwhile True:\ndata = f.read(BUF_SIZE)\nif not data:\nbreak\nsha.update(data)\nsha_val = sha.hexdigest()\nlogging.debug(f\"Hash for file {path}: {sha_val}\")\nreturn sha_val\n</code></pre>"},{"location":"reference/utils/#utils.get_folder_hash","title":"<code>get_folder_hash(path)</code>","text":"<p>Generates a hash for all the contents of the folder. This procedure hashes all of the files in the folder, sorts them and then hashes that list.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Folder to hash</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>sha256 hash of the whole folder</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_folder_hash(path: str) -&gt; str:\n\"\"\"Generates a hash for all the contents of the folder. This procedure\n    hashes all of the files in the folder, sorts them and then hashes that list.\n    Args:\n        path (str): Folder to hash\n    Returns:\n        str: sha256 hash of the whole folder\n    \"\"\"\nhashes = []\nfor root, _, files in os.walk(path, topdown=False):\nfor file in files:\nlogging.debug(f\"Hashing file {file}\")\nfilepath = os.path.join(root, file)\nhashes.append(get_file_hash(filepath))\nhashes = sorted(hashes)\nsha = hashlib.sha256()\nfor hash in hashes:\nsha.update(hash.encode(\"utf-8\"))\nhash_val = sha.hexdigest()\nlogging.debug(f\"Folder hash: {hash_val}\")\nreturn hash_val\n</code></pre>"},{"location":"reference/utils/#utils.get_uids","title":"<code>get_uids(path)</code>","text":"<p>Retrieves the UID of all the elements in the specified path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: UIDs of objects in path.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_uids(path: str) -&gt; List[str]:\n\"\"\"Retrieves the UID of all the elements in the specified path.\n    Returns:\n        List[str]: UIDs of objects in path.\n    \"\"\"\nlogging.debug(\"Retrieving datasets\")\nuids = next(os.walk(path))[1]\nlogging.debug(f\"Found {len(uids)} datasets\")\nlogging.debug(f\"Datasets: {uids}\")\nreturn uids\n</code></pre>"},{"location":"reference/utils/#utils.init_config","title":"<code>init_config()</code>","text":"<p>builds the initial configuration file</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def init_config():\n\"\"\"builds the initial configuration file\"\"\"\nos.makedirs(config.storage, exist_ok=True)\nconfig_file = base_storage_path(config.config_path)\nif os.path.exists(config_file):\nreturn\nconfig_p = ConfigManager()\n# default profile\nconfig_p[config.default_profile_name] = default_profile()\n# testauth profile\nconfig_p[config.testauth_profile_name] = default_profile()\nconfig_p[config.testauth_profile_name][\"server\"] = config.local_server\nconfig_p[config.testauth_profile_name][\"certificate\"] = config.local_certificate\nconfig_p[config.testauth_profile_name][\"auth_audience\"] = config.auth_dev_audience\nconfig_p[config.testauth_profile_name][\"auth_domain\"] = config.auth_dev_domain\nconfig_p[config.testauth_profile_name][\"auth_jwks_url\"] = config.auth_dev_jwks_url\nconfig_p[config.testauth_profile_name][\n\"auth_idtoken_issuer\"\n] = config.auth_dev_idtoken_issuer\nconfig_p[config.testauth_profile_name][\"auth_client_id\"] = config.auth_dev_client_id\n# local profile\nconfig_p[config.test_profile_name] = default_profile()\nconfig_p[config.test_profile_name][\"server\"] = config.local_server\nconfig_p[config.test_profile_name][\"certificate\"] = config.local_certificate\nconfig_p[config.test_profile_name][\"auth_class\"] = \"Local\"\nconfig_p.activate(config.default_profile_name)\nconfig_p.write(config_file)\n</code></pre>"},{"location":"reference/utils/#utils.init_storage","title":"<code>init_storage()</code>","text":"<p>Builds the general medperf folder structure.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def init_storage():\n\"\"\"Builds the general medperf folder structure.\"\"\"\nlogging.info(\"Initializing storage\")\nparent = config.storage\ndata = storage_path(config.data_storage)\ncubes = storage_path(config.cubes_storage)\nresults = storage_path(config.results_storage)\ntmp = storage_path(config.tmp_storage)\nbmks = storage_path(config.benchmarks_storage)\ndemo = storage_path(config.demo_data_storage)\nlog = storage_path(config.logs_storage)\nimgs = base_storage_path(config.images_storage)\ntests = storage_path(config.test_storage)\nexp_log = storage_path(config.experiments_logs_storage)\ndirs = [parent, bmks, data, cubes, results, tmp, demo, log, imgs, tests, exp_log]\nfor dir in dirs:\nlogging.info(f\"Creating {dir} directory\")\ntry:\nos.makedirs(dir, exist_ok=True)\nexcept FileExistsError:\nlogging.warning(f\"Tried to create existing folder {dir}\")\n</code></pre>"},{"location":"reference/utils/#utils.pretty_error","title":"<code>pretty_error(msg)</code>","text":"<p>Prints an error message with typer protocol</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Error message to show to the user</p> required Source code in <code>cli/medperf/utils.py</code> <pre><code>def pretty_error(msg: str):\n\"\"\"Prints an error message with typer protocol\n    Args:\n        msg (str): Error message to show to the user\n    \"\"\"\nui = config.ui\nlogging.warning(\n\"MedPerf had to stop execution. See logs above for more information\"\n)\nif msg[-1] != \".\":\nmsg = msg + \".\"\nui.print_error(msg)\n</code></pre>"},{"location":"reference/utils/#utils.remove_path","title":"<code>remove_path(path)</code>","text":"<p>Cleans up a clutter object. In case of failure, it is moved to <code>.trash</code></p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def remove_path(path):\n\"\"\"Cleans up a clutter object. In case of failure, it is moved to `.trash`\"\"\"\n# NOTE: We assume medperf will always have permissions to unlink\n# and rename clutter paths, since for now they are expected to live\n# in folders owned by medperf\nif not os.path.exists(path):\nreturn\nlogging.info(f\"Removing clutter path: {path}\")\n# Don't delete symlinks\nif os.path.islink(path):\nos.unlink(path)\nreturn\ntry:\nif os.path.isfile(path):\nos.remove(path)\nelse:\nshutil.rmtree(path)\nexcept OSError as e:\nlogging.error(f\"Could not remove {path}: {str(e)}\")\nmove_to_trash(path)\n</code></pre>"},{"location":"reference/utils/#utils.sanitize_json","title":"<code>sanitize_json(data)</code>","text":"<p>Makes sure the input data is JSON compliant.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>dictionary containing data to be represented as JSON.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>sanitized dictionary</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def sanitize_json(data: dict) -&gt; dict:\n\"\"\"Makes sure the input data is JSON compliant.\n    Args:\n        data (dict): dictionary containing data to be represented as JSON.\n    Returns:\n        dict: sanitized dictionary\n    \"\"\"\njson_string = json.dumps(data)\njson_string = re.sub(r\"\\bNaN\\b\", '\"nan\"', json_string)\njson_string = re.sub(r\"(-?)\\bInfinity\\b\", r'\"\\1Infinity\"', json_string)\ndata = json.loads(json_string)\nreturn data\n</code></pre>"},{"location":"reference/utils/#utils.set_custom_config","title":"<code>set_custom_config(args)</code>","text":"<p>Function to set parameters defined by the user</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>custom config params</p> required Source code in <code>cli/medperf/utils.py</code> <pre><code>def set_custom_config(args: dict):\n\"\"\"Function to set parameters defined by the user\n    Args:\n        args (dict): custom config params\n    \"\"\"\nfor param in args:\nval = args[param]\nsetattr(config, param, val)\n</code></pre>"},{"location":"reference/utils/#utils.set_unique_tmp_config","title":"<code>set_unique_tmp_config()</code>","text":"<p>Set current process' temporary unique storage Enables simultaneous execution without cleanup collision</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def set_unique_tmp_config():\n\"\"\"Set current process' temporary unique storage\n    Enables simultaneous execution without cleanup collision\n    \"\"\"\npid = str(os.getpid())\nconfig.tmp_storage += pid\nconfig.trash_folder = os.path.join(config.trash_folder, pid)\n</code></pre>"},{"location":"reference/utils/#utils.storage_path","title":"<code>storage_path(subpath)</code>","text":"<p>Helper function that converts a path to deployment storage-related path</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def storage_path(subpath: str):\n\"\"\"Helper function that converts a path to deployment storage-related path\"\"\"\nserver_path = config.server.split(\"//\")[1]\nserver_path = re.sub(r\"[.:]\", \"_\", server_path)\nreturn os.path.join(config.storage, server_path, subpath)\n</code></pre>"},{"location":"reference/utils/#utils.untar","title":"<code>untar(filepath, remove=True)</code>","text":"<p>Untars and optionally removes the tar.gz file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where the tar.gz file can be found.</p> required <code>remove</code> <code>bool</code> <p>Wether to delete the tar.gz file. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>location where the untared files can be found.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def untar(filepath: str, remove: bool = True) -&gt; str:\n\"\"\"Untars and optionally removes the tar.gz file\n    Args:\n        filepath (str): Path where the tar.gz file can be found.\n        remove (bool): Wether to delete the tar.gz file. Defaults to True.\n    Returns:\n        str: location where the untared files can be found.\n    \"\"\"\nlogging.info(f\"Uncompressing tar.gz at {filepath}\")\naddpath = str(Path(filepath).parent)\ntar = tarfile.open(filepath)\ntar.extractall(addpath)\ntar.close()\n# OS Specific issue: Mac Creates superfluous files with tarfile library\n[\nremove_path(spurious_file)\nfor spurious_file in glob(addpath + \"/**/._*\", recursive=True)\n]\nif remove:\nlogging.info(f\"Deleting {filepath}\")\nremove_path(filepath)\nreturn addpath\n</code></pre>"},{"location":"reference/utils/#utils.verify_hash","title":"<code>verify_hash(obtained_hash, expected_hash)</code>","text":"<p>Checks hash exact match, and throws an error if not a match</p> <p>Parameters:</p> Name Type Description Default <code>obtained_hash</code> <code>str</code> <p>local hash computed from asset</p> required <code>expected_hash</code> <code>str</code> <p>expected hash obtained externally</p> required <p>Raises:</p> Type Description <code>InvalidEntityError</code> <p>Thrown if hashes don't match</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def verify_hash(obtained_hash: str, expected_hash: str):\n\"\"\"Checks hash exact match, and throws an error if not a match\n    Args:\n        obtained_hash (str): local hash computed from asset\n        expected_hash (str): expected hash obtained externally\n    Raises:\n        InvalidEntityError: Thrown if hashes don't match\n    \"\"\"\nif expected_hash and expected_hash != obtained_hash:\nraise InvalidEntityError(\nf\"Hash mismatch. Expected {expected_hash}, found {obtained_hash}.\"\n)\n</code></pre>"},{"location":"reference/commands/execution/","title":"Execution","text":""},{"location":"reference/commands/execution/#commands.execution.Execution","title":"<code>Execution</code>","text":"Source code in <code>cli/medperf/commands/execution.py</code> <pre><code>class Execution:\n@classmethod\ndef run(\ncls, dataset: Dataset, model: Cube, evaluator: Cube, ignore_model_errors=False\n):\n\"\"\"Benchmark execution flow.\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n            data_uid (str): Registered Dataset UID\n            model_uid (int): UID of model to execute\n        \"\"\"\nexecution = cls(dataset, model, evaluator, ignore_model_errors)\nexecution.prepare()\nwith execution.ui.interactive():\nexecution.run_inference()\nexecution.run_evaluation()\nexecution_summary = execution.todict()\nreturn execution_summary\ndef __init__(\nself, dataset: Dataset, model: Cube, evaluator: Cube, ignore_model_errors=False\n):\nself.comms = config.comms\nself.ui = config.ui\nself.dataset = dataset\nself.model = model\nself.evaluator = evaluator\nself.ignore_model_errors = ignore_model_errors\ndef prepare(self):\nself.partial = False\nself.preds_path = self.__setup_predictions_path()\nself.model_logs_path, self.metrics_logs_path = self.__setup_logs_path()\nself.results_path = generate_tmp_path()\nlogging.debug(f\"tmp results output: {self.results_path}\")\ndef __setup_logs_path(self):\nmodel_uid = self.model.generated_uid\neval_uid = self.evaluator.generated_uid\ndata_hash = self.dataset.generated_uid\nlogs_path = os.path.join(\nconfig.experiments_logs_storage, str(model_uid), str(data_hash)\n)\nlogs_path = storage_path(logs_path)\nos.makedirs(logs_path, exist_ok=True)\nmodel_logs_path = os.path.join(logs_path, \"model.log\")\nmetrics_logs_path = os.path.join(logs_path, f\"metrics_{eval_uid}.log\")\nreturn model_logs_path, metrics_logs_path\ndef __setup_predictions_path(self):\nmodel_uid = self.model.generated_uid\ndata_hash = self.dataset.generated_uid\npreds_path = os.path.join(\nconfig.predictions_storage, str(model_uid), str(data_hash)\n)\npreds_path = storage_path(preds_path)\nif os.path.exists(preds_path):\nmsg = f\"Found existing predictions for model {self.model.id} on dataset \"\nmsg += f\"{self.dataset.id} at {preds_path}. Consider deleting this \"\nmsg += \"folder if you wish to overwrite the predictions.\"\nraise ExecutionError(msg)\nreturn preds_path\ndef run_inference(self):\nself.ui.text = \"Running model inference on dataset\"\ninfer_timeout = config.infer_timeout\npreds_path = self.preds_path\ndata_path = self.dataset.data_path\ntry:\nself.model.run(\ntask=\"infer\",\noutput_logs=self.model_logs_path,\ntimeout=infer_timeout,\ndata_path=data_path,\noutput_path=preds_path,\nstring_params={\"Ptasks.infer.parameters.input.data_path.opts\": \"ro\"},\n)\nself.ui.print(\"&gt; Model execution complete\")\nexcept ExecutionError as e:\nif not self.ignore_model_errors:\nlogging.error(f\"Model MLCube Execution failed: {e}\")\nraise ExecutionError(\"Model MLCube failed\")\nelse:\nself.partial = True\nlogging.warning(f\"Model MLCube Execution failed: {e}\")\ndef run_evaluation(self):\nself.ui.text = \"Running model evaluation on dataset\"\nevaluate_timeout = config.evaluate_timeout\npreds_path = self.preds_path\nlabels_path = self.dataset.labels_path\nresults_path = self.results_path\nself.ui.text = \"Evaluating results\"\ntry:\nself.evaluator.run(\ntask=\"evaluate\",\noutput_logs=self.metrics_logs_path,\ntimeout=evaluate_timeout,\npredictions=preds_path,\nlabels=labels_path,\noutput_path=results_path,\nstring_params={\n\"Ptasks.evaluate.parameters.input.predictions.opts\": \"ro\",\n\"Ptasks.evaluate.parameters.input.labels.opts\": \"ro\",\n},\n)\nexcept ExecutionError as e:\nlogging.error(f\"Metrics MLCube Execution failed: {e}\")\nraise ExecutionError(\"Metrics MLCube failed\")\ndef todict(self):\nreturn {\n\"results\": self.get_results(),\n\"partial\": self.partial,\n}\ndef get_results(self):\nwith open(self.results_path, \"r\") as f:\nresults = yaml.safe_load(f)\nreturn results\n</code></pre>"},{"location":"reference/commands/execution/#commands.execution.Execution.run","title":"<code>run(dataset, model, evaluator, ignore_model_errors=False)</code>  <code>classmethod</code>","text":"<p>Benchmark execution flow.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <code>data_uid</code> <code>str</code> <p>Registered Dataset UID</p> required <code>model_uid</code> <code>int</code> <p>UID of model to execute</p> required Source code in <code>cli/medperf/commands/execution.py</code> <pre><code>@classmethod\ndef run(\ncls, dataset: Dataset, model: Cube, evaluator: Cube, ignore_model_errors=False\n):\n\"\"\"Benchmark execution flow.\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n        data_uid (str): Registered Dataset UID\n        model_uid (int): UID of model to execute\n    \"\"\"\nexecution = cls(dataset, model, evaluator, ignore_model_errors)\nexecution.prepare()\nwith execution.ui.interactive():\nexecution.run_inference()\nexecution.run_evaluation()\nexecution_summary = execution.todict()\nreturn execution_summary\n</code></pre>"},{"location":"reference/commands/list/","title":"List","text":""},{"location":"reference/commands/list/#commands.list.EntityList","title":"<code>EntityList</code>","text":"Source code in <code>cli/medperf/commands/list.py</code> <pre><code>class EntityList:\n@staticmethod\ndef run(\nentity_class,\nfields,\nlocal_only: bool = False,\nmine_only: bool = False,\n**kwargs,\n):\n\"\"\"Lists all local datasets\n        Args:\n            local_only (bool, optional): Display all local results. Defaults to False.\n            mine_only (bool, optional): Display all current-user results. Defaults to False.\n            kwargs (dict): Additional parameters for filtering entity lists.\n        \"\"\"\nentity_list = EntityList(entity_class, fields, local_only, mine_only, **kwargs)\nentity_list.prepare()\nentity_list.validate()\nentity_list.filter()\nentity_list.display()\ndef __init__(self, entity_class, fields, local_only, mine_only, **kwargs):\nself.entity_class = entity_class\nself.fields = fields\nself.local_only = local_only\nself.mine_only = mine_only\nself.filters = kwargs\nself.data = []\ndef prepare(self):\nif self.mine_only:\nself.filters[\"owner\"] = get_medperf_user_data()[\"id\"]\nentities = self.entity_class.all(\nlocal_only=self.local_only, filters=self.filters\n)\nself.data = [entity.display_dict() for entity in entities]\ndef validate(self):\nif self.data:\nvalid_fields = set(self.data[0].keys())\nchosen_fields = set(self.fields)\nif not chosen_fields.issubset(valid_fields):\ninvalid_fields = chosen_fields.difference(valid_fields)\ninvalid_fields = \", \".join(invalid_fields)\nraise InvalidArgumentError(f\"Invalid field(s): {invalid_fields}\")\ndef filter(self):\nself.data = [\n{field: entity_dict[field] for field in self.fields}\nfor entity_dict in self.data\n]\ndef display(self):\nheaders = self.fields\ndata_lists = [list(entity_dict.values()) for entity_dict in self.data]\ntab = tabulate(data_lists, headers=headers)\nconfig.ui.print(tab)\n</code></pre>"},{"location":"reference/commands/list/#commands.list.EntityList.run","title":"<code>run(entity_class, fields, local_only=False, mine_only=False, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Lists all local datasets</p> <p>Parameters:</p> Name Type Description Default <code>local_only</code> <code>bool</code> <p>Display all local results. Defaults to False.</p> <code>False</code> <code>mine_only</code> <code>bool</code> <p>Display all current-user results. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>dict</code> <p>Additional parameters for filtering entity lists.</p> <code>{}</code> Source code in <code>cli/medperf/commands/list.py</code> <pre><code>@staticmethod\ndef run(\nentity_class,\nfields,\nlocal_only: bool = False,\nmine_only: bool = False,\n**kwargs,\n):\n\"\"\"Lists all local datasets\n    Args:\n        local_only (bool, optional): Display all local results. Defaults to False.\n        mine_only (bool, optional): Display all current-user results. Defaults to False.\n        kwargs (dict): Additional parameters for filtering entity lists.\n    \"\"\"\nentity_list = EntityList(entity_class, fields, local_only, mine_only, **kwargs)\nentity_list.prepare()\nentity_list.validate()\nentity_list.filter()\nentity_list.display()\n</code></pre>"},{"location":"reference/commands/profile/","title":"Profile","text":""},{"location":"reference/commands/profile/#commands.profile.activate","title":"<code>activate(profile)</code>","text":"<p>Assigns the active profile, which is used by default</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>Name of the profile to be used.</p> required Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"activate\")\n@clean_except\ndef activate(profile: str):\n\"\"\"Assigns the active profile, which is used by default\n    Args:\n        profile (str): Name of the profile to be used.\n    \"\"\"\nconfig_p = read_config()\nif profile not in config_p:\nraise InvalidArgumentError(\"The provided profile does not exists\")\nconfig_p.activate(profile)\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.create","title":"<code>create(ctx, name=typer.Option(..., '--name', '-n', help=\"Profile's name\"))</code>","text":"<p>Creates a new profile for managing and customizing configuration</p> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"create\")\n@clean_except\n@configurable\ndef create(\nctx: typer.Context,\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Profile's name\"),\n):\n\"\"\"Creates a new profile for managing and customizing configuration\"\"\"\nargs = ctx.params\nargs.pop(\"name\")\nconfig_p = read_config()\nif name in config_p:\nraise InvalidArgumentError(\"A profile with the same name already exists\")\nconfig_p[name] = args\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.delete","title":"<code>delete(profile)</code>","text":"<p>Deletes a profile's configuration.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>Profile to delete.</p> required Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"delete\")\n@clean_except\ndef delete(profile: str):\n\"\"\"Deletes a profile's configuration.\n    Args:\n        profile (str): Profile to delete.\n    \"\"\"\nconfig_p = read_config()\nif profile not in config_p.profiles:\nraise InvalidArgumentError(\"The provided profile does not exists\")\nif profile in [\nconfig.default_profile_name,\nconfig.testauth_profile_name,\nconfig.test_profile_name,\n]:\nraise InvalidArgumentError(\"Cannot delete reserved profiles\")\nif config_p.is_profile_active(profile):\nraise InvalidArgumentError(\"Cannot delete a currently activated profile\")\ndel config_p[profile]\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.list","title":"<code>list()</code>","text":"<p>Lists all available profiles</p> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list():\n\"\"\"Lists all available profiles\"\"\"\nui = config.ui\nconfig_p = read_config()\nfor profile in config_p:\nif config_p.is_profile_active(profile):\nui.print_highlight(\"* \" + profile)\nelse:\nui.print(\"  \" + profile)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.set_args","title":"<code>set_args(ctx)</code>","text":"<p>Assign key-value configuration pairs to the current profile.</p> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"set\")\n@clean_except\n@configurable\ndef set_args(ctx: typer.Context):\n\"\"\"Assign key-value configuration pairs to the current profile.\"\"\"\nargs = ctx.params\nconfig_p = read_config()\nconfig_p.active_profile.update(args)\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.view","title":"<code>view(profile=typer.Argument(None))</code>","text":"<p>Displays a profile's configuration.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>Profile to display information from. Defaults to active profile.</p> <code>typer.Argument(None)</code> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(profile: str = typer.Argument(None)):\n\"\"\"Displays a profile's configuration.\n    Args:\n        profile (str, optional): Profile to display information from. Defaults to active profile.\n    \"\"\"\nconfig_p = read_config()\nprofile_config = config_p.active_profile\nif profile:\nprofile_config = config_p[profile]\nprofile_config.pop(config.credentials_keyword, None)\nprofile_name = profile if profile else config_p.active_profile_name\nconfig.ui.print(f\"\\nProfile '{profile_name}':\")\ndict_pretty_print(profile_config, skip_none_values=False)\n</code></pre>"},{"location":"reference/commands/view/","title":"View","text":""},{"location":"reference/commands/view/#commands.view.EntityView","title":"<code>EntityView</code>","text":"Source code in <code>cli/medperf/commands/view.py</code> <pre><code>class EntityView:\n@staticmethod\ndef run(\nentity_id: Union[int, str],\nentity_class: Entity,\nformat: str = \"yaml\",\nlocal_only: bool = False,\nmine_only: bool = False,\noutput: str = None,\n**kwargs,\n):\n\"\"\"Displays the contents of a single or multiple entities of a given type\n        Args:\n            entity_id (Union[int, str]): Entity identifies\n            entity_class (Entity): Entity type\n            local_only (bool, optional): Display all local entities. Defaults to False.\n            mine_only (bool, optional): Display all current-user entities. Defaults to False.\n            format (str, optional): What format to use to display the contents. Valid formats: [yaml, json]. Defaults to yaml.\n            output (str, optional): Path to a file for storing the entity contents. If not provided, the contents are printed.\n            kwargs (dict): Additional parameters for filtering entity lists.\n        \"\"\"\nentity_view = EntityView(\nentity_id, entity_class, format, local_only, mine_only, output, **kwargs\n)\nentity_view.validate()\nentity_view.prepare()\nif output is None:\nentity_view.display()\nelse:\nentity_view.store()\ndef __init__(\nself, entity_id, entity_class, format, local_only, mine_only, output, **kwargs\n):\nself.entity_id = entity_id\nself.entity_class = entity_class\nself.format = format\nself.local_only = local_only\nself.mine_only = mine_only\nself.output = output\nself.filters = kwargs\nself.data = []\ndef validate(self):\nvalid_formats = set([\"yaml\", \"json\"])\nif self.format not in valid_formats:\nraise InvalidArgumentError(\"The provided format is not supported\")\ndef prepare(self):\nif self.entity_id is not None:\nentities = [self.entity_class.get(self.entity_id)]\nelse:\nif self.mine_only:\nself.filters[\"owner\"] = get_medperf_user_data()[\"id\"]\nentities = self.entity_class.all(\nlocal_only=self.local_only, filters=self.filters\n)\nself.data = [entity.todict() for entity in entities]\nif self.entity_id is not None:\n# User expects a single entity if id provided\n# Don't output the view as a list of entities\nself.data = self.data[0]\ndef display(self):\nif self.format == \"json\":\nformatter = json.dumps\nif self.format == \"yaml\":\nformatter = yaml.dump\nformatted_data = formatter(self.data)\nconfig.ui.print(formatted_data)\ndef store(self):\nif self.format == \"json\":\nformatter = json.dump\nif self.format == \"yaml\":\nformatter = yaml.dump\nwith open(self.output, \"w\") as f:\nformatter(self.data, f)\n</code></pre>"},{"location":"reference/commands/view/#commands.view.EntityView.run","title":"<code>run(entity_id, entity_class, format='yaml', local_only=False, mine_only=False, output=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Displays the contents of a single or multiple entities of a given type</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>Union[int, str]</code> <p>Entity identifies</p> required <code>entity_class</code> <code>Entity</code> <p>Entity type</p> required <code>local_only</code> <code>bool</code> <p>Display all local entities. Defaults to False.</p> <code>False</code> <code>mine_only</code> <code>bool</code> <p>Display all current-user entities. Defaults to False.</p> <code>False</code> <code>format</code> <code>str</code> <p>What format to use to display the contents. Valid formats: [yaml, json]. Defaults to yaml.</p> <code>'yaml'</code> <code>output</code> <code>str</code> <p>Path to a file for storing the entity contents. If not provided, the contents are printed.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Additional parameters for filtering entity lists.</p> <code>{}</code> Source code in <code>cli/medperf/commands/view.py</code> <pre><code>@staticmethod\ndef run(\nentity_id: Union[int, str],\nentity_class: Entity,\nformat: str = \"yaml\",\nlocal_only: bool = False,\nmine_only: bool = False,\noutput: str = None,\n**kwargs,\n):\n\"\"\"Displays the contents of a single or multiple entities of a given type\n    Args:\n        entity_id (Union[int, str]): Entity identifies\n        entity_class (Entity): Entity type\n        local_only (bool, optional): Display all local entities. Defaults to False.\n        mine_only (bool, optional): Display all current-user entities. Defaults to False.\n        format (str, optional): What format to use to display the contents. Valid formats: [yaml, json]. Defaults to yaml.\n        output (str, optional): Path to a file for storing the entity contents. If not provided, the contents are printed.\n        kwargs (dict): Additional parameters for filtering entity lists.\n    \"\"\"\nentity_view = EntityView(\nentity_id, entity_class, format, local_only, mine_only, output, **kwargs\n)\nentity_view.validate()\nentity_view.prepare()\nif output is None:\nentity_view.display()\nelse:\nentity_view.store()\n</code></pre>"},{"location":"reference/commands/association/approval/","title":"Approval","text":""},{"location":"reference/commands/association/approval/#commands.association.approval.Approval","title":"<code>Approval</code>","text":"Source code in <code>cli/medperf/commands/association/approval.py</code> <pre><code>class Approval:\n@staticmethod\ndef run(\nbenchmark_uid: int,\napproval_status: str,\ndataset_uid: int = None,\nmlcube_uid: int = None,\n):\n\"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube\n        Args:\n            benchmark_uid (int): Benchmark UID.\n            approval_status (str): Desired approval status to set for the association.\n            comms (Comms): Instance of Comms interface.\n            ui (UI): Instance of UI interface.\n            dataset_uid (int, optional): Dataset UID. Defaults to None.\n            mlcube_uid (int, optional): MLCube UID. Defaults to None.\n        \"\"\"\ncomms = config.comms\ntoo_many_resources = dataset_uid and mlcube_uid\nno_resource = dataset_uid is None and mlcube_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\"Must provide either a dataset or mlcube\")\nif dataset_uid:\ncomms.set_dataset_association_approval(\nbenchmark_uid, dataset_uid, approval_status.value\n)\nif mlcube_uid:\ncomms.set_mlcube_association_approval(\nbenchmark_uid, mlcube_uid, approval_status.value\n)\n</code></pre>"},{"location":"reference/commands/association/approval/#commands.association.approval.Approval.run","title":"<code>run(benchmark_uid, approval_status, dataset_uid=None, mlcube_uid=None)</code>  <code>staticmethod</code>","text":"<p>Sets approval status for an association between a benchmark and a dataset or mlcube</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> required <code>approval_status</code> <code>str</code> <p>Desired approval status to set for the association.</p> required <code>comms</code> <code>Comms</code> <p>Instance of Comms interface.</p> required <code>ui</code> <code>UI</code> <p>Instance of UI interface.</p> required <code>dataset_uid</code> <code>int</code> <p>Dataset UID. Defaults to None.</p> <code>None</code> <code>mlcube_uid</code> <code>int</code> <p>MLCube UID. Defaults to None.</p> <code>None</code> Source code in <code>cli/medperf/commands/association/approval.py</code> <pre><code>@staticmethod\ndef run(\nbenchmark_uid: int,\napproval_status: str,\ndataset_uid: int = None,\nmlcube_uid: int = None,\n):\n\"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        approval_status (str): Desired approval status to set for the association.\n        comms (Comms): Instance of Comms interface.\n        ui (UI): Instance of UI interface.\n        dataset_uid (int, optional): Dataset UID. Defaults to None.\n        mlcube_uid (int, optional): MLCube UID. Defaults to None.\n    \"\"\"\ncomms = config.comms\ntoo_many_resources = dataset_uid and mlcube_uid\nno_resource = dataset_uid is None and mlcube_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\"Must provide either a dataset or mlcube\")\nif dataset_uid:\ncomms.set_dataset_association_approval(\nbenchmark_uid, dataset_uid, approval_status.value\n)\nif mlcube_uid:\ncomms.set_mlcube_association_approval(\nbenchmark_uid, mlcube_uid, approval_status.value\n)\n</code></pre>"},{"location":"reference/commands/association/association/","title":"Association","text":""},{"location":"reference/commands/association/association/#commands.association.association.approve","title":"<code>approve(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='Benchmark UID'), dataset_uid=typer.Option(None, '--dataset', '-d', help='Dataset UID'), mlcube_uid=typer.Option(None, '--mlcube', '-m', help='MLCube UID'))</code>","text":"<p>Approves an association between a benchmark and a dataset or model mlcube</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>typer.Option(..., '--benchmark', '-b', help='Benchmark UID')</code> <code>dataset_uid</code> <code>int</code> <p>Dataset UID.</p> <code>typer.Option(None, '--dataset', '-d', help='Dataset UID')</code> <code>mlcube_uid</code> <code>int</code> <p>Model MLCube UID.</p> <code>typer.Option(None, '--mlcube', '-m', help='MLCube UID')</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"approve\")\n@clean_except\ndef approve(\nbenchmark_uid: int = typer.Option(..., \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\ndataset_uid: int = typer.Option(None, \"--dataset\", \"-d\", help=\"Dataset UID\"),\nmlcube_uid: int = typer.Option(None, \"--mlcube\", \"-m\", help=\"MLCube UID\"),\n):\n\"\"\"Approves an association between a benchmark and a dataset or model mlcube\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        dataset_uid (int, optional): Dataset UID.\n        mlcube_uid (int, optional): Model MLCube UID.\n    \"\"\"\nApproval.run(benchmark_uid, Status.APPROVED, dataset_uid, mlcube_uid)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/association/association/#commands.association.association.list","title":"<code>list(filter=typer.Argument(None))</code>","text":"<p>Display all associations related to the current user.</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>str</code> <p>Filter associations by approval status. Defaults to displaying all user associations.</p> <code>typer.Argument(None)</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(filter: Optional[str] = typer.Argument(None)):\n\"\"\"Display all associations related to the current user.\n    Args:\n        filter (str, optional): Filter associations by approval status.\n            Defaults to displaying all user associations.\n    \"\"\"\nListAssociations.run(filter)\n</code></pre>"},{"location":"reference/commands/association/association/#commands.association.association.reject","title":"<code>reject(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='Benchmark UID'), dataset_uid=typer.Option(None, '--dataset', '-d', help='Dataset UID'), mlcube_uid=typer.Option(None, '--mlcube', '-m', help='MLCube UID'))</code>","text":"<p>Rejects an association between a benchmark and a dataset or model mlcube</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>typer.Option(..., '--benchmark', '-b', help='Benchmark UID')</code> <code>dataset_uid</code> <code>int</code> <p>Dataset UID.</p> <code>typer.Option(None, '--dataset', '-d', help='Dataset UID')</code> <code>mlcube_uid</code> <code>int</code> <p>Model MLCube UID.</p> <code>typer.Option(None, '--mlcube', '-m', help='MLCube UID')</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"reject\")\n@clean_except\ndef reject(\nbenchmark_uid: int = typer.Option(..., \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\ndataset_uid: int = typer.Option(None, \"--dataset\", \"-d\", help=\"Dataset UID\"),\nmlcube_uid: int = typer.Option(None, \"--mlcube\", \"-m\", help=\"MLCube UID\"),\n):\n\"\"\"Rejects an association between a benchmark and a dataset or model mlcube\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        dataset_uid (int, optional): Dataset UID.\n        mlcube_uid (int, optional): Model MLCube UID.\n    \"\"\"\nApproval.run(benchmark_uid, Status.REJECTED, dataset_uid, mlcube_uid)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/association/association/#commands.association.association.set_priority","title":"<code>set_priority(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='Benchmark UID'), mlcube_uid=typer.Option(..., '--mlcube', '-m', help='MLCube UID'), priority=typer.Option(..., '--priority', '-p', help='Priority, an integer'))</code>","text":"<p>Updates the priority of a benchmark-model association. Model priorities within a benchmark define which models need to be executed before others when this benchmark is run. A model with a higher priority is executed before a model with lower priority. The order of execution of models of the same priority is arbitrary.</p> <p>Examples:</p> <p>Assume there are three models of IDs (1,2,3), associated with a certain benchmark, all having priority = 0. - By setting the priority of model (2) to the value of 1, the client will make sure that model (2) is executed before models (1,3). - By setting the priority of model (1) to the value of -5, the client will make sure that models (2,3) are executed before model (1).</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>typer.Option(..., '--benchmark', '-b', help='Benchmark UID')</code> <code>mlcube_uid</code> <code>int</code> <p>Model MLCube UID.</p> <code>typer.Option(..., '--mlcube', '-m', help='MLCube UID')</code> <code>priority</code> <code>int</code> <p>Priority, an integer</p> <code>typer.Option(..., '--priority', '-p', help='Priority, an integer')</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"set_priority\")\n@clean_except\ndef set_priority(\nbenchmark_uid: int = typer.Option(..., \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\nmlcube_uid: int = typer.Option(..., \"--mlcube\", \"-m\", help=\"MLCube UID\"),\npriority: int = typer.Option(..., \"--priority\", \"-p\", help=\"Priority, an integer\"),\n):\n\"\"\"Updates the priority of a benchmark-model association. Model priorities within\n    a benchmark define which models need to be executed before others when\n    this benchmark is run. A model with a higher priority is executed before\n    a model with lower priority. The order of execution of models of the same priority\n    is arbitrary.\n    Examples:\n    Assume there are three models of IDs (1,2,3), associated with a certain benchmark,\n    all having priority = 0.\n    - By setting the priority of model (2) to the value of 1, the client will make\n    sure that model (2) is executed before models (1,3).\n    - By setting the priority of model (1) to the value of -5, the client will make\n    sure that models (2,3) are executed before model (1).\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        mlcube_uid (int): Model MLCube UID.\n        priority (int): Priority, an integer\n    \"\"\"\nAssociationPriority.run(benchmark_uid, mlcube_uid, priority)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/association/list/","title":"List","text":""},{"location":"reference/commands/association/list/#commands.association.list.ListAssociations","title":"<code>ListAssociations</code>","text":"Source code in <code>cli/medperf/commands/association/list.py</code> <pre><code>class ListAssociations:\n@staticmethod\ndef run(filter: str = None):\n\"\"\"Get Pending association requests\"\"\"\ncomms = config.comms\nui = config.ui\ndset_assocs = comms.get_datasets_associations()\ncube_assocs = comms.get_cubes_associations()\n# Might be worth seeing if creating an association class that encapsulates\n# most of the logic here is useful\nassocs = dset_assocs + cube_assocs\nif filter:\nfilter = filter.upper()\nassocs = [assoc for assoc in assocs if assoc[\"approval_status\"] == filter]\nassocs_info = []\nfor assoc in assocs:\nassoc_info = (\nassoc.get(\"dataset\", None),\nassoc.get(\"model_mlcube\", None),\nassoc[\"benchmark\"],\nassoc[\"initiated_by\"],\nassoc[\"approval_status\"],\nassoc.get(\"priority\", None),\n# NOTE: We should find a better way to show priorities, since a priority\n# is better shown when listing cube associations only, of a specific\n# benchmark. Maybe this is resolved after we add a general filtering\n# feature to list commands.\n)\nassocs_info.append(assoc_info)\nheaders = [\n\"Dataset UID\",\n\"MLCube UID\",\n\"Benchmark UID\",\n\"Initiated by\",\n\"Status\",\n\"Priority\",\n]\ntab = tabulate(assocs_info, headers=headers)\nui.print(tab)\n</code></pre>"},{"location":"reference/commands/association/list/#commands.association.list.ListAssociations.run","title":"<code>run(filter=None)</code>  <code>staticmethod</code>","text":"<p>Get Pending association requests</p> Source code in <code>cli/medperf/commands/association/list.py</code> <pre><code>@staticmethod\ndef run(filter: str = None):\n\"\"\"Get Pending association requests\"\"\"\ncomms = config.comms\nui = config.ui\ndset_assocs = comms.get_datasets_associations()\ncube_assocs = comms.get_cubes_associations()\n# Might be worth seeing if creating an association class that encapsulates\n# most of the logic here is useful\nassocs = dset_assocs + cube_assocs\nif filter:\nfilter = filter.upper()\nassocs = [assoc for assoc in assocs if assoc[\"approval_status\"] == filter]\nassocs_info = []\nfor assoc in assocs:\nassoc_info = (\nassoc.get(\"dataset\", None),\nassoc.get(\"model_mlcube\", None),\nassoc[\"benchmark\"],\nassoc[\"initiated_by\"],\nassoc[\"approval_status\"],\nassoc.get(\"priority\", None),\n# NOTE: We should find a better way to show priorities, since a priority\n# is better shown when listing cube associations only, of a specific\n# benchmark. Maybe this is resolved after we add a general filtering\n# feature to list commands.\n)\nassocs_info.append(assoc_info)\nheaders = [\n\"Dataset UID\",\n\"MLCube UID\",\n\"Benchmark UID\",\n\"Initiated by\",\n\"Status\",\n\"Priority\",\n]\ntab = tabulate(assocs_info, headers=headers)\nui.print(tab)\n</code></pre>"},{"location":"reference/commands/association/priority/","title":"Priority","text":""},{"location":"reference/commands/association/priority/#commands.association.priority.AssociationPriority","title":"<code>AssociationPriority</code>","text":"Source code in <code>cli/medperf/commands/association/priority.py</code> <pre><code>class AssociationPriority:\n@staticmethod\ndef run(\nbenchmark_uid: int, mlcube_uid: int, priority: int,\n):\n\"\"\"Sets priority for an association between a benchmark and an mlcube\n        Args:\n            benchmark_uid (int): Benchmark UID.\n            mlcube_uid (int): MLCube UID.\n            priority (int): priority value\n        \"\"\"\nassociated_cubes = config.comms.get_benchmark_models(benchmark_uid)\nif mlcube_uid not in associated_cubes:\nraise InvalidArgumentError(\n\"The given mlcube doesn't exist or is not associated with the benchmark\"\n)\nconfig.comms.set_mlcube_association_priority(\nbenchmark_uid, mlcube_uid, priority\n)\n</code></pre>"},{"location":"reference/commands/association/priority/#commands.association.priority.AssociationPriority.run","title":"<code>run(benchmark_uid, mlcube_uid, priority)</code>  <code>staticmethod</code>","text":"<p>Sets priority for an association between a benchmark and an mlcube</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> required <code>mlcube_uid</code> <code>int</code> <p>MLCube UID.</p> required <code>priority</code> <code>int</code> <p>priority value</p> required Source code in <code>cli/medperf/commands/association/priority.py</code> <pre><code>@staticmethod\ndef run(\nbenchmark_uid: int, mlcube_uid: int, priority: int,\n):\n\"\"\"Sets priority for an association between a benchmark and an mlcube\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        mlcube_uid (int): MLCube UID.\n        priority (int): priority value\n    \"\"\"\nassociated_cubes = config.comms.get_benchmark_models(benchmark_uid)\nif mlcube_uid not in associated_cubes:\nraise InvalidArgumentError(\n\"The given mlcube doesn't exist or is not associated with the benchmark\"\n)\nconfig.comms.set_mlcube_association_priority(\nbenchmark_uid, mlcube_uid, priority\n)\n</code></pre>"},{"location":"reference/commands/auth/auth/","title":"Auth","text":""},{"location":"reference/commands/auth/auth/#commands.auth.auth.login","title":"<code>login(email=typer.Option(None, '--email', '-e', help='The email associated with your account'))</code>","text":"<p>Authenticate to be able to access the MedPerf server. A verification link will be provided and should be open in a browser to complete the login process.</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"login\")\n@clean_except\ndef login(\nemail: str = typer.Option(\nNone, \"--email\", \"-e\", help=\"The email associated with your account\"\n)\n):\n\"\"\"Authenticate to be able to access the MedPerf server. A verification link will\n    be provided and should be open in a browser to complete the login process.\"\"\"\nLogin.run(email)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/auth/auth/#commands.auth.auth.logout","title":"<code>logout()</code>","text":"<p>Revoke the currently active login state.</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"logout\")\n@clean_except\ndef logout():\n\"\"\"Revoke the currently active login state.\"\"\"\nLogout.run()\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/auth/auth/#commands.auth.auth.status","title":"<code>status()</code>","text":"<p>Shows the currently logged in user.</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"status\")\n@clean_except\ndef status():\n\"\"\"Shows the currently logged in user.\"\"\"\nStatus.run()\n</code></pre>"},{"location":"reference/commands/auth/auth/#commands.auth.auth.synapse_login","title":"<code>synapse_login(username=typer.Option(None, '--username', '-u', help='Username to login with'), password=typer.Option(None, '--password', '-p', help='Password to login with'), token=typer.Option(None, '--token', '-t', help='Personal Access Token to login with'))</code>","text":"<p>Login to the synapse server. Provide either a username and a password, or a token</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"synapse_login\")\n@clean_except\ndef synapse_login(\nusername: str = typer.Option(\nNone, \"--username\", \"-u\", help=\"Username to login with\"\n),\npassword: str = typer.Option(\nNone, \"--password\", \"-p\", help=\"Password to login with\"\n),\ntoken: str = typer.Option(\nNone, \"--token\", \"-t\", help=\"Personal Access Token to login with\"\n),\n):\n\"\"\"Login to the synapse server.\n    Provide either a username and a password, or a token\n    \"\"\"\nSynapseLogin.run(username=username, password=password, token=token)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/auth/login/","title":"Login","text":""},{"location":"reference/commands/auth/login/#commands.auth.login.Login","title":"<code>Login</code>","text":"Source code in <code>cli/medperf/commands/auth/login.py</code> <pre><code>class Login:\n@staticmethod\ndef run(email: str = None):\n\"\"\"Authenticate to be able to access the MedPerf server. A verification link will\n        be provided and should be open in a browser to complete the login process.\"\"\"\nif not email:\nemail = config.ui.prompt(\"Please type your email: \")\ntry:\nvalidate_email(email, check_deliverability=False)\nexcept EmailNotValidError as e:\nraise InvalidArgumentError(str(e))\nconfig.auth.login(email)\n</code></pre>"},{"location":"reference/commands/auth/login/#commands.auth.login.Login.run","title":"<code>run(email=None)</code>  <code>staticmethod</code>","text":"<p>Authenticate to be able to access the MedPerf server. A verification link will be provided and should be open in a browser to complete the login process.</p> Source code in <code>cli/medperf/commands/auth/login.py</code> <pre><code>@staticmethod\ndef run(email: str = None):\n\"\"\"Authenticate to be able to access the MedPerf server. A verification link will\n    be provided and should be open in a browser to complete the login process.\"\"\"\nif not email:\nemail = config.ui.prompt(\"Please type your email: \")\ntry:\nvalidate_email(email, check_deliverability=False)\nexcept EmailNotValidError as e:\nraise InvalidArgumentError(str(e))\nconfig.auth.login(email)\n</code></pre>"},{"location":"reference/commands/auth/logout/","title":"Logout","text":""},{"location":"reference/commands/auth/logout/#commands.auth.logout.Logout","title":"<code>Logout</code>","text":"Source code in <code>cli/medperf/commands/auth/logout.py</code> <pre><code>class Logout:\n@staticmethod\ndef run():\n\"\"\"Revoke the currently active login state.\"\"\"\nconfig.auth.logout()\n</code></pre>"},{"location":"reference/commands/auth/logout/#commands.auth.logout.Logout.run","title":"<code>run()</code>  <code>staticmethod</code>","text":"<p>Revoke the currently active login state.</p> Source code in <code>cli/medperf/commands/auth/logout.py</code> <pre><code>@staticmethod\ndef run():\n\"\"\"Revoke the currently active login state.\"\"\"\nconfig.auth.logout()\n</code></pre>"},{"location":"reference/commands/auth/status/","title":"Status","text":""},{"location":"reference/commands/auth/status/#commands.auth.status.Status","title":"<code>Status</code>","text":"Source code in <code>cli/medperf/commands/auth/status.py</code> <pre><code>class Status:\n@staticmethod\ndef run():\n\"\"\"Shows the currently logged in user.\"\"\"\ntry:\naccount_info = read_user_account()\nexcept MedperfException as e:\n# TODO: create a specific exception about unauthenticated client\nconfig.ui.print(str(e))\nreturn\nemail = account_info[\"email\"]\nconfig.ui.print(f\"Logged in user email address: {email}\")\n</code></pre>"},{"location":"reference/commands/auth/status/#commands.auth.status.Status.run","title":"<code>run()</code>  <code>staticmethod</code>","text":"<p>Shows the currently logged in user.</p> Source code in <code>cli/medperf/commands/auth/status.py</code> <pre><code>@staticmethod\ndef run():\n\"\"\"Shows the currently logged in user.\"\"\"\ntry:\naccount_info = read_user_account()\nexcept MedperfException as e:\n# TODO: create a specific exception about unauthenticated client\nconfig.ui.print(str(e))\nreturn\nemail = account_info[\"email\"]\nconfig.ui.print(f\"Logged in user email address: {email}\")\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/","title":"Synapse login","text":""},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin","title":"<code>SynapseLogin</code>","text":"Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>class SynapseLogin:\n@classmethod\ndef run(cls, username: str = None, password: str = None, token: str = None):\n\"\"\"Login to the Synapse server. Must be done only once.\n        \"\"\"\nif not any([username, password, token]):\nmsg = \"How do you want to login?\\n\"\nmsg += \"[1] Personal Access Token\\n\"\nmsg += \"[2] Username and Password\\n\"\nmsg += \"Select an option (1 or 2): \"\nmethod = config.ui.prompt(msg)\nif method == \"1\":\ncls.login_with_token()\nelif method == \"2\":\ncls.login_with_password()\nelse:\nraise InvalidArgumentError(\"Invalid input. Select either number 1 or 2\")\nelse:\nif token:\nif any([username, password]):\nraise InvalidArgumentError(\n\"Invalid input. Either an access token, or a username and password, should be given\"\n)\ncls.login_with_token(token)\nelse:\ncls.login_with_password(username, password)\n@classmethod\ndef login_with_password(cls, username: str = None, password: str = None):\n\"\"\"Login to the Synapse server. Must be done only once.\n        \"\"\"\nusername = username if username else config.ui.prompt(\"username: \")\npassword = password if password else config.ui.hidden_prompt(\"password: \")\nsyn = synapseclient.Synapse()\ntry:\nsyn.login(username, password, rememberMe=True)\nexcept SynapseAuthenticationError:\nraise CommunicationAuthenticationError(\"Invalid Synapse credentials\")\n@classmethod\ndef login_with_token(cls, access_token=None):\n\"\"\"Login to the Synapse server. Must be done only once.\n        \"\"\"\naccess_token = (\naccess_token if access_token else config.ui.hidden_prompt(\"access token: \")\n)\nsyn = synapseclient.Synapse()\ntry:\nsyn.login(authToken=access_token, rememberMe=True)\nexcept SynapseAuthenticationError:\nraise CommunicationAuthenticationError(\"Invalid Synapse credentials\")\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin.login_with_password","title":"<code>login_with_password(username=None, password=None)</code>  <code>classmethod</code>","text":"<p>Login to the Synapse server. Must be done only once.</p> Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>@classmethod\ndef login_with_password(cls, username: str = None, password: str = None):\n\"\"\"Login to the Synapse server. Must be done only once.\n    \"\"\"\nusername = username if username else config.ui.prompt(\"username: \")\npassword = password if password else config.ui.hidden_prompt(\"password: \")\nsyn = synapseclient.Synapse()\ntry:\nsyn.login(username, password, rememberMe=True)\nexcept SynapseAuthenticationError:\nraise CommunicationAuthenticationError(\"Invalid Synapse credentials\")\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin.login_with_token","title":"<code>login_with_token(access_token=None)</code>  <code>classmethod</code>","text":"<p>Login to the Synapse server. Must be done only once.</p> Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>@classmethod\ndef login_with_token(cls, access_token=None):\n\"\"\"Login to the Synapse server. Must be done only once.\n    \"\"\"\naccess_token = (\naccess_token if access_token else config.ui.hidden_prompt(\"access token: \")\n)\nsyn = synapseclient.Synapse()\ntry:\nsyn.login(authToken=access_token, rememberMe=True)\nexcept SynapseAuthenticationError:\nraise CommunicationAuthenticationError(\"Invalid Synapse credentials\")\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin.run","title":"<code>run(username=None, password=None, token=None)</code>  <code>classmethod</code>","text":"<p>Login to the Synapse server. Must be done only once.</p> Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>@classmethod\ndef run(cls, username: str = None, password: str = None, token: str = None):\n\"\"\"Login to the Synapse server. Must be done only once.\n    \"\"\"\nif not any([username, password, token]):\nmsg = \"How do you want to login?\\n\"\nmsg += \"[1] Personal Access Token\\n\"\nmsg += \"[2] Username and Password\\n\"\nmsg += \"Select an option (1 or 2): \"\nmethod = config.ui.prompt(msg)\nif method == \"1\":\ncls.login_with_token()\nelif method == \"2\":\ncls.login_with_password()\nelse:\nraise InvalidArgumentError(\"Invalid input. Select either number 1 or 2\")\nelse:\nif token:\nif any([username, password]):\nraise InvalidArgumentError(\n\"Invalid input. Either an access token, or a username and password, should be given\"\n)\ncls.login_with_token(token)\nelse:\ncls.login_with_password(username, password)\n</code></pre>"},{"location":"reference/commands/benchmark/associate/","title":"Associate","text":""},{"location":"reference/commands/benchmark/associate/#commands.benchmark.associate.AssociateBenchmark","title":"<code>AssociateBenchmark</code>","text":"Source code in <code>cli/medperf/commands/benchmark/associate.py</code> <pre><code>class AssociateBenchmark:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\nmodel_uid: int,\ndata_uid: int,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a dataset or model to the given benchmark\n        Args:\n            benchmark_uid (int): UID of benchmark to associate entities with\n            model_uid (int): UID of model to associate with benchmark\n            data_uid (int): UID of dataset to associate with benchmark\n            comms (Comms): Instance of Communications interface\n            ui (UI): Instance of UI interface\n            approved (bool): Skip approval step. Defaults to False\n        \"\"\"\ntoo_many_resources = data_uid and model_uid\nno_resource = data_uid is None and model_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\"Must provide either a dataset or mlcube\")\nif model_uid is not None:\nAssociateCube.run(\nmodel_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\nif data_uid is not None:\nAssociateDataset.run(\ndata_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\n</code></pre>"},{"location":"reference/commands/benchmark/associate/#commands.benchmark.associate.AssociateBenchmark.run","title":"<code>run(benchmark_uid, model_uid, data_uid, approved=False, no_cache=False)</code>  <code>classmethod</code>","text":"<p>Associates a dataset or model to the given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of benchmark to associate entities with</p> required <code>model_uid</code> <code>int</code> <p>UID of model to associate with benchmark</p> required <code>data_uid</code> <code>int</code> <p>UID of dataset to associate with benchmark</p> required <code>comms</code> <code>Comms</code> <p>Instance of Communications interface</p> required <code>ui</code> <code>UI</code> <p>Instance of UI interface</p> required <code>approved</code> <code>bool</code> <p>Skip approval step. Defaults to False</p> <code>False</code> Source code in <code>cli/medperf/commands/benchmark/associate.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\nmodel_uid: int,\ndata_uid: int,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a dataset or model to the given benchmark\n    Args:\n        benchmark_uid (int): UID of benchmark to associate entities with\n        model_uid (int): UID of model to associate with benchmark\n        data_uid (int): UID of dataset to associate with benchmark\n        comms (Comms): Instance of Communications interface\n        ui (UI): Instance of UI interface\n        approved (bool): Skip approval step. Defaults to False\n    \"\"\"\ntoo_many_resources = data_uid and model_uid\nno_resource = data_uid is None and model_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\"Must provide either a dataset or mlcube\")\nif model_uid is not None:\nAssociateCube.run(\nmodel_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\nif data_uid is not None:\nAssociateDataset.run(\ndata_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/","title":"Benchmark","text":""},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.associate","title":"<code>associate(benchmark_uid=typer.Option(..., '--benchmark_uid', '-b', help='UID of benchmark to associate with'), model_uid=typer.Option(None, '--model_uid', '-m', help='UID of model MLCube to associate'), dataset_uid=typer.Option(None, '--data_uid', '-d', help='Server UID of registered dataset to associate'), approval=typer.Option(False, '-y', help='Skip approval step'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'))</code>","text":"<p>Associates a benchmark with a given mlcube or dataset. Only one option at a time.</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark_uid\", \"-b\", help=\"UID of benchmark to associate with\"\n),\nmodel_uid: int = typer.Option(\nNone, \"--model_uid\", \"-m\", help=\"UID of model MLCube to associate\"\n),\ndataset_uid: int = typer.Option(\nNone, \"--data_uid\", \"-d\", help=\"Server UID of registered dataset to associate\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute the test even if results already exist\",\n),\n):\n\"\"\"Associates a benchmark with a given mlcube or dataset. Only one option at a time.\"\"\"\nAssociateBenchmark.run(\nbenchmark_uid, model_uid, dataset_uid, approved=approval, no_cache=no_cache\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.list","title":"<code>list(local=typer.Option(False, '--local', help='Get local benchmarks'), mine=typer.Option(False, '--mine', help='Get current-user benchmarks'))</code>","text":"<p>List benchmarks stored locally and remotely from the user</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nlocal: bool = typer.Option(False, \"--local\", help=\"Get local benchmarks\"),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user benchmarks\"),\n):\n\"\"\"List benchmarks stored locally and remotely from the user\"\"\"\nEntityList.run(\nBenchmark,\nfields=[\"UID\", \"Name\", \"Description\", \"State\", \"Approval Status\", \"Registered\"],\nlocal_only=local,\nmine_only=mine,\n)\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.run","title":"<code>run(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='UID of the desired benchmark'), data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), file=typer.Option(None, '--models-from-file', '-f', help='A file containing the model UIDs to be executed.\\n\\n        The file should contain a single line as a list of\\n\\n        comma-separated integers corresponding to the model UIDs'), ignore_model_errors=typer.Option(False, '--ignore-model-errors', help='Ignore failing model cubes, allowing for possibly submitting partial results'), no_cache=typer.Option(False, '--no-cache', help='Execute even if results already exist'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"run\")\n@clean_except\ndef run(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nfile: str = typer.Option(\nNone,\n\"--models-from-file\",\n\"-f\",\nhelp=\"\"\"A file containing the model UIDs to be executed.\\n\n        The file should contain a single line as a list of\\n\n        comma-separated integers corresponding to the model UIDs\"\"\",\n),\nignore_model_errors: bool = typer.Option(\nFalse,\n\"--ignore-model-errors\",\nhelp=\"Ignore failing model cubes, allowing for possibly submitting partial results\",\n),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute even if results already exist\",\n),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nBenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\nmodels_uids=None,\nno_cache=no_cache,\nmodels_input_file=file,\nignore_model_errors=ignore_model_errors,\nshow_summary=True,\nignore_failed_experiments=True,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the benchmark'), description=typer.Option(..., '--description', '-d', help='Description of the benchmark'), docs_url=typer.Option('', '--docs-url', '-u', help='URL to documentation'), demo_url=typer.Option('', '--demo-url', help='Identifier to download the demonstration dataset tarball file.\\n\\n        See `medperf mlcube submit --help` for more information'), demo_hash=typer.Option('', '--demo-hash', help='Hash of demonstration dataset tarball file'), data_preparation_mlcube=typer.Option(..., '--data-preparation-mlcube', '-p', help='Data Preparation MLCube UID'), reference_model_mlcube=typer.Option(..., '--reference-model-mlcube', '-m', help='Reference Model MLCube UID'), evaluator_mlcube=typer.Option(..., '--evaluator-mlcube', '-e', help='Evaluator MLCube UID'))</code>","text":"<p>Submits a new benchmark to the platform</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the benchmark\"),\ndescription: str = typer.Option(\n..., \"--description\", \"-d\", help=\"Description of the benchmark\"\n),\ndocs_url: str = typer.Option(\"\", \"--docs-url\", \"-u\", help=\"URL to documentation\"),\ndemo_url: str = typer.Option(\n\"\",\n\"--demo-url\",\nhelp=\"\"\"Identifier to download the demonstration dataset tarball file.\\n\n        See `medperf mlcube submit --help` for more information\"\"\",\n),\ndemo_hash: str = typer.Option(\n\"\", \"--demo-hash\", help=\"Hash of demonstration dataset tarball file\"\n),\ndata_preparation_mlcube: int = typer.Option(\n..., \"--data-preparation-mlcube\", \"-p\", help=\"Data Preparation MLCube UID\"\n),\nreference_model_mlcube: int = typer.Option(\n..., \"--reference-model-mlcube\", \"-m\", help=\"Reference Model MLCube UID\"\n),\nevaluator_mlcube: int = typer.Option(\n..., \"--evaluator-mlcube\", \"-e\", help=\"Evaluator MLCube UID\"\n),\n):\n\"\"\"Submits a new benchmark to the platform\"\"\"\nbenchmark_info = {\n\"name\": name,\n\"description\": description,\n\"docs_url\": docs_url,\n\"demo_dataset_tarball_url\": demo_url,\n\"demo_dataset_tarball_hash\": demo_hash,\n\"data_preparation_mlcube\": data_preparation_mlcube,\n\"reference_model_mlcube\": reference_model_mlcube,\n\"data_evaluator_mlcube\": evaluator_mlcube,\n}\nSubmitBenchmark.run(benchmark_info)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.view","title":"<code>view(entity_id=typer.Argument(None, help='Benchmark ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), local=typer.Option(False, '--local', help='Display local benchmarks if benchmark ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user benchmarks if benchmark ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more benchmarks</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"Benchmark ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nlocal: bool = typer.Option(\nFalse,\n\"--local\",\nhelp=\"Display local benchmarks if benchmark ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user benchmarks if benchmark ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more benchmarks\"\"\"\nEntityView.run(entity_id, Benchmark, format, local, mine, output)\n</code></pre>"},{"location":"reference/commands/benchmark/submit/","title":"Submit","text":""},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark","title":"<code>SubmitBenchmark</code>","text":"Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>class SubmitBenchmark:\n@classmethod\ndef run(cls, benchmark_info: dict, no_cache: bool = True):\n\"\"\"Submits a new cube to the medperf platform\n        Args:\n            benchmark_info (dict): benchmark information\n                expected keys:\n                    name (str): benchmark name\n                    description (str): benchmark description\n                    docs_url (str): benchmark documentation url\n                    demo_url (str): benchmark demo dataset url\n                    demo_hash (str): benchmark demo dataset hash\n                    data_preparation_mlcube (int): benchmark data preparation mlcube uid\n                    reference_model_mlcube (int): benchmark reference model mlcube uid\n                    evaluator_mlcube (int): benchmark data evaluator mlcube uid\n        \"\"\"\nui = config.ui\nsubmission = cls(benchmark_info, no_cache)\nwith ui.interactive():\nui.text = \"Getting additional information\"\nsubmission.get_extra_information()\nui.print(\"&gt; Completed benchmark registration information\")\nui.text = \"Submitting Benchmark to MedPerf\"\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.to_permanent_path(updated_benchmark_body)\nsubmission.write(updated_benchmark_body)\ndef __init__(self, benchmark_info: dict, no_cache: bool = True):\nself.ui = config.ui\nself.bmk = Benchmark(**benchmark_info)\nself.no_cache = no_cache\nconfig.tmp_paths.append(self.bmk.path)\ndef get_extra_information(self):\n\"\"\"Retrieves information that must be populated automatically,\n        like hash, generated uid and test results\n        \"\"\"\nbmk_demo_url = self.bmk.demo_dataset_tarball_url\nbmk_demo_hash = self.bmk.demo_dataset_tarball_hash\ntry:\n_, demo_hash = resources.get_benchmark_demo_dataset(bmk_demo_url, bmk_demo_hash)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Demo dataset {bmk_demo_url}: {e}\")\nself.bmk.demo_dataset_tarball_hash = demo_hash\ndemo_uid, results = self.run_compatibility_test()\nself.bmk.demo_dataset_generated_uid = demo_uid\nself.bmk.metadata = {\"results\": results}\ndef run_compatibility_test(self):\n\"\"\"Runs a compatibility test to ensure elements are compatible,\n        and to extract additional information required for submission\n        \"\"\"\nself.ui.print(\"Running compatibility test\")\nself.bmk.write()\ndata_uid, results = CompatibilityTestExecution.run(\nbenchmark=self.bmk.generated_uid, no_cache=self.no_cache\n)\nreturn data_uid, results\ndef submit(self):\nupdated_body = self.bmk.upload()\nreturn updated_body\ndef to_permanent_path(self, bmk_dict: dict):\n\"\"\"Renames the temporary benchmark submission to a permanent one\n        Args:\n            bmk_dict (dict): dictionary containing updated information of the submitted benchmark\n        \"\"\"\nold_bmk_loc = self.bmk.path\nupdated_bmk = Benchmark(**bmk_dict)\nnew_bmk_loc = updated_bmk.path\nremove_path(new_bmk_loc)\nos.rename(old_bmk_loc, new_bmk_loc)\ndef write(self, updated_body):\nbmk = Benchmark(**updated_body)\nbmk.write()\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.get_extra_information","title":"<code>get_extra_information()</code>","text":"<p>Retrieves information that must be populated automatically, like hash, generated uid and test results</p> Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>def get_extra_information(self):\n\"\"\"Retrieves information that must be populated automatically,\n    like hash, generated uid and test results\n    \"\"\"\nbmk_demo_url = self.bmk.demo_dataset_tarball_url\nbmk_demo_hash = self.bmk.demo_dataset_tarball_hash\ntry:\n_, demo_hash = resources.get_benchmark_demo_dataset(bmk_demo_url, bmk_demo_hash)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Demo dataset {bmk_demo_url}: {e}\")\nself.bmk.demo_dataset_tarball_hash = demo_hash\ndemo_uid, results = self.run_compatibility_test()\nself.bmk.demo_dataset_generated_uid = demo_uid\nself.bmk.metadata = {\"results\": results}\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.run","title":"<code>run(benchmark_info, no_cache=True)</code>  <code>classmethod</code>","text":"<p>Submits a new cube to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_info</code> <code>dict</code> <p>benchmark information expected keys:     name (str): benchmark name     description (str): benchmark description     docs_url (str): benchmark documentation url     demo_url (str): benchmark demo dataset url     demo_hash (str): benchmark demo dataset hash     data_preparation_mlcube (int): benchmark data preparation mlcube uid     reference_model_mlcube (int): benchmark reference model mlcube uid     evaluator_mlcube (int): benchmark data evaluator mlcube uid</p> required Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>@classmethod\ndef run(cls, benchmark_info: dict, no_cache: bool = True):\n\"\"\"Submits a new cube to the medperf platform\n    Args:\n        benchmark_info (dict): benchmark information\n            expected keys:\n                name (str): benchmark name\n                description (str): benchmark description\n                docs_url (str): benchmark documentation url\n                demo_url (str): benchmark demo dataset url\n                demo_hash (str): benchmark demo dataset hash\n                data_preparation_mlcube (int): benchmark data preparation mlcube uid\n                reference_model_mlcube (int): benchmark reference model mlcube uid\n                evaluator_mlcube (int): benchmark data evaluator mlcube uid\n    \"\"\"\nui = config.ui\nsubmission = cls(benchmark_info, no_cache)\nwith ui.interactive():\nui.text = \"Getting additional information\"\nsubmission.get_extra_information()\nui.print(\"&gt; Completed benchmark registration information\")\nui.text = \"Submitting Benchmark to MedPerf\"\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.to_permanent_path(updated_benchmark_body)\nsubmission.write(updated_benchmark_body)\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.run_compatibility_test","title":"<code>run_compatibility_test()</code>","text":"<p>Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission</p> Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>def run_compatibility_test(self):\n\"\"\"Runs a compatibility test to ensure elements are compatible,\n    and to extract additional information required for submission\n    \"\"\"\nself.ui.print(\"Running compatibility test\")\nself.bmk.write()\ndata_uid, results = CompatibilityTestExecution.run(\nbenchmark=self.bmk.generated_uid, no_cache=self.no_cache\n)\nreturn data_uid, results\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.to_permanent_path","title":"<code>to_permanent_path(bmk_dict)</code>","text":"<p>Renames the temporary benchmark submission to a permanent one</p> <p>Parameters:</p> Name Type Description Default <code>bmk_dict</code> <code>dict</code> <p>dictionary containing updated information of the submitted benchmark</p> required Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>def to_permanent_path(self, bmk_dict: dict):\n\"\"\"Renames the temporary benchmark submission to a permanent one\n    Args:\n        bmk_dict (dict): dictionary containing updated information of the submitted benchmark\n    \"\"\"\nold_bmk_loc = self.bmk.path\nupdated_bmk = Benchmark(**bmk_dict)\nnew_bmk_loc = updated_bmk.path\nremove_path(new_bmk_loc)\nos.rename(old_bmk_loc, new_bmk_loc)\n</code></pre>"},{"location":"reference/commands/compatibility_test/compatibility_test/","title":"Compatibility test","text":""},{"location":"reference/commands/compatibility_test/compatibility_test/#commands.compatibility_test.compatibility_test.list","title":"<code>list()</code>","text":"<p>List previously executed tests reports.</p> Source code in <code>cli/medperf/commands/compatibility_test/compatibility_test.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list():\n\"\"\"List previously executed tests reports.\"\"\"\nEntityList.run(TestReport, fields=[\"UID\", \"Data Source\", \"Model\", \"Evaluator\"])\n</code></pre>"},{"location":"reference/commands/compatibility_test/compatibility_test/#commands.compatibility_test.compatibility_test.run","title":"<code>run(benchmark_uid=typer.Option(None, '--benchmark', '-b', help='UID of the benchmark to test. Optional'), data_uid=typer.Option(None, '--data_uid', '-d', help='Prepared Dataset UID. Used for dataset testing. Optional. Defaults to benchmark demo dataset.'), demo_dataset_url=typer.Option(None, '--demo_dataset_url', help='Identifier to download the demonstration dataset tarball file.\\n\\n            See `medperf mlcube submit --help` for more information'), demo_dataset_hash=typer.Option(None, '--demo_dataset_hash', help='Hash of the demo dataset, if provided.'), data_path=typer.Option(None, '--data_path', help='Path to raw input data.'), labels_path=typer.Option(None, '--labels_path', help='Path to the labels of the raw input data, if provided.'), data_prep=typer.Option(None, '--data_preparation', '-p', help='UID or local path to the data preparation mlcube. Optional. Defaults to benchmark data preparation mlcube.'), model=typer.Option(None, '--model', '-m', help='UID or local path to the model mlcube. Optional. Defaults to benchmark reference mlcube.'), evaluator=typer.Option(None, '--evaluator', '-e', help='UID or local path to the evaluator mlcube. Optional. Defaults to benchmark evaluator mlcube'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'), offline=typer.Option(False, '--offline', help='Execute the test without connecting to the MedPerf server.'))</code>","text":"<p>Executes a compatibility test for a determined benchmark. Can test prepared and unprepared datasets, remote and local models independently.</p> Source code in <code>cli/medperf/commands/compatibility_test/compatibility_test.py</code> <pre><code>@app.command(\"run\")\n@clean_except\ndef run(\nbenchmark_uid: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"UID of the benchmark to test. Optional\",\n),\ndata_uid: str = typer.Option(\nNone,\n\"--data_uid\",\n\"-d\",\nhelp=\"Prepared Dataset UID. Used for dataset testing. Optional. Defaults to benchmark demo dataset.\",\n),\ndemo_dataset_url: str = typer.Option(\nNone,\n\"--demo_dataset_url\",\nhelp=\"\"\"Identifier to download the demonstration dataset tarball file.\\n\n            See `medperf mlcube submit --help` for more information\"\"\",\n),\ndemo_dataset_hash: str = typer.Option(\nNone, \"--demo_dataset_hash\", help=\"Hash of the demo dataset, if provided.\",\n),\ndata_path: str = typer.Option(None, \"--data_path\", help=\"Path to raw input data.\",),\nlabels_path: str = typer.Option(\nNone,\n\"--labels_path\",\nhelp=\"Path to the labels of the raw input data, if provided.\",\n),\ndata_prep: str = typer.Option(\nNone,\n\"--data_preparation\",\n\"-p\",\nhelp=\"UID or local path to the data preparation mlcube. Optional. Defaults to benchmark data preparation mlcube.\",\n),\nmodel: str = typer.Option(\nNone,\n\"--model\",\n\"-m\",\nhelp=\"UID or local path to the model mlcube. Optional. Defaults to benchmark reference mlcube.\",\n),\nevaluator: str = typer.Option(\nNone,\n\"--evaluator\",\n\"-e\",\nhelp=\"UID or local path to the evaluator mlcube. Optional. Defaults to benchmark evaluator mlcube\",\n),\nno_cache: bool = typer.Option(\nFalse, \"--no-cache\", help=\"Execute the test even if results already exist\",\n),\noffline: bool = typer.Option(\nFalse,\n\"--offline\",\nhelp=\"Execute the test without connecting to the MedPerf server.\",\n),\n):\n\"\"\"\n    Executes a compatibility test for a determined benchmark.\n    Can test prepared and unprepared datasets, remote and local models independently.\n    \"\"\"\nCompatibilityTestExecution.run(\nbenchmark_uid,\ndata_prep,\nmodel,\nevaluator,\ndata_path,\nlabels_path,\ndemo_dataset_url,\ndemo_dataset_hash,\ndata_uid,\nno_cache=no_cache,\noffline=offline,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/compatibility_test/compatibility_test/#commands.compatibility_test.compatibility_test.view","title":"<code>view(entity_id=typer.Argument(None, help='Test report ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more test reports</p> Source code in <code>cli/medperf/commands/compatibility_test/compatibility_test.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[str] = typer.Argument(None, help=\"Test report ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more test reports\n    \"\"\"\nEntityView.run(entity_id, TestReport, format, output=output)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/","title":"Run","text":""},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution","title":"<code>CompatibilityTestExecution</code>","text":"Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>class CompatibilityTestExecution:\n@classmethod\ndef run(\ncls,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\nno_cache: bool = False,\noffline: bool = False,\n) -&gt; List:\n\"\"\"Execute a test workflow. Components of a complete workflow should be passed.\n        When only the benchmark is provided, it implies the following workflow will be used:\n        - the benchmark's demo dataset is used as the raw data\n        - the benchmark's data preparation cube is used\n        - the benchmark's reference model cube is used\n        - the benchmark's metrics cube is used\n        Overriding benchmark's components:\n        - The data prepration, model, and metrics cubes can be overriden by specifying a cube either\n        as an integer (registered) or a path (local). The path can refer either to the mlcube config\n        file or to the mlcube directory containing the mlcube config file.\n        - Instead of using the demo dataset of the benchmark, The input raw data can be overriden by providing:\n            - a demo dataset url and its hash\n            - data path and labels path\n        - A prepared dataset can be directly used. In this case the data preparator cube is never used.\n        The prepared data can be provided by either specifying an integer (registered) or a hash of a\n        locally prepared dataset.\n        Whether the benchmark is provided or not, the command will fail either if the user fails to\n        provide a valid complete workflow, or if the user provided extra redundant parameters.\n        Args:\n            benchmark (int, optional): Benchmark to run the test workflow for\n            data_prep (str, optional): data preparation mlcube uid or local path.\n            model (str, optional): model mlcube uid or local path.\n            evaluator (str, optional): evaluator mlcube uid or local path.\n            data_path (str, optional): path to a local raw data\n            labels_path (str, optional): path to the labels of the local raw data\n            demo_dataset_url (str, optional): Identifier to download the demonstration dataset tarball file.\\n\n            See `medperf mlcube submit --help` for more information\n            demo_dataset_hash (str, optional): The hash of the demo dataset tarball file\n            data_uid (str, optional): A prepared dataset UID\n            no_cache (bool): Whether to ignore cached results of the test execution. Defaults to False.\n            offline (bool): Whether to disable communication to the MedPerf server and rely only on\n            local copies of the server assets. Defaults to False.\n        Returns:\n            (str): Prepared Dataset UID used for the test. Could be the one provided or a generated one.\n            (dict): Results generated by the test.\n        \"\"\"\nlogging.info(\"Starting test execution\")\ntest_exec = cls(\nbenchmark,\ndata_prep,\nmodel,\nevaluator,\ndata_path,\nlabels_path,\ndemo_dataset_url,\ndemo_dataset_hash,\ndata_uid,\nno_cache,\noffline,\n)\ntest_exec.validate()\ntest_exec.set_data_source()\ntest_exec.process_benchmark()\ntest_exec.prepare_cubes()\ntest_exec.prepare_dataset()\ntest_exec.initialize_report()\nresults = test_exec.cached_results()\nif results is None:\nresults = test_exec.execute()\ntest_exec.write(results)\nreturn test_exec.data_uid, results\ndef __init__(\nself,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\nno_cache: bool = False,\noffline: bool = False,\n):\nself.benchmark_uid = benchmark\nself.data_prep = data_prep\nself.model = model\nself.evaluator = evaluator\nself.data_path = data_path\nself.labels_path = labels_path\nself.demo_dataset_url = demo_dataset_url\nself.demo_dataset_hash = demo_dataset_hash\nself.data_uid = data_uid\nself.no_cache = no_cache\nself.offline = offline\n# This property will be set to either \"path\", \"demo\", \"prepared\", or \"benchmark\"\nself.data_source = None\nself.dataset = None\nself.model_cube = None\nself.evaluator_cube = None\nself.validator = CompatibilityTestParamsValidator(\nself.benchmark_uid,\nself.data_prep,\nself.model,\nself.evaluator,\nself.data_path,\nself.labels_path,\nself.demo_dataset_url,\nself.demo_dataset_hash,\nself.data_uid,\n)\ndef validate(self):\nself.validator.validate()\ndef set_data_source(self):\nself.data_source = self.validator.get_data_source()\ndef process_benchmark(self):\n\"\"\"Process the benchmark input if given. Sets the needed parameters from\n        the benchmark.\"\"\"\nif not self.benchmark_uid:\nreturn\nbenchmark = Benchmark.get(self.benchmark_uid, local_only=self.offline)\nif self.data_source != \"prepared\":\nself.data_prep = self.data_prep or benchmark.data_preparation_mlcube\nself.model = self.model or benchmark.reference_model_mlcube\nself.evaluator = self.evaluator or benchmark.data_evaluator_mlcube\nif self.data_source == \"benchmark\":\nself.demo_dataset_url = benchmark.demo_dataset_tarball_url\nself.demo_dataset_hash = benchmark.demo_dataset_tarball_hash\ndef prepare_cubes(self):\n\"\"\"Prepares the mlcubes. If the provided mlcube is a path, it will create\n        a temporary uid and link the cube path to the medperf storage path.\"\"\"\nif self.data_source != \"prepared\":\nlogging.info(f\"Establishing the data preparation cube: {self.data_prep}\")\nself.data_prep = prepare_cube(self.data_prep)\nlogging.info(f\"Establishing the model cube: {self.model}\")\nself.model = prepare_cube(self.model)\nlogging.info(f\"Establishing the evaluator cube: {self.evaluator}\")\nself.evaluator = prepare_cube(self.evaluator)\nself.model_cube = get_cube(self.model, \"Model\", local_only=self.offline)\nself.evaluator_cube = get_cube(\nself.evaluator, \"Evaluator\", local_only=self.offline\n)\ndef prepare_dataset(self):\n\"\"\"Assigns the data_uid used for testing and retrieves the dataset.\n        If the data is not prepared, it calls the data preparation step\n        on the given local data path or using a remote demo dataset.\"\"\"\nlogging.info(\"Establishing data_uid for test execution\")\nif self.data_source != \"prepared\":\nif self.data_source == \"path\":\ndata_path, labels_path = self.data_path, self.labels_path\nelse:\ndata_path, labels_path = download_demo_data(\nself.demo_dataset_url, self.demo_dataset_hash\n)\nself.data_uid = DataPreparation.run(\nNone,\nself.data_prep,\ndata_path,\nlabels_path,\nrun_test=True,\nname=\"demo_data\",\nlocation=\"local\",\n)\nself.dataset = Dataset.get(self.data_uid, local_only=self.offline)\ndef initialize_report(self):\n\"\"\"Initializes an instance of `TestReport` to hold the current test information.\"\"\"\nreport_data = {\n\"demo_dataset_url\": self.demo_dataset_url,\n\"demo_dataset_hash\": self.demo_dataset_hash,\n\"data_path\": self.data_path,\n\"labels_path\": self.labels_path,\n\"prepared_data_hash\": self.data_uid,\n\"data_preparation_mlcube\": self.data_prep,\n\"model\": self.model,\n\"data_evaluator_mlcube\": self.evaluator,\n}\nself.report = TestReport(**report_data)\ndef cached_results(self):\n\"\"\"checks the existance of, and retrieves if possible, the compatibility test\n        result. This method is called prior to the test execution.\n        Returns:\n            (dict|None): None if the results does not exist or if self.no_cache is True,\n            otherwise it returns the found results.\n        \"\"\"\nif self.no_cache:\nreturn\nuid = self.report.generated_uid\ntry:\nreport = TestReport.get(uid)\nexcept InvalidArgumentError:\nreturn\nlogging.info(f\"Existing report {uid} was detected.\")\nlogging.info(\"The compatibilty test will not be re-executed.\")\nreturn report.results\ndef execute(self):\n\"\"\"Runs the test execution flow and returns the results\n        Returns:\n            dict: returns the results of the test execution.\n        \"\"\"\nexecution_summary = Execution.run(\ndataset=self.dataset,\nmodel=self.model_cube,\nevaluator=self.evaluator_cube,\nignore_model_errors=False,\n)\nreturn execution_summary[\"results\"]\ndef write(self, results):\n\"\"\"Writes a report of the test execution to the disk\n        Args:\n            results (dict): the results of the test execution\n        \"\"\"\nself.report.set_results(results)\nself.report.write()\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.cached_results","title":"<code>cached_results()</code>","text":"<p>checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>None if the results does not exist or if self.no_cache is True,</p> <p>otherwise it returns the found results.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def cached_results(self):\n\"\"\"checks the existance of, and retrieves if possible, the compatibility test\n    result. This method is called prior to the test execution.\n    Returns:\n        (dict|None): None if the results does not exist or if self.no_cache is True,\n        otherwise it returns the found results.\n    \"\"\"\nif self.no_cache:\nreturn\nuid = self.report.generated_uid\ntry:\nreport = TestReport.get(uid)\nexcept InvalidArgumentError:\nreturn\nlogging.info(f\"Existing report {uid} was detected.\")\nlogging.info(\"The compatibilty test will not be re-executed.\")\nreturn report.results\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.execute","title":"<code>execute()</code>","text":"<p>Runs the test execution flow and returns the results</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>returns the results of the test execution.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def execute(self):\n\"\"\"Runs the test execution flow and returns the results\n    Returns:\n        dict: returns the results of the test execution.\n    \"\"\"\nexecution_summary = Execution.run(\ndataset=self.dataset,\nmodel=self.model_cube,\nevaluator=self.evaluator_cube,\nignore_model_errors=False,\n)\nreturn execution_summary[\"results\"]\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.initialize_report","title":"<code>initialize_report()</code>","text":"<p>Initializes an instance of <code>TestReport</code> to hold the current test information.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def initialize_report(self):\n\"\"\"Initializes an instance of `TestReport` to hold the current test information.\"\"\"\nreport_data = {\n\"demo_dataset_url\": self.demo_dataset_url,\n\"demo_dataset_hash\": self.demo_dataset_hash,\n\"data_path\": self.data_path,\n\"labels_path\": self.labels_path,\n\"prepared_data_hash\": self.data_uid,\n\"data_preparation_mlcube\": self.data_prep,\n\"model\": self.model,\n\"data_evaluator_mlcube\": self.evaluator,\n}\nself.report = TestReport(**report_data)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.prepare_cubes","title":"<code>prepare_cubes()</code>","text":"<p>Prepares the mlcubes. If the provided mlcube is a path, it will create a temporary uid and link the cube path to the medperf storage path.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def prepare_cubes(self):\n\"\"\"Prepares the mlcubes. If the provided mlcube is a path, it will create\n    a temporary uid and link the cube path to the medperf storage path.\"\"\"\nif self.data_source != \"prepared\":\nlogging.info(f\"Establishing the data preparation cube: {self.data_prep}\")\nself.data_prep = prepare_cube(self.data_prep)\nlogging.info(f\"Establishing the model cube: {self.model}\")\nself.model = prepare_cube(self.model)\nlogging.info(f\"Establishing the evaluator cube: {self.evaluator}\")\nself.evaluator = prepare_cube(self.evaluator)\nself.model_cube = get_cube(self.model, \"Model\", local_only=self.offline)\nself.evaluator_cube = get_cube(\nself.evaluator, \"Evaluator\", local_only=self.offline\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.prepare_dataset","title":"<code>prepare_dataset()</code>","text":"<p>Assigns the data_uid used for testing and retrieves the dataset. If the data is not prepared, it calls the data preparation step on the given local data path or using a remote demo dataset.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def prepare_dataset(self):\n\"\"\"Assigns the data_uid used for testing and retrieves the dataset.\n    If the data is not prepared, it calls the data preparation step\n    on the given local data path or using a remote demo dataset.\"\"\"\nlogging.info(\"Establishing data_uid for test execution\")\nif self.data_source != \"prepared\":\nif self.data_source == \"path\":\ndata_path, labels_path = self.data_path, self.labels_path\nelse:\ndata_path, labels_path = download_demo_data(\nself.demo_dataset_url, self.demo_dataset_hash\n)\nself.data_uid = DataPreparation.run(\nNone,\nself.data_prep,\ndata_path,\nlabels_path,\nrun_test=True,\nname=\"demo_data\",\nlocation=\"local\",\n)\nself.dataset = Dataset.get(self.data_uid, local_only=self.offline)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.process_benchmark","title":"<code>process_benchmark()</code>","text":"<p>Process the benchmark input if given. Sets the needed parameters from the benchmark.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def process_benchmark(self):\n\"\"\"Process the benchmark input if given. Sets the needed parameters from\n    the benchmark.\"\"\"\nif not self.benchmark_uid:\nreturn\nbenchmark = Benchmark.get(self.benchmark_uid, local_only=self.offline)\nif self.data_source != \"prepared\":\nself.data_prep = self.data_prep or benchmark.data_preparation_mlcube\nself.model = self.model or benchmark.reference_model_mlcube\nself.evaluator = self.evaluator or benchmark.data_evaluator_mlcube\nif self.data_source == \"benchmark\":\nself.demo_dataset_url = benchmark.demo_dataset_tarball_url\nself.demo_dataset_hash = benchmark.demo_dataset_tarball_hash\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.run","title":"<code>run(benchmark=None, data_prep=None, model=None, evaluator=None, data_path=None, labels_path=None, demo_dataset_url=None, demo_dataset_hash=None, data_uid=None, no_cache=False, offline=False)</code>  <code>classmethod</code>","text":"<p>Execute a test workflow. Components of a complete workflow should be passed. When only the benchmark is provided, it implies the following workflow will be used: - the benchmark's demo dataset is used as the raw data - the benchmark's data preparation cube is used - the benchmark's reference model cube is used - the benchmark's metrics cube is used</p> <p>Overriding benchmark's components: - The data prepration, model, and metrics cubes can be overriden by specifying a cube either as an integer (registered) or a path (local). The path can refer either to the mlcube config file or to the mlcube directory containing the mlcube config file. - Instead of using the demo dataset of the benchmark, The input raw data can be overriden by providing:     - a demo dataset url and its hash     - data path and labels path - A prepared dataset can be directly used. In this case the data preparator cube is never used. The prepared data can be provided by either specifying an integer (registered) or a hash of a locally prepared dataset.</p> <p>Whether the benchmark is provided or not, the command will fail either if the user fails to provide a valid complete workflow, or if the user provided extra redundant parameters.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>int</code> <p>Benchmark to run the test workflow for</p> <code>None</code> <code>data_prep</code> <code>str</code> <p>data preparation mlcube uid or local path.</p> <code>None</code> <code>model</code> <code>str</code> <p>model mlcube uid or local path.</p> <code>None</code> <code>evaluator</code> <code>str</code> <p>evaluator mlcube uid or local path.</p> <code>None</code> <code>data_path</code> <code>str</code> <p>path to a local raw data</p> <code>None</code> <code>labels_path</code> <code>str</code> <p>path to the labels of the local raw data</p> <code>None</code> <code>demo_dataset_url</code> <code>str</code> <p>Identifier to download the demonstration dataset tarball file.</p> <code>None</code> <code>demo_dataset_hash</code> <code>str</code> <p>The hash of the demo dataset tarball file</p> <code>None</code> <code>data_uid</code> <code>str</code> <p>A prepared dataset UID</p> <code>None</code> <code>no_cache</code> <code>bool</code> <p>Whether to ignore cached results of the test execution. Defaults to False.</p> <code>False</code> <code>offline</code> <code>bool</code> <p>Whether to disable communication to the MedPerf server and rely only on</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Prepared Dataset UID used for the test. Could be the one provided or a generated one.</p> <code>dict</code> <p>Results generated by the test.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\nno_cache: bool = False,\noffline: bool = False,\n) -&gt; List:\n\"\"\"Execute a test workflow. Components of a complete workflow should be passed.\n    When only the benchmark is provided, it implies the following workflow will be used:\n    - the benchmark's demo dataset is used as the raw data\n    - the benchmark's data preparation cube is used\n    - the benchmark's reference model cube is used\n    - the benchmark's metrics cube is used\n    Overriding benchmark's components:\n    - The data prepration, model, and metrics cubes can be overriden by specifying a cube either\n    as an integer (registered) or a path (local). The path can refer either to the mlcube config\n    file or to the mlcube directory containing the mlcube config file.\n    - Instead of using the demo dataset of the benchmark, The input raw data can be overriden by providing:\n        - a demo dataset url and its hash\n        - data path and labels path\n    - A prepared dataset can be directly used. In this case the data preparator cube is never used.\n    The prepared data can be provided by either specifying an integer (registered) or a hash of a\n    locally prepared dataset.\n    Whether the benchmark is provided or not, the command will fail either if the user fails to\n    provide a valid complete workflow, or if the user provided extra redundant parameters.\n    Args:\n        benchmark (int, optional): Benchmark to run the test workflow for\n        data_prep (str, optional): data preparation mlcube uid or local path.\n        model (str, optional): model mlcube uid or local path.\n        evaluator (str, optional): evaluator mlcube uid or local path.\n        data_path (str, optional): path to a local raw data\n        labels_path (str, optional): path to the labels of the local raw data\n        demo_dataset_url (str, optional): Identifier to download the demonstration dataset tarball file.\\n\n        See `medperf mlcube submit --help` for more information\n        demo_dataset_hash (str, optional): The hash of the demo dataset tarball file\n        data_uid (str, optional): A prepared dataset UID\n        no_cache (bool): Whether to ignore cached results of the test execution. Defaults to False.\n        offline (bool): Whether to disable communication to the MedPerf server and rely only on\n        local copies of the server assets. Defaults to False.\n    Returns:\n        (str): Prepared Dataset UID used for the test. Could be the one provided or a generated one.\n        (dict): Results generated by the test.\n    \"\"\"\nlogging.info(\"Starting test execution\")\ntest_exec = cls(\nbenchmark,\ndata_prep,\nmodel,\nevaluator,\ndata_path,\nlabels_path,\ndemo_dataset_url,\ndemo_dataset_hash,\ndata_uid,\nno_cache,\noffline,\n)\ntest_exec.validate()\ntest_exec.set_data_source()\ntest_exec.process_benchmark()\ntest_exec.prepare_cubes()\ntest_exec.prepare_dataset()\ntest_exec.initialize_report()\nresults = test_exec.cached_results()\nif results is None:\nresults = test_exec.execute()\ntest_exec.write(results)\nreturn test_exec.data_uid, results\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.write","title":"<code>write(results)</code>","text":"<p>Writes a report of the test execution to the disk</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>the results of the test execution</p> required Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def write(self, results):\n\"\"\"Writes a report of the test execution to the disk\n    Args:\n        results (dict): the results of the test execution\n    \"\"\"\nself.report.set_results(results)\nself.report.write()\n</code></pre>"},{"location":"reference/commands/compatibility_test/utils/","title":"Utils","text":""},{"location":"reference/commands/compatibility_test/utils/#commands.compatibility_test.utils.download_demo_data","title":"<code>download_demo_data(dset_url, dset_hash)</code>","text":"<p>Retrieves the demo dataset associated to the specified benchmark</p> <p>Returns:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Location of the downloaded data</p> <code>labels_path</code> <code>str</code> <p>Location of the downloaded labels</p> Source code in <code>cli/medperf/commands/compatibility_test/utils.py</code> <pre><code>def download_demo_data(dset_url, dset_hash):\n\"\"\"Retrieves the demo dataset associated to the specified benchmark\n    Returns:\n        data_path (str): Location of the downloaded data\n        labels_path (str): Location of the downloaded labels\n    \"\"\"\ntry:\ndemo_dset_path, _ = resources.get_benchmark_demo_dataset(dset_url, dset_hash)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Demo dataset {dset_url}: {e}\")\n# It is assumed that all demo datasets contain a file\n# which specifies the input of the data preparation step\npaths_file = os.path.join(demo_dset_path, config.demo_dset_paths_file)\nwith open(paths_file, \"r\") as f:\npaths = yaml.safe_load(f)\ndata_path = os.path.join(demo_dset_path, paths[\"data_path\"])\nlabels_path = os.path.join(demo_dset_path, paths[\"labels_path\"])\nreturn data_path, labels_path\n</code></pre>"},{"location":"reference/commands/compatibility_test/utils/#commands.compatibility_test.utils.prepare_cube","title":"<code>prepare_cube(cube_uid)</code>","text":"<p>Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path.</p> <p>Parameters:</p> Name Type Description Default <code>attr</code> <code>str</code> <p>Attribute to check and/or reassign.</p> required <code>fallback</code> <code>any</code> <p>Value to assign if attribute is empty. Defaults to None.</p> required Source code in <code>cli/medperf/commands/compatibility_test/utils.py</code> <pre><code>def prepare_cube(cube_uid: str):\n\"\"\"Assigns the attr used for testing according to the initialization parameters.\n    If the value is a path, it will create a temporary uid and link the cube path to\n    the medperf storage path.\n    Arguments:\n        attr (str): Attribute to check and/or reassign.\n        fallback (any): Value to assign if attribute is empty. Defaults to None.\n    \"\"\"\n# Test if value looks like an mlcube_uid, if so skip path validation\nif str(cube_uid).isdigit():\nlogging.info(f\"MLCube value {cube_uid} resembles an mlcube_uid\")\nreturn cube_uid\n# Check if value is a local mlcube\npath = Path(cube_uid)\nif path.is_file():\npath = path.parent\npath = path.resolve()\nif os.path.exists(path):\nlogging.info(\"local path provided. Creating symbolic link\")\ntemp_uid = prepare_local_cube(path)\nreturn temp_uid\nlogging.error(f\"mlcube {cube_uid} was not found as an existing mlcube\")\nraise InvalidArgumentError(\nf\"The provided mlcube ({cube_uid}) could not be found as a local or remote mlcube\"\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/validate_params/","title":"Validate params","text":""},{"location":"reference/commands/compatibility_test/validate_params/#commands.compatibility_test.validate_params.CompatibilityTestParamsValidator","title":"<code>CompatibilityTestParamsValidator</code>","text":"<p>Validates the input parameters to the CompatibilityTestExecution class</p> Source code in <code>cli/medperf/commands/compatibility_test/validate_params.py</code> <pre><code>class CompatibilityTestParamsValidator:\n\"\"\"Validates the input parameters to the CompatibilityTestExecution class\"\"\"\ndef __init__(\nself,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\n):\nself.benchmark_uid = benchmark\nself.data_prep = data_prep\nself.model = model\nself.evaluator = evaluator\nself.data_path = data_path\nself.labels_path = labels_path\nself.demo_dataset_url = demo_dataset_url\nself.demo_dataset_hash = demo_dataset_hash\nself.data_uid = data_uid\ndef __validate_cubes(self):\nif not self.model and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A model mlcube or a benchmark should at least be specified\"\n)\nif not self.evaluator and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A metrics mlcube or a benchmark should at least be specified\"\n)\ndef __raise_redundant_data_source(self):\nmsg = \"Make sure you pass only one data source: \"\nmsg += \"either a prepared dataset, a data path and labels path, or a demo dataset url\"\nraise InvalidArgumentError(msg)\ndef __validate_prepared_data_source(self):\nif any(\n[\nself.data_path,\nself.labels_path,\nself.demo_dataset_url,\nself.demo_dataset_hash,\n]\n):\nself.__raise_redundant_data_source()\nif self.data_prep:\nraise InvalidArgumentError(\n\"A data preparation cube is not needed when specifying a prepared dataset\"\n)\ndef __validate_data_path_source(self):\nif not self.labels_path:\nraise InvalidArgumentError(\n\"Labels path should be specified when providing data path\"\n)\nif any([self.demo_dataset_url, self.demo_dataset_hash, self.data_uid]):\nself.__raise_redundant_data_source()\nif not self.data_prep and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A data preparation cube should be passed when specifying raw data input\"\n)\ndef __validate_demo_data_source(self):\nif not self.demo_dataset_hash:\nraise InvalidArgumentError(\n\"The hash of the provided demo dataset should be specified\"\n)\nif any([self.data_path, self.labels_path, self.data_uid]):\nself.__raise_redundant_data_source()\nif not self.data_prep and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A data preparation cube should be passed when specifying raw data input\"\n)\ndef __validate_data_source(self):\nif self.data_uid:\nself.__validate_prepared_data_source()\nreturn\nif self.data_path:\nself.__validate_data_path_source()\nreturn\nif self.demo_dataset_url:\nself.__validate_demo_data_source()\nreturn\nif self.benchmark_uid:\nreturn\nmsg = \"A data source should at least be specified, either by providing\"\nmsg += \" a prepared data uid, a demo dataset url, data path, or a benchmark\"\nraise InvalidArgumentError(msg)\ndef __validate_redundant_benchmark(self):\nif not self.benchmark_uid:\nreturn\nredundant_bmk_demo = any([self.data_uid, self.data_path, self.demo_dataset_url])\nredundant_bmk_model = self.model is not None\nredundant_bmk_evaluator = self.evaluator is not None\nredundant_bmk_preparator = (\nself.data_prep is not None or self.data_uid is not None\n)\nif all(\n[\nredundant_bmk_demo,\nredundant_bmk_model,\nredundant_bmk_evaluator,\nredundant_bmk_preparator,\n]\n):\nraise InvalidArgumentError(\"The provided benchmark will not be used\")\ndef validate(self):\n\"\"\"Ensures test has been passed a valid combination of parameters.\n        Raises `medperf.exceptions.InvalidArgumentError` when the parameters are\n        invalid.\n        \"\"\"\nself.__validate_cubes()\nself.__validate_data_source()\nself.__validate_redundant_benchmark()\ndef get_data_source(self):\n\"\"\"Parses the input parameters and returns a string, one of:\n        \"prepared\", if the source of data is a prepared dataset uid,\n        \"path\", if the source of data is a local path to raw data,\n        \"demo\", if the source of data is a demo dataset url,\n        or \"benchmark\", if the source of data is the demo dataset of a benchmark.\n        This function assumes the passed parameters to the constructor have been already\n        validated.\n        \"\"\"\nif self.data_uid:\nreturn \"prepared\"\nif self.data_path:\nreturn \"path\"\nif self.demo_dataset_url:\nreturn \"demo\"\nif self.benchmark_uid:\nreturn \"benchmark\"\nraise MedperfException(\n\"Ensure calling the `validate` method before using this method\"\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/validate_params/#commands.compatibility_test.validate_params.CompatibilityTestParamsValidator.get_data_source","title":"<code>get_data_source()</code>","text":"<p>Parses the input parameters and returns a string, one of: \"prepared\", if the source of data is a prepared dataset uid, \"path\", if the source of data is a local path to raw data, \"demo\", if the source of data is a demo dataset url, or \"benchmark\", if the source of data is the demo dataset of a benchmark.</p> <p>This function assumes the passed parameters to the constructor have been already validated.</p> Source code in <code>cli/medperf/commands/compatibility_test/validate_params.py</code> <pre><code>def get_data_source(self):\n\"\"\"Parses the input parameters and returns a string, one of:\n    \"prepared\", if the source of data is a prepared dataset uid,\n    \"path\", if the source of data is a local path to raw data,\n    \"demo\", if the source of data is a demo dataset url,\n    or \"benchmark\", if the source of data is the demo dataset of a benchmark.\n    This function assumes the passed parameters to the constructor have been already\n    validated.\n    \"\"\"\nif self.data_uid:\nreturn \"prepared\"\nif self.data_path:\nreturn \"path\"\nif self.demo_dataset_url:\nreturn \"demo\"\nif self.benchmark_uid:\nreturn \"benchmark\"\nraise MedperfException(\n\"Ensure calling the `validate` method before using this method\"\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/validate_params/#commands.compatibility_test.validate_params.CompatibilityTestParamsValidator.validate","title":"<code>validate()</code>","text":"<p>Ensures test has been passed a valid combination of parameters. Raises <code>medperf.exceptions.InvalidArgumentError</code> when the parameters are invalid.</p> Source code in <code>cli/medperf/commands/compatibility_test/validate_params.py</code> <pre><code>def validate(self):\n\"\"\"Ensures test has been passed a valid combination of parameters.\n    Raises `medperf.exceptions.InvalidArgumentError` when the parameters are\n    invalid.\n    \"\"\"\nself.__validate_cubes()\nself.__validate_data_source()\nself.__validate_redundant_benchmark()\n</code></pre>"},{"location":"reference/commands/dataset/associate/","title":"Associate","text":""},{"location":"reference/commands/dataset/associate/#commands.dataset.associate.AssociateDataset","title":"<code>AssociateDataset</code>","text":"Source code in <code>cli/medperf/commands/dataset/associate.py</code> <pre><code>class AssociateDataset:\n@staticmethod\ndef run(data_uid: int, benchmark_uid: int, approved=False, no_cache=False):\n\"\"\"Associates a registered dataset with a benchmark\n        Args:\n            data_uid (int): UID of the registered dataset to associate\n            benchmark_uid (int): UID of the benchmark to associate with\n        \"\"\"\ncomms = config.comms\nui = config.ui\ndset = Dataset.get(data_uid)\nif dset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nbenchmark = Benchmark.get(benchmark_uid)\nif dset.data_preparation_mlcube != benchmark.data_preparation_mlcube:\nraise InvalidArgumentError(\n\"The specified dataset wasn't prepared for this benchmark\"\n)\nresult = BenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[benchmark.reference_model_mlcube],\nno_cache=no_cache,\n)[0]\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(result.results)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the dataset {dset.name} with the benchmark {benchmark.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating dataset benchmark association\")\nmetadata = {\"test_result\": result.results}\ncomms.associate_dset(dset.id, benchmark_uid, metadata)\nelse:\nui.print(\"Dataset association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/associate/#commands.dataset.associate.AssociateDataset.run","title":"<code>run(data_uid, benchmark_uid, approved=False, no_cache=False)</code>  <code>staticmethod</code>","text":"<p>Associates a registered dataset with a benchmark</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>UID of the registered dataset to associate</p> required <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark to associate with</p> required Source code in <code>cli/medperf/commands/dataset/associate.py</code> <pre><code>@staticmethod\ndef run(data_uid: int, benchmark_uid: int, approved=False, no_cache=False):\n\"\"\"Associates a registered dataset with a benchmark\n    Args:\n        data_uid (int): UID of the registered dataset to associate\n        benchmark_uid (int): UID of the benchmark to associate with\n    \"\"\"\ncomms = config.comms\nui = config.ui\ndset = Dataset.get(data_uid)\nif dset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nbenchmark = Benchmark.get(benchmark_uid)\nif dset.data_preparation_mlcube != benchmark.data_preparation_mlcube:\nraise InvalidArgumentError(\n\"The specified dataset wasn't prepared for this benchmark\"\n)\nresult = BenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[benchmark.reference_model_mlcube],\nno_cache=no_cache,\n)[0]\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(result.results)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the dataset {dset.name} with the benchmark {benchmark.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating dataset benchmark association\")\nmetadata = {\"test_result\": result.results}\ncomms.associate_dset(dset.id, benchmark_uid, metadata)\nelse:\nui.print(\"Dataset association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/create/","title":"Create","text":""},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation","title":"<code>DataPreparation</code>","text":"Source code in <code>cli/medperf/commands/dataset/create.py</code> <pre><code>class DataPreparation:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\nprep_cube_uid: int,\ndata_path: str,\nlabels_path: str,\nrun_test=False,\nname: str = None,\ndescription: str = None,\nlocation: str = None,\n):\npreparation = cls(\nbenchmark_uid,\nprep_cube_uid,\ndata_path,\nlabels_path,\nname,\ndescription,\nlocation,\nrun_test,\n)\npreparation.validate()\nwith preparation.ui.interactive():\npreparation.get_prep_cube()\npreparation.run_cube_tasks()\npreparation.get_statistics()\npreparation.generate_uids()\npreparation.to_permanent_path()\npreparation.write()\nreturn preparation.generated_uid\ndef __init__(\nself,\nbenchmark_uid: int,\nprep_cube_uid: int,\ndata_path: str,\nlabels_path: str,\nname: str,\ndescription: str,\nlocation: str,\nrun_test=False,\n):\nself.comms = config.comms\nself.ui = config.ui\nself.data_path = str(Path(data_path).resolve())\nself.labels_path = str(Path(labels_path).resolve())\nout_path = generate_tmp_path()\nself.out_statistics_path = generate_tmp_path()\nself.out_path = out_path\nself.name = name\nself.description = description\nself.location = location\nself.out_datapath = os.path.join(out_path, \"data\")\nself.out_labelspath = os.path.join(out_path, \"labels\")\nself.run_test = run_test\nself.benchmark_uid = benchmark_uid\nself.prep_cube_uid = prep_cube_uid\nself.in_uid = None\nself.generated_uid = None\nlogging.debug(f\"tmp data preparation output: {out_path}\")\nlogging.debug(f\"tmp data statistics output: {self.out_statistics_path}\")\ndef validate(self):\nif not os.path.exists(self.data_path):\nraise InvalidArgumentError(\"The provided data path doesn't exist\")\nif not os.path.exists(self.labels_path):\nraise InvalidArgumentError(\"The provided labels path doesn't exist\")\ntoo_many_resources = self.benchmark_uid and self.prep_cube_uid\nno_resource = self.benchmark_uid is None and self.prep_cube_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\n\"Must provide either a benchmark or a preparation mlcube\"\n)\ndef get_prep_cube(self):\ncube_uid = self.prep_cube_uid\nif cube_uid is None:\nbenchmark = Benchmark.get(self.benchmark_uid)\ncube_uid = benchmark.data_preparation_mlcube\nself.ui.print(f\"Benchmark Data Preparation: {benchmark.name}\")\nself.ui.text = \"Retrieving data preparation cube\"\nself.cube = Cube.get(cube_uid)\nself.ui.print(\"&gt; Preparation cube download complete\")\ndef run_cube_tasks(self):\nprepare_timeout = config.prepare_timeout\nsanity_check_timeout = config.sanity_check_timeout\nstatistics_timeout = config.statistics_timeout\ndata_path = self.data_path\nlabels_path = self.labels_path\nout_datapath = self.out_datapath\nout_labelspath = self.out_labelspath\n# Specify parameters for the tasks\nprepare_params = {\n\"data_path\": data_path,\n\"labels_path\": labels_path,\n\"output_path\": out_datapath,\n\"output_labels_path\": out_labelspath,\n}\nprepare_str_params = {\n\"Ptasks.prepare.parameters.input.data_path.opts\": \"ro\",\n\"Ptasks.prepare.parameters.input.labels_path.opts\": \"ro\",\n}\nsanity_params = {\n\"data_path\": out_datapath,\n\"labels_path\": out_labelspath,\n}\nsanity_str_params = {\n\"Ptasks.sanity_check.parameters.input.data_path.opts\": \"ro\"\n}\nstatistics_params = {\n\"data_path\": out_datapath,\n\"output_path\": self.out_statistics_path,\n\"labels_path\": out_labelspath,\n}\nstatistics_str_params = {\n\"Ptasks.statistics.parameters.input.data_path.opts\": \"ro\"\n}\n# Run the tasks\nself.ui.text = \"Running preparation step...\"\nself.cube.run(\ntask=\"prepare\",\nstring_params=prepare_str_params,\ntimeout=prepare_timeout,\n**prepare_params,\n)\nself.ui.print(\"&gt; Cube execution complete\")\nself.ui.text = \"Running sanity check...\"\nself.cube.run(\ntask=\"sanity_check\",\nstring_params=sanity_str_params,\ntimeout=sanity_check_timeout,\n**sanity_params,\n)\nself.ui.print(\"&gt; Sanity checks complete\")\nself.ui.text = \"Generating statistics...\"\nself.cube.run(\ntask=\"statistics\",\nstring_params=statistics_str_params,\ntimeout=statistics_timeout,\n**statistics_params,\n)\nself.ui.print(\"&gt; Statistics complete\")\ndef get_statistics(self):\nwith open(self.out_statistics_path, \"r\") as f:\nstats = yaml.safe_load(f)\nself.generated_metadata = stats\ndef generate_uids(self):\n\"\"\"Auto-generates dataset UIDs for both input and output paths\"\"\"\nself.in_uid = get_folder_hash(self.data_path)\nself.generated_uid = get_folder_hash(self.out_datapath)\ndef to_permanent_path(self) -&gt; str:\n\"\"\"Renames the temporary data folder to permanent one using the hash of\n        the registration file\n        \"\"\"\nnew_path = os.path.join(storage_path(config.data_storage), self.generated_uid)\nremove_path(new_path)\nos.rename(self.out_path, new_path)\nself.out_path = new_path\ndef todict(self) -&gt; dict:\n\"\"\"Dictionary representation of the dataset\n        Returns:\n            dict: dictionary containing information pertaining the dataset.\n        \"\"\"\nreturn {\n\"id\": None,\n\"name\": self.name,\n\"description\": self.description,\n\"location\": self.location,\n\"data_preparation_mlcube\": self.cube.identifier,\n\"input_data_hash\": self.in_uid,\n\"generated_uid\": self.generated_uid,\n\"split_seed\": 0,  # Currently this is not used\n\"generated_metadata\": self.generated_metadata,\n\"status\": Status.PENDING.value,  # not in the server\n\"state\": \"OPERATION\",\n\"for_test\": self.run_test,  # not in the server (OK)\n}\ndef write(self) -&gt; str:\n\"\"\"Writes the registration into disk\n        Args:\n            filename (str, optional): name of the file. Defaults to config.reg_file.\n        \"\"\"\ndataset_dict = self.todict()\ndataset = Dataset(**dataset_dict)\ndataset.write()\n</code></pre>"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.generate_uids","title":"<code>generate_uids()</code>","text":"<p>Auto-generates dataset UIDs for both input and output paths</p> Source code in <code>cli/medperf/commands/dataset/create.py</code> <pre><code>def generate_uids(self):\n\"\"\"Auto-generates dataset UIDs for both input and output paths\"\"\"\nself.in_uid = get_folder_hash(self.data_path)\nself.generated_uid = get_folder_hash(self.out_datapath)\n</code></pre>"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.to_permanent_path","title":"<code>to_permanent_path()</code>","text":"<p>Renames the temporary data folder to permanent one using the hash of the registration file</p> Source code in <code>cli/medperf/commands/dataset/create.py</code> <pre><code>def to_permanent_path(self) -&gt; str:\n\"\"\"Renames the temporary data folder to permanent one using the hash of\n    the registration file\n    \"\"\"\nnew_path = os.path.join(storage_path(config.data_storage), self.generated_uid)\nremove_path(new_path)\nos.rename(self.out_path, new_path)\nself.out_path = new_path\n</code></pre>"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.todict","title":"<code>todict()</code>","text":"<p>Dictionary representation of the dataset</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary containing information pertaining the dataset.</p> Source code in <code>cli/medperf/commands/dataset/create.py</code> <pre><code>def todict(self) -&gt; dict:\n\"\"\"Dictionary representation of the dataset\n    Returns:\n        dict: dictionary containing information pertaining the dataset.\n    \"\"\"\nreturn {\n\"id\": None,\n\"name\": self.name,\n\"description\": self.description,\n\"location\": self.location,\n\"data_preparation_mlcube\": self.cube.identifier,\n\"input_data_hash\": self.in_uid,\n\"generated_uid\": self.generated_uid,\n\"split_seed\": 0,  # Currently this is not used\n\"generated_metadata\": self.generated_metadata,\n\"status\": Status.PENDING.value,  # not in the server\n\"state\": \"OPERATION\",\n\"for_test\": self.run_test,  # not in the server (OK)\n}\n</code></pre>"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.write","title":"<code>write()</code>","text":"<p>Writes the registration into disk</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>name of the file. Defaults to config.reg_file.</p> required Source code in <code>cli/medperf/commands/dataset/create.py</code> <pre><code>def write(self) -&gt; str:\n\"\"\"Writes the registration into disk\n    Args:\n        filename (str, optional): name of the file. Defaults to config.reg_file.\n    \"\"\"\ndataset_dict = self.todict()\ndataset = Dataset(**dataset_dict)\ndataset.write()\n</code></pre>"},{"location":"reference/commands/dataset/dataset/","title":"Dataset","text":""},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.associate","title":"<code>associate(data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), benchmark_uid=typer.Option(..., '--benchmark_uid', '-b', help='Benchmark UID'), approval=typer.Option(False, '-y', help='Skip approval step'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'))</code>","text":"<p>Associate a registered dataset with a specific benchmark. The dataset and benchmark must share the same data preparation cube.</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark_uid\", \"-b\", help=\"Benchmark UID\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nno_cache: bool = typer.Option(\nFalse, \"--no-cache\", help=\"Execute the test even if results already exist\",\n),\n):\n\"\"\"Associate a registered dataset with a specific benchmark.\n    The dataset and benchmark must share the same data preparation cube.\n    \"\"\"\nui = config.ui\nAssociateDataset.run(data_uid, benchmark_uid, approved=approval, no_cache=no_cache)\nui.print(\"\u2705 Done!\")\nui.print(\nf\"Next step: Once approved, run the benchmark with 'medperf run -b {benchmark_uid} -d {data_uid}'\"\n)\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.create","title":"<code>create(benchmark_uid=typer.Option(None, '--benchmark', '-b', help='UID of the desired benchmark'), data_prep_uid=typer.Option(None, '--data_prep', '-p', help='UID of the desired preparation cube'), data_path=typer.Option(..., '--data_path', '-d', help='Location of the data to be prepared'), labels_path=typer.Option(..., '--labels_path', '-l', help='Labels file location'), name=typer.Option(..., '--name', help='Name of the dataset'), description=typer.Option(..., '--description', help='Description of the dataset'), location=typer.Option(..., '--location', help='Location or Institution the data belongs to'))</code>","text":"<p>Runs the Data preparation step for a specified benchmark and raw dataset</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"create\")\n@clean_except\ndef create(\nbenchmark_uid: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_prep_uid: int = typer.Option(\nNone, \"--data_prep\", \"-p\", help=\"UID of the desired preparation cube\"\n),\ndata_path: str = typer.Option(\n..., \"--data_path\", \"-d\", help=\"Location of the data to be prepared\"\n),\nlabels_path: str = typer.Option(\n..., \"--labels_path\", \"-l\", help=\"Labels file location\"\n),\nname: str = typer.Option(..., \"--name\", help=\"Name of the dataset\"),\ndescription: str = typer.Option(\n..., \"--description\", help=\"Description of the dataset\"\n),\nlocation: str = typer.Option(\n..., \"--location\", help=\"Location or Institution the data belongs to\"\n),\n):\n\"\"\"Runs the Data preparation step for a specified benchmark and raw dataset\n    \"\"\"\nui = config.ui\ndata_uid = DataPreparation.run(\nbenchmark_uid,\ndata_prep_uid,\ndata_path,\nlabels_path,\nname=name,\ndescription=description,\nlocation=location,\n)\nui.print(\"\u2705 Done!\")\nui.print(\nf\"Next step: register the dataset with 'medperf dataset submit -d {data_uid}'\"\n)\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.list","title":"<code>list(local=typer.Option(False, '--local', help='Get local datasets'), mine=typer.Option(False, '--mine', help='Get current-user datasets'))</code>","text":"<p>List datasets stored locally and remotely from the user</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nlocal: bool = typer.Option(False, \"--local\", help=\"Get local datasets\"),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user datasets\"),\n):\n\"\"\"List datasets stored locally and remotely from the user\"\"\"\nEntityList.run(\nDataset,\nfields=[\"UID\", \"Name\", \"Data Preparation Cube UID\", \"Registered\"],\nlocal_only=local,\nmine_only=mine,\n)\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.register","title":"<code>register(data_uid=typer.Option(..., '--data_uid', '-d', help='Unregistered Dataset UID'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Submits an unregistered Dataset instance to the backend</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef register(\ndata_uid: str = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Unregistered Dataset UID\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Submits an unregistered Dataset instance to the backend\n    \"\"\"\nui = config.ui\nuid = DatasetRegistration.run(data_uid, approved=approval)\nui.print(\"\u2705 Done!\")\nui.print(\nf\"Next step: associate the dataset with 'medperf dataset associate -b &lt;BENCHMARK_UID&gt; -d {uid}'\"\n)\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.view","title":"<code>view(entity_id=typer.Argument(None, help='Dataset ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), local=typer.Option(False, '--local', help='Display local datasets if dataset ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user datasets if dataset ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more datasets</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[str] = typer.Argument(None, help=\"Dataset ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nlocal: bool = typer.Option(\nFalse, \"--local\", help=\"Display local datasets if dataset ID is not provided\"\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user datasets if dataset ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more datasets\n    \"\"\"\nEntityView.run(entity_id, Dataset, format, local, mine, output)\n</code></pre>"},{"location":"reference/commands/dataset/submit/","title":"Submit","text":""},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DatasetRegistration","title":"<code>DatasetRegistration</code>","text":"Source code in <code>cli/medperf/commands/dataset/submit.py</code> <pre><code>class DatasetRegistration:\n@staticmethod\ndef run(data_uid: str, approved=False):\n\"\"\"Registers a database to the backend.\n        Args:\n            data_uid (str): UID Hint of the unregistered dataset\n        \"\"\"\ncomms = config.comms\nui = config.ui\ndset = Dataset.get(data_uid)\nif dset.id is not None:\n# TODO: should get_dataset and update locally. solves existing issue?\nraise InvalidArgumentError(\"This dataset has already been registered\")\nremote_dsets = comms.get_user_datasets()\nremote_dset = [\nremote_dset\nfor remote_dset in remote_dsets\nif remote_dset[\"generated_uid\"] == dset.generated_uid\n]\nif len(remote_dset) == 1:\ndset = Dataset(**remote_dset[0])\ndset.write()\nui.print(f\"Remote dataset {dset.name} detected. Updating local dataset.\")\nreturn dset.id\ndict_pretty_print(dset.todict())\nmsg = \"Do you approve the registration of the presented data to the MLCommons comms? [Y/n] \"\napproved = approved or approval_prompt(msg)\ndset.status = Status(\"APPROVED\") if approved else Status(\"REJECTED\")\nif approved:\nui.print(\"Uploading...\")\nupdated_dset_dict = dset.upload()\nupdated_dset = Dataset(**updated_dset_dict)\nold_dset_loc = dset.path\nnew_dset_loc = updated_dset.path\nremove_path(new_dset_loc)\nos.rename(old_dset_loc, new_dset_loc)\nupdated_dset.write()\nreturn updated_dset.id\nelse:\nui.print(\"Registration request cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DatasetRegistration.run","title":"<code>run(data_uid, approved=False)</code>  <code>staticmethod</code>","text":"<p>Registers a database to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>str</code> <p>UID Hint of the unregistered dataset</p> required Source code in <code>cli/medperf/commands/dataset/submit.py</code> <pre><code>@staticmethod\ndef run(data_uid: str, approved=False):\n\"\"\"Registers a database to the backend.\n    Args:\n        data_uid (str): UID Hint of the unregistered dataset\n    \"\"\"\ncomms = config.comms\nui = config.ui\ndset = Dataset.get(data_uid)\nif dset.id is not None:\n# TODO: should get_dataset and update locally. solves existing issue?\nraise InvalidArgumentError(\"This dataset has already been registered\")\nremote_dsets = comms.get_user_datasets()\nremote_dset = [\nremote_dset\nfor remote_dset in remote_dsets\nif remote_dset[\"generated_uid\"] == dset.generated_uid\n]\nif len(remote_dset) == 1:\ndset = Dataset(**remote_dset[0])\ndset.write()\nui.print(f\"Remote dataset {dset.name} detected. Updating local dataset.\")\nreturn dset.id\ndict_pretty_print(dset.todict())\nmsg = \"Do you approve the registration of the presented data to the MLCommons comms? [Y/n] \"\napproved = approved or approval_prompt(msg)\ndset.status = Status(\"APPROVED\") if approved else Status(\"REJECTED\")\nif approved:\nui.print(\"Uploading...\")\nupdated_dset_dict = dset.upload()\nupdated_dset = Dataset(**updated_dset_dict)\nold_dset_loc = dset.path\nnew_dset_loc = updated_dset.path\nremove_path(new_dset_loc)\nos.rename(old_dset_loc, new_dset_loc)\nupdated_dset.write()\nreturn updated_dset.id\nelse:\nui.print(\"Registration request cancelled.\")\n</code></pre>"},{"location":"reference/commands/mlcube/associate/","title":"Associate","text":""},{"location":"reference/commands/mlcube/associate/#commands.mlcube.associate.AssociateCube","title":"<code>AssociateCube</code>","text":"Source code in <code>cli/medperf/commands/mlcube/associate.py</code> <pre><code>class AssociateCube:\n@classmethod\ndef run(\ncls, cube_uid: int, benchmark_uid: int, approved=False, no_cache=False,\n):\n\"\"\"Associates a cube with a given benchmark\n        Args:\n            cube_uid (int): UID of model MLCube\n            benchmark_uid (int): UID of benchmark\n            approved (bool): Skip validation step. Defualts to False\n        \"\"\"\ncomms = config.comms\nui = config.ui\ncube = Cube.get(cube_uid)\nbenchmark = Benchmark.get(benchmark_uid)\n_, results = CompatibilityTestExecution.run(\nbenchmark=benchmark_uid, model=cube_uid, no_cache=no_cache\n)\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(results)\nmsg = \"Please confirm that you would like to associate \"\nmsg += f\"the MLCube '{cube.name}' with the benchmark '{benchmark.name}' [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating mlcube benchmark association\")\nmetadata = {\"test_result\": results}\ncomms.associate_cube(cube_uid, benchmark_uid, metadata)\nelse:\nui.print(\"MLCube association operation cancelled\")\n</code></pre>"},{"location":"reference/commands/mlcube/associate/#commands.mlcube.associate.AssociateCube.run","title":"<code>run(cube_uid, benchmark_uid, approved=False, no_cache=False)</code>  <code>classmethod</code>","text":"<p>Associates a cube with a given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>UID of model MLCube</p> required <code>benchmark_uid</code> <code>int</code> <p>UID of benchmark</p> required <code>approved</code> <code>bool</code> <p>Skip validation step. Defualts to False</p> <code>False</code> Source code in <code>cli/medperf/commands/mlcube/associate.py</code> <pre><code>@classmethod\ndef run(\ncls, cube_uid: int, benchmark_uid: int, approved=False, no_cache=False,\n):\n\"\"\"Associates a cube with a given benchmark\n    Args:\n        cube_uid (int): UID of model MLCube\n        benchmark_uid (int): UID of benchmark\n        approved (bool): Skip validation step. Defualts to False\n    \"\"\"\ncomms = config.comms\nui = config.ui\ncube = Cube.get(cube_uid)\nbenchmark = Benchmark.get(benchmark_uid)\n_, results = CompatibilityTestExecution.run(\nbenchmark=benchmark_uid, model=cube_uid, no_cache=no_cache\n)\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(results)\nmsg = \"Please confirm that you would like to associate \"\nmsg += f\"the MLCube '{cube.name}' with the benchmark '{benchmark.name}' [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating mlcube benchmark association\")\nmetadata = {\"test_result\": results}\ncomms.associate_cube(cube_uid, benchmark_uid, metadata)\nelse:\nui.print(\"MLCube association operation cancelled\")\n</code></pre>"},{"location":"reference/commands/mlcube/create/","title":"Create","text":""},{"location":"reference/commands/mlcube/create/#commands.mlcube.create.CreateCube","title":"<code>CreateCube</code>","text":"Source code in <code>cli/medperf/commands/mlcube/create.py</code> <pre><code>class CreateCube:\n@classmethod\ndef run(cls, template_name: str, output_path: str = \".\", config_file: str = None):\n\"\"\"Creates a new MLCube based on one of the provided templates\n        Args:\n            template_name (str): The name of the template to use\n            output_path (str, Optional): The desired path for the MLCube. Defaults to current path.\n            config_file (str, Optional): Path to a JSON configuration file. If not passed, user is prompted.\n        \"\"\"\ntemplate_dirs = config.templates\nif template_name not in template_dirs:\ntemplates = list(template_dirs.keys())\nraise InvalidArgumentError(\nf\"Invalid template name. Available templates: [{' | '.join(templates)}]\"\n)\nno_input = False\nif config_file is not None:\nno_input = True\n# Get package parent path\npath = abspath(Path(__file__).parent.parent.parent)\ntemplate_dir = template_dirs[template_name]\ncookiecutter(\npath,\ndirectory=template_dir,\noutput_dir=output_path,\nconfig_file=config_file,\nno_input=no_input,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/create/#commands.mlcube.create.CreateCube.run","title":"<code>run(template_name, output_path='.', config_file=None)</code>  <code>classmethod</code>","text":"<p>Creates a new MLCube based on one of the provided templates</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>The name of the template to use</p> required <code>output_path</code> <code>(str, Optional)</code> <p>The desired path for the MLCube. Defaults to current path.</p> <code>'.'</code> <code>config_file</code> <code>(str, Optional)</code> <p>Path to a JSON configuration file. If not passed, user is prompted.</p> <code>None</code> Source code in <code>cli/medperf/commands/mlcube/create.py</code> <pre><code>@classmethod\ndef run(cls, template_name: str, output_path: str = \".\", config_file: str = None):\n\"\"\"Creates a new MLCube based on one of the provided templates\n    Args:\n        template_name (str): The name of the template to use\n        output_path (str, Optional): The desired path for the MLCube. Defaults to current path.\n        config_file (str, Optional): Path to a JSON configuration file. If not passed, user is prompted.\n    \"\"\"\ntemplate_dirs = config.templates\nif template_name not in template_dirs:\ntemplates = list(template_dirs.keys())\nraise InvalidArgumentError(\nf\"Invalid template name. Available templates: [{' | '.join(templates)}]\"\n)\nno_input = False\nif config_file is not None:\nno_input = True\n# Get package parent path\npath = abspath(Path(__file__).parent.parent.parent)\ntemplate_dir = template_dirs[template_name]\ncookiecutter(\npath,\ndirectory=template_dir,\noutput_dir=output_path,\nconfig_file=config_file,\nno_input=no_input,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/","title":"Mlcube","text":""},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.associate","title":"<code>associate(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='Benchmark UID'), model_uid=typer.Option(..., '--model_uid', '-m', help='Model UID'), approval=typer.Option(False, '-y', help='Skip approval step'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'))</code>","text":"<p>Associates an MLCube to a benchmark</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\nbenchmark_uid: int = typer.Option(..., \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\nmodel_uid: int = typer.Option(..., \"--model_uid\", \"-m\", help=\"Model UID\"),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute the test even if results already exist\",\n),\n):\n\"\"\"Associates an MLCube to a benchmark\"\"\"\nAssociateCube.run(model_uid, benchmark_uid, approved=approval, no_cache=no_cache)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.create","title":"<code>create(template=typer.Argument(..., help=f'MLCube template name. Available templates: [{' | '.join(config.templates.keys())}]'), output_path=typer.Option('.', '--output', '-o', help='Save the generated MLCube to the specified path'), config_file=typer.Option(None, '--config-file', '-c', help='JSON Configuration file. If not present then user is prompted for configuration'))</code>","text":"<p>Creates an MLCube based on one of the specified templates</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"create\")\n@clean_except\ndef create(\ntemplate: str = typer.Argument(\n...,\nhelp=f\"MLCube template name. Available templates: [{' | '.join(config.templates.keys())}]\",\n),\noutput_path: str = typer.Option(\n\".\", \"--output\", \"-o\", help=\"Save the generated MLCube to the specified path\"\n),\nconfig_file: str = typer.Option(\nNone,\n\"--config-file\",\n\"-c\",\nhelp=\"JSON Configuration file. If not present then user is prompted for configuration\",\n),\n):\n\"\"\"Creates an MLCube based on one of the specified templates\"\"\"\nCreateCube.run(template, output_path, config_file)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.list","title":"<code>list(local=typer.Option(False, '--local', help='Get local mlcubes'), mine=typer.Option(False, '--mine', help='Get current-user mlcubes'))</code>","text":"<p>List mlcubes stored locally and remotely from the user</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nlocal: bool = typer.Option(False, \"--local\", help=\"Get local mlcubes\"),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user mlcubes\"),\n):\n\"\"\"List mlcubes stored locally and remotely from the user\"\"\"\nEntityList.run(\nCube,\nfields=[\"UID\", \"Name\", \"State\", \"Registered\"],\nlocal_only=local,\nmine_only=mine,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the mlcube'), mlcube_file=typer.Option(..., '--mlcube-file', '-m', help='Identifier to download the mlcube file. See the description above'), mlcube_hash=typer.Option('', '--mlcube-hash', help='hash of mlcube file'), parameters_file=typer.Option('', '--parameters-file', '-p', help='Identifier to download the parameters file. See the description above'), parameters_hash=typer.Option('', '--parameters-hash', help='hash of parameters file'), additional_file=typer.Option('', '--additional-file', '-a', help='Identifier to download the additional files tarball. See the description above'), additional_hash=typer.Option('', '--additional-hash', help='hash of additional file'), image_file=typer.Option('', '--image-file', '-i', help='Identifier to download the image file. See the description above'), image_hash=typer.Option('', '--image-hash', help='hash of image file'))</code>","text":"<p>Submits a new cube to the platform.</p> The following assets <ul> <li> <p>mlcube_file</p> </li> <li> <p>parameters_file</p> </li> <li> <p>additional_file</p> </li> <li> <p>image_file</p> </li> </ul> <p>are expected to be given in the following format:  where <code>source_prefix</code> instructs the client how to download the resource, and <code>resource_identifier</code> is the identifier used to download the asset. The following are supported: <ol> <li> <p>A direct link: \"direct:\" <li> <p>An asset hosted on the Synapse platform: \"synapse:\" <p>If a URL is given without a source prefix, it will be treated as a direct download link.</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the mlcube\"),\nmlcube_file: str = typer.Option(\n...,\n\"--mlcube-file\",\n\"-m\",\nhelp=\"Identifier to download the mlcube file. See the description above\",\n),\nmlcube_hash: str = typer.Option(\"\", \"--mlcube-hash\", help=\"hash of mlcube file\"),\nparameters_file: str = typer.Option(\n\"\",\n\"--parameters-file\",\n\"-p\",\nhelp=\"Identifier to download the parameters file. See the description above\",\n),\nparameters_hash: str = typer.Option(\n\"\", \"--parameters-hash\", help=\"hash of parameters file\"\n),\nadditional_file: str = typer.Option(\n\"\",\n\"--additional-file\",\n\"-a\",\nhelp=\"Identifier to download the additional files tarball. See the description above\",\n),\nadditional_hash: str = typer.Option(\n\"\", \"--additional-hash\", help=\"hash of additional file\"\n),\nimage_file: str = typer.Option(\n\"\",\n\"--image-file\",\n\"-i\",\nhelp=\"Identifier to download the image file. See the description above\",\n),\nimage_hash: str = typer.Option(\"\", \"--image-hash\", help=\"hash of image file\"),\n):\n\"\"\"Submits a new cube to the platform.\\n\n    The following assets:\\n\n        - mlcube_file\\n\n        - parameters_file\\n\n        - additional_file\\n\n        - image_file\\n\n    are expected to be given in the following format: &lt;source_prefix:resource_identifier&gt;\n    where `source_prefix` instructs the client how to download the resource, and `resource_identifier`\n    is the identifier used to download the asset. The following are supported:\\n\n    1. A direct link: \"direct:&lt;URL&gt;\"\\n\n    2. An asset hosted on the Synapse platform: \"synapse:&lt;synapse ID&gt;\"\\n\\n\n    If a URL is given without a source prefix, it will be treated as a direct download link.\n    \"\"\"\nmlcube_info = {\n\"name\": name,\n\"git_mlcube_url\": mlcube_file,\n\"git_mlcube_hash\": mlcube_hash,\n\"git_parameters_url\": parameters_file,\n\"parameters_hash\": parameters_hash,\n\"image_tarball_url\": image_file,\n\"image_tarball_hash\": image_hash,\n\"additional_files_tarball_url\": additional_file,\n\"additional_files_tarball_hash\": additional_hash,\n}\nSubmitCube.run(mlcube_info)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.view","title":"<code>view(entity_id=typer.Argument(None, help='MLCube ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), local=typer.Option(False, '--local', help='Display local mlcubes if mlcube ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user mlcubes if mlcube ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more mlcubes</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"MLCube ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nlocal: bool = typer.Option(\nFalse, \"--local\", help=\"Display local mlcubes if mlcube ID is not provided\"\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user mlcubes if mlcube ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more mlcubes\"\"\"\nEntityView.run(entity_id, Cube, format, local, mine, output)\n</code></pre>"},{"location":"reference/commands/mlcube/submit/","title":"Submit","text":""},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube","title":"<code>SubmitCube</code>","text":"Source code in <code>cli/medperf/commands/mlcube/submit.py</code> <pre><code>class SubmitCube:\n@classmethod\ndef run(cls, submit_info: dict):\n\"\"\"Submits a new cube to the medperf platform\n        Args:\n            submit_info (dict): Dictionary containing the cube information.\n        \"\"\"\nui = config.ui\nsubmission = cls(submit_info)\nwith ui.interactive():\nui.text = \"Validating MLCube can be downloaded\"\nsubmission.download()\nui.text = \"Submitting MLCube to MedPerf\"\nupdated_cube_dict = submission.upload()\nsubmission.to_permanent_path(updated_cube_dict)\nsubmission.write(updated_cube_dict)\ndef __init__(self, submit_info: dict):\nself.comms = config.comms\nself.ui = config.ui\nself.cube = Cube(**submit_info)\nconfig.tmp_paths.append(self.cube.path)\ndef download(self):\nself.cube.download()\ndef upload(self):\nupdated_body = self.cube.upload()\nreturn updated_body\ndef to_permanent_path(self, cube_dict):\n\"\"\"Renames the temporary cube submission to a permanent one using the uid of\n        the registered cube\n        \"\"\"\nold_cube_loc = self.cube.path\nupdated_cube = Cube(**cube_dict)\nnew_cube_loc = updated_cube.path\nremove_path(new_cube_loc)\nos.rename(old_cube_loc, new_cube_loc)\ndef write(self, updated_cube_dict):\ncube = Cube(**updated_cube_dict)\ncube.write()\n</code></pre>"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube.run","title":"<code>run(submit_info)</code>  <code>classmethod</code>","text":"<p>Submits a new cube to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>submit_info</code> <code>dict</code> <p>Dictionary containing the cube information.</p> required Source code in <code>cli/medperf/commands/mlcube/submit.py</code> <pre><code>@classmethod\ndef run(cls, submit_info: dict):\n\"\"\"Submits a new cube to the medperf platform\n    Args:\n        submit_info (dict): Dictionary containing the cube information.\n    \"\"\"\nui = config.ui\nsubmission = cls(submit_info)\nwith ui.interactive():\nui.text = \"Validating MLCube can be downloaded\"\nsubmission.download()\nui.text = \"Submitting MLCube to MedPerf\"\nupdated_cube_dict = submission.upload()\nsubmission.to_permanent_path(updated_cube_dict)\nsubmission.write(updated_cube_dict)\n</code></pre>"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube.to_permanent_path","title":"<code>to_permanent_path(cube_dict)</code>","text":"<p>Renames the temporary cube submission to a permanent one using the uid of the registered cube</p> Source code in <code>cli/medperf/commands/mlcube/submit.py</code> <pre><code>def to_permanent_path(self, cube_dict):\n\"\"\"Renames the temporary cube submission to a permanent one using the uid of\n    the registered cube\n    \"\"\"\nold_cube_loc = self.cube.path\nupdated_cube = Cube(**cube_dict)\nnew_cube_loc = updated_cube.path\nremove_path(new_cube_loc)\nos.rename(old_cube_loc, new_cube_loc)\n</code></pre>"},{"location":"reference/commands/result/create/","title":"Create","text":""},{"location":"reference/commands/result/create/#commands.result.create.BenchmarkExecution","title":"<code>BenchmarkExecution</code>","text":"Source code in <code>cli/medperf/commands/result/create.py</code> <pre><code>class BenchmarkExecution:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\ndata_uid: int,\nmodels_uids: Optional[List[int]] = None,\nmodels_input_file: Optional[str] = None,\nignore_model_errors=False,\nignore_failed_experiments=False,\nno_cache=False,\nshow_summary=False,\n):\n\"\"\"Benchmark execution flow.\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n            data_uid (str): Registered Dataset UID\n            models_uids (List|None): list of model UIDs to execute.\n                                    if None, models_input_file will be used\n            models_input_file: filename to read from\n            if models_uids and models_input_file are None, use all benchmark models\n        \"\"\"\nexecution = cls(\nbenchmark_uid,\ndata_uid,\nmodels_uids,\nmodels_input_file,\nignore_model_errors,\nignore_failed_experiments,\n)\nexecution.prepare()\nexecution.validate()\nexecution.prepare_models()\nexecution.validate_models()\nif not no_cache:\nexecution.load_cached_results()\nwith execution.ui.interactive():\nresults = execution.run_experiments()\nif show_summary:\nexecution.print_summary()\nreturn results\ndef __init__(\nself,\nbenchmark_uid: int,\ndata_uid: int,\nmodels_uids,\nmodels_input_file: str = None,\nignore_model_errors=False,\nignore_failed_experiments=False,\n):\nself.benchmark_uid = benchmark_uid\nself.data_uid = data_uid\nself.models_uids = models_uids\nself.models_input_file = models_input_file\nself.ui = config.ui\nself.evaluator = None\nself.ignore_model_errors = ignore_model_errors\nself.ignore_failed_experiments = ignore_failed_experiments\nself.cached_results = {}\nself.experiments = []\ndef prepare(self):\nself.benchmark = Benchmark.get(self.benchmark_uid)\nself.ui.print(f\"Benchmark Execution: {self.benchmark.name}\")\nself.dataset = Dataset.get(self.data_uid)\nevaluator_uid = self.benchmark.data_evaluator_mlcube\nself.evaluator = self.__get_cube(evaluator_uid, \"Evaluator\")\ndef validate(self):\ndset_prep_cube = self.dataset.data_preparation_mlcube\nbmark_prep_cube = self.benchmark.data_preparation_mlcube\nif self.dataset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nif dset_prep_cube != bmark_prep_cube:\nmsg = \"The provided dataset is not compatible with the specified benchmark.\"\nraise InvalidArgumentError(msg)\ndef prepare_models(self):\nif self.models_input_file:\nself.models_uids = self.__get_models_from_file()\nelif self.models_uids is None:\nself.models_uids = self.benchmark.models\ndef __get_models_from_file(self):\nif not os.path.exists(self.models_input_file):\nraise InvalidArgumentError(\"The given file does not exist\")\nwith open(self.models_input_file) as f:\ntext = f.read()\nmodels = text.strip().split(\",\")\ntry:\nreturn list(map(int, models))\nexcept ValueError as e:\nmsg = f\"Could not parse the given file: {e}. \"\nmsg += \"The file should contain a list of comma-separated integers\"\nraise InvalidArgumentError(msg)\ndef validate_models(self):\nmodels_set = set(self.models_uids)\nbenchmark_models_set = set(self.benchmark.models)\nnon_assoc_cubes = models_set.difference(benchmark_models_set)\nif non_assoc_cubes:\nif len(non_assoc_cubes) &gt; 1:\nmsg = f\"Model of UID {non_assoc_cubes} is not associated with the specified benchmark.\"\nelse:\nmsg = f\"Models of UIDs {non_assoc_cubes} are not associated with the specified benchmark.\"\nraise InvalidArgumentError(msg)\ndef load_cached_results(self):\nresults = Result.all()\nbenchmark_dset_results = [\nresult\nfor result in results\nif result.benchmark == self.benchmark_uid\nand result.dataset == self.data_uid\n]\nself.cached_results = {\nresult.model: result for result in benchmark_dset_results\n}\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\nself.ui.text = f\"Retrieving {name} cube\"\ncube = Cube.get(uid)\nself.ui.print(f\"&gt; {name} cube download complete\")\nreturn cube\ndef run_experiments(self):\nfor model_uid in self.models_uids:\nif model_uid in self.cached_results:\nself.experiments.append(\n{\n\"model_uid\": model_uid,\n\"result\": self.cached_results[model_uid],\n\"cached\": True,\n\"error\": \"\",\n}\n)\ncontinue\ntry:\nmodel_cube = self.__get_cube(model_uid, \"Model\")\nexecution_summary = Execution.run(\ndataset=self.dataset,\nmodel=model_cube,\nevaluator=self.evaluator,\nignore_model_errors=self.ignore_model_errors,\n)\nexcept MedperfException as e:\nself.__handle_experiment_error(model_uid, e)\nself.experiments.append(\n{\n\"model_uid\": model_uid,\n\"result\": None,\n\"cached\": False,\n\"error\": str(e),\n}\n)\ncontinue\npartial = execution_summary[\"partial\"]\nresults = execution_summary[\"results\"]\nresult = self.__write_result(model_uid, results, partial)\nself.experiments.append(\n{\n\"model_uid\": model_uid,\n\"result\": result,\n\"cached\": False,\n\"error\": \"\",\n}\n)\nreturn [experiment[\"result\"] for experiment in self.experiments]\ndef __handle_experiment_error(self, model_uid, exception):\nif isinstance(exception, InvalidEntityError):\nconfig.ui.print_error(\nf\"There was an error when retrieving the model mlcube {model_uid}: {exception}\"\n)\nelif isinstance(exception, ExecutionError):\nconfig.ui.print_error(\nf\"There was an error when executing the benchmark with the model {model_uid}: {exception}\"\n)\nelse:\nraise exception\nif not self.ignore_failed_experiments:\nraise exception\ndef __result_dict(self, model_uid, results, partial):\nreturn {\n\"name\": f\"b{self.benchmark_uid}m{model_uid}d{self.data_uid}\",\n\"benchmark\": self.benchmark_uid,\n\"model\": model_uid,\n\"dataset\": self.data_uid,\n\"results\": results,\n\"metadata\": {\"partial\": partial},\n}\ndef __write_result(self, model_uid, results, partial):\nresults_info = self.__result_dict(model_uid, results, partial)\nresult = Result(**results_info)\nresult.write()\nreturn result\ndef print_summary(self):\nheaders = [\"model\", \"local result UID\", \"partial result\", \"from cache\", \"error\"]\ndata_lists_for_display = []\nnum_total = len(self.experiments)\nnum_success_run = 0\nnum_failed = 0\nnum_skipped = 0\nnum_partial_skipped = 0\nnum_partial_run = 0\nfor experiment in self.experiments:\n# populate display data\nif experiment[\"result\"]:\ndata_lists_for_display.append(\n[\nexperiment[\"model_uid\"],\nexperiment[\"result\"].generated_uid,\nexperiment[\"result\"].metadata[\"partial\"],\nexperiment[\"cached\"],\nexperiment[\"error\"],\n]\n)\nelse:\ndata_lists_for_display.append(\n[experiment[\"model_uid\"], \"\", \"\", \"\", experiment[\"error\"]]\n)\n# statistics\nif experiment[\"error\"]:\nnum_failed += 1\nelif experiment[\"cached\"]:\nnum_skipped += 1\nif experiment[\"result\"].metadata[\"partial\"]:\nnum_partial_skipped += 1\nelif experiment[\"result\"]:\nnum_success_run += 1\nif experiment[\"result\"].metadata[\"partial\"]:\nnum_partial_run += 1\ntab = tabulate(data_lists_for_display, headers=headers)\nmsg = f\"Total number of models: {num_total}\\n\"\nmsg += f\"\\t{num_skipped} were skipped (already executed), \"\nmsg += f\"of which {num_partial_run} have partial results\\n\"\nmsg += f\"\\t{num_failed} failed\\n\"\nmsg += f\"\\t{num_success_run} ran successfully, \"\nmsg += f\"of which {num_partial_run} have partial results\\n\"\nconfig.ui.print(tab)\nconfig.ui.print(msg)\n</code></pre>"},{"location":"reference/commands/result/create/#commands.result.create.BenchmarkExecution.run","title":"<code>run(benchmark_uid, data_uid, models_uids=None, models_input_file=None, ignore_model_errors=False, ignore_failed_experiments=False, no_cache=False, show_summary=False)</code>  <code>classmethod</code>","text":"<p>Benchmark execution flow.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <code>data_uid</code> <code>str</code> <p>Registered Dataset UID</p> required <code>models_uids</code> <code>List | None</code> <p>list of model UIDs to execute.                     if None, models_input_file will be used</p> <code>None</code> <code>models_input_file</code> <code>Optional[str]</code> <p>filename to read from</p> <code>None</code> Source code in <code>cli/medperf/commands/result/create.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\ndata_uid: int,\nmodels_uids: Optional[List[int]] = None,\nmodels_input_file: Optional[str] = None,\nignore_model_errors=False,\nignore_failed_experiments=False,\nno_cache=False,\nshow_summary=False,\n):\n\"\"\"Benchmark execution flow.\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n        data_uid (str): Registered Dataset UID\n        models_uids (List|None): list of model UIDs to execute.\n                                if None, models_input_file will be used\n        models_input_file: filename to read from\n        if models_uids and models_input_file are None, use all benchmark models\n    \"\"\"\nexecution = cls(\nbenchmark_uid,\ndata_uid,\nmodels_uids,\nmodels_input_file,\nignore_model_errors,\nignore_failed_experiments,\n)\nexecution.prepare()\nexecution.validate()\nexecution.prepare_models()\nexecution.validate_models()\nif not no_cache:\nexecution.load_cached_results()\nwith execution.ui.interactive():\nresults = execution.run_experiments()\nif show_summary:\nexecution.print_summary()\nreturn results\n</code></pre>"},{"location":"reference/commands/result/result/","title":"Result","text":""},{"location":"reference/commands/result/result/#commands.result.result.create","title":"<code>create(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='UID of the desired benchmark'), data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), model_uid=typer.Option(..., '--model_uid', '-m', help='UID of model to execute'), ignore_model_errors=typer.Option(False, '--ignore-model-errors', help='Ignore failing model cubes, allowing for possibly submitting partial results'), no_cache=typer.Option(False, '--no-cache', help='Execute even if results already exist'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/result/result.py</code> <pre><code>@app.command(\"create\")\n@clean_except\ndef create(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nmodel_uid: int = typer.Option(\n..., \"--model_uid\", \"-m\", help=\"UID of model to execute\"\n),\nignore_model_errors: bool = typer.Option(\nFalse,\n\"--ignore-model-errors\",\nhelp=\"Ignore failing model cubes, allowing for possibly submitting partial results\",\n),\nno_cache: bool = typer.Option(\nFalse, \"--no-cache\", help=\"Execute even if results already exist\",\n),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\n    \"\"\"\nBenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[model_uid],\nno_cache=no_cache,\nignore_model_errors=ignore_model_errors,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/result/result/#commands.result.result.list","title":"<code>list(local=typer.Option(False, '--local', help='Get local results'), mine=typer.Option(False, '--mine', help='Get current-user results'), benchmark=typer.Option(None, '--benchmark', '-b', help='Get results for a given benchmark'))</code>","text":"<p>List results stored locally and remotely from the user</p> Source code in <code>cli/medperf/commands/result/result.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nlocal: bool = typer.Option(False, \"--local\", help=\"Get local results\"),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user results\"),\nbenchmark: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"Get results for a given benchmark\"\n),\n):\n\"\"\"List results stored locally and remotely from the user\"\"\"\nEntityList.run(\nResult,\nfields=[\"UID\", \"Benchmark\", \"Model\", \"Dataset\", \"Registered\"],\nlocal_only=local,\nmine_only=mine,\nbenchmark=benchmark,\n)\n</code></pre>"},{"location":"reference/commands/result/result/#commands.result.result.submit","title":"<code>submit(result_uid=typer.Option(..., '--result', '-r', help='Unregistered result UID'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Submits already obtained results to the server</p> Source code in <code>cli/medperf/commands/result/result.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nresult_uid: str = typer.Option(\n..., \"--result\", \"-r\", help=\"Unregistered result UID\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Submits already obtained results to the server\"\"\"\nResultSubmission.run(result_uid, approved=approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/result/result/#commands.result.result.view","title":"<code>view(entity_id=typer.Argument(None, help='Result ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), local=typer.Option(False, '--local', help='Display local results if result ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user results if result ID is not provided'), benchmark=typer.Option(None, '--benchmark', '-b', help='Get results for a given benchmark'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more results</p> Source code in <code>cli/medperf/commands/result/result.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[str] = typer.Argument(None, help=\"Result ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nlocal: bool = typer.Option(\nFalse, \"--local\", help=\"Display local results if result ID is not provided\"\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user results if result ID is not provided\",\n),\nbenchmark: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"Get results for a given benchmark\"\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more results\n    \"\"\"\nEntityView.run(entity_id, Result, format, local, mine, output, benchmark=benchmark)\n</code></pre>"},{"location":"reference/commands/result/submit/","title":"Submit","text":""},{"location":"reference/commands/result/submit/#commands.result.submit.ResultSubmission","title":"<code>ResultSubmission</code>","text":"Source code in <code>cli/medperf/commands/result/submit.py</code> <pre><code>class ResultSubmission:\n@classmethod\ndef run(cls, result_uid, approved=False):\nsub = cls(result_uid, approved=approved)\nupdated_result_dict = sub.upload_results()\nsub.to_permanent_path(updated_result_dict)\nsub.write(updated_result_dict)\ndef __init__(self, result_uid, approved=False):\nself.result_uid = result_uid\nself.comms = config.comms\nself.ui = config.ui\nself.approved = approved\ndef request_approval(self, result):\nif result.approval_status == Status.APPROVED:\nreturn True\ndict_pretty_print(result.results)\nself.ui.print(\"Above are the results generated by the model\")\napproved = approval_prompt(\n\"Do you approve uploading the presented results to the MLCommons comms? [Y/n]\"\n)\nreturn approved\ndef upload_results(self):\nresult = Result.get(self.result_uid)\napproved = self.approved or self.request_approval(result)\nif not approved:\nraise CleanExit(\"Results upload operation cancelled\")\nupdated_result_dict = result.upload()\nreturn updated_result_dict\ndef to_permanent_path(self, result_dict: dict):\n\"\"\"Rename the temporary result submission to a permanent one\n        Args:\n            result_dict (dict): updated results dictionary\n        \"\"\"\nresult = Result(**result_dict)\nresult_storage = storage_path(config.results_storage)\nold_res_loc = os.path.join(result_storage, result.generated_uid)\nnew_res_loc = result.path\nremove_path(new_res_loc)\nos.rename(old_res_loc, new_res_loc)\ndef write(self, updated_result_dict):\nresult = Result(**updated_result_dict)\nresult.write()\n</code></pre>"},{"location":"reference/commands/result/submit/#commands.result.submit.ResultSubmission.to_permanent_path","title":"<code>to_permanent_path(result_dict)</code>","text":"<p>Rename the temporary result submission to a permanent one</p> <p>Parameters:</p> Name Type Description Default <code>result_dict</code> <code>dict</code> <p>updated results dictionary</p> required Source code in <code>cli/medperf/commands/result/submit.py</code> <pre><code>def to_permanent_path(self, result_dict: dict):\n\"\"\"Rename the temporary result submission to a permanent one\n    Args:\n        result_dict (dict): updated results dictionary\n    \"\"\"\nresult = Result(**result_dict)\nresult_storage = storage_path(config.results_storage)\nold_res_loc = os.path.join(result_storage, result.generated_uid)\nnew_res_loc = result.path\nremove_path(new_res_loc)\nos.rename(old_res_loc, new_res_loc)\n</code></pre>"},{"location":"reference/comms/factory/","title":"Factory","text":""},{"location":"reference/comms/interface/","title":"Interface","text":""},{"location":"reference/comms/interface/#comms.interface.Comms","title":"<code>Comms</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>class Comms(ABC):\n@abstractmethod\ndef __init__(self, source: str):\n\"\"\"Create an instance of a communication object.\n        Args:\n            source (str): location of the communication source. Where messages are going to be sent.\n            ui (UI): Implementation of the UI interface.\n            token (str, Optional): authentication token to be used throughout communication. Defaults to None.\n        \"\"\"\n@classmethod\n@abstractmethod\ndef parse_url(self, url: str) -&gt; str:\n\"\"\"Parse the source URL so that it can be used by the comms implementation.\n        It should handle protocols and versioning to be able to communicate with the API.\n        Args:\n            url (str): base URL\n        Returns:\n            str: parsed URL with protocol and version\n        \"\"\"\n@abstractmethod\ndef get_current_user(self):\n\"\"\"Retrieve the currently-authenticated user information\"\"\"\n@abstractmethod\ndef get_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks in the platform.\n        Returns:\n            List[dict]: all benchmarks information.\n        \"\"\"\n@abstractmethod\ndef get_benchmark(self, benchmark_uid: int) -&gt; dict:\n\"\"\"Retrieves the benchmark specification file from the server\n        Args:\n            benchmark_uid (int): uid for the desired benchmark\n        Returns:\n            dict: benchmark specification\n        \"\"\"\n@abstractmethod\ndef get_benchmark_models(self, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves all the models associated with a benchmark. reference model not included\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n        Returns:\n            list[int]: List of model UIDS\n        \"\"\"\n@abstractmethod\ndef get_user_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks created by the user\n        Returns:\n            List[dict]: Benchmarks data\n        \"\"\"\n@abstractmethod\ndef get_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves all MLCubes in the platform\n        Returns:\n            List[dict]: List containing the data of all MLCubes\n        \"\"\"\n@abstractmethod\ndef get_cube_metadata(self, cube_uid: int) -&gt; dict:\n\"\"\"Retrieves metadata about the specified cube\n        Args:\n            cube_uid (int): UID of the desired cube.\n        Returns:\n            dict: Dictionary containing url and hashes for the cube files\n        \"\"\"\n@abstractmethod\ndef get_user_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves metadata from all cubes registered by the user\n        Returns:\n            List[dict]: List of dictionaries containing the mlcubes registration information\n        \"\"\"\n@abstractmethod\ndef upload_benchmark(self, benchmark_dict: dict) -&gt; int:\n\"\"\"Uploads a new benchmark to the server.\n        Args:\n            benchmark_dict (dict): benchmark_data to be uploaded\n        Returns:\n            int: UID of newly created benchmark\n        \"\"\"\n@abstractmethod\ndef upload_mlcube(self, mlcube_body: dict) -&gt; int:\n\"\"\"Uploads an MLCube instance to the platform\n        Args:\n            mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes\n        Returns:\n            int: id of the created mlcube instance on the platform\n        \"\"\"\n@abstractmethod\ndef get_datasets(self) -&gt; List[dict]:\n\"\"\"Retrieves all datasets in the platform\n        Returns:\n            List[dict]: List of data from all datasets\n        \"\"\"\n@abstractmethod\ndef get_dataset(self, dset_uid: str) -&gt; dict:\n\"\"\"Retrieves a specific dataset\n        Args:\n            dset_uid (str): Dataset UID\n        Returns:\n            dict: Dataset metadata\n        \"\"\"\n@abstractmethod\ndef get_user_datasets(self) -&gt; dict:\n\"\"\"Retrieves all datasets registered by the user\n        Returns:\n            dict: dictionary with the contents of each dataset registration query\n        \"\"\"\n@abstractmethod\ndef upload_dataset(self, reg_dict: dict) -&gt; int:\n\"\"\"Uploads registration data to the server, under the sha name of the file.\n        Args:\n            reg_dict (dict): Dictionary containing registration information.\n        Returns:\n            int: id of the created dataset registration.\n        \"\"\"\n@abstractmethod\ndef get_results(self) -&gt; List[dict]:\n\"\"\"Retrieves all results\n        Returns:\n            List[dict]: List of results\n        \"\"\"\n@abstractmethod\ndef get_result(self, result_uid: str) -&gt; dict:\n\"\"\"Retrieves a specific result data\n        Args:\n            result_uid (str): Result UID\n        Returns:\n            dict: Result metadata\n        \"\"\"\n@abstractmethod\ndef get_user_results(self) -&gt; dict:\n\"\"\"Retrieves all results registered by the user\n        Returns:\n            dict: dictionary with the contents of each dataset registration query\n        \"\"\"\n@abstractmethod\ndef get_benchmark_results(self, benchmark_id: int) -&gt; dict:\n\"\"\"Retrieves all results for a given benchmark\n        Args:\n            benchmark_id (int): benchmark ID to retrieve results from\n        Returns:\n            dict: dictionary with the contents of each result in the specified benchmark\n        \"\"\"\n@abstractmethod\ndef upload_result(self, results_dict: dict) -&gt; int:\n\"\"\"Uploads result to the server.\n        Args:\n            results_dict (dict): Dictionary containing results information.\n        Returns:\n            int: id of the generated results entry\n        \"\"\"\n@abstractmethod\ndef associate_dset(self, data_uid: int, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create a Dataset Benchmark association\n        Args:\n            data_uid (int): Registered dataset UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\n@abstractmethod\ndef associate_cube(self, cube_uid: str, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create an MLCube-Benchmark association\n        Args:\n            cube_uid (str): MLCube UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\n@abstractmethod\ndef set_dataset_association_approval(\nself, dataset_uid: str, benchmark_uid: str, status: str\n):\n\"\"\"Approves a dataset association\n        Args:\n            dataset_uid (str): Dataset UID\n            benchmark_uid (str): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\n@abstractmethod\ndef set_mlcube_association_approval(\nself, mlcube_uid: str, benchmark_uid: str, status: str\n):\n\"\"\"Approves an mlcube association\n        Args:\n            mlcube_uid (str): Dataset UID\n            benchmark_uid (str): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\n@abstractmethod\ndef get_datasets_associations(self) -&gt; List[dict]:\n\"\"\"Get all dataset associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\n@abstractmethod\ndef get_cubes_associations(self) -&gt; List[dict]:\n\"\"\"Get all cube associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\n@abstractmethod\ndef set_mlcube_association_priority(\nself, benchmark_uid: str, mlcube_uid: str, priority: int\n):\n\"\"\"Sets the priority of an mlcube-benchmark association\n        Args:\n            mlcube_uid (str): MLCube UID\n            benchmark_uid (str): Benchmark UID\n            priority (int): priority value to set for the association\n        \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.__init__","title":"<code>__init__(source)</code>  <code>abstractmethod</code>","text":"<p>Create an instance of a communication object.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>location of the communication source. Where messages are going to be sent.</p> required <code>ui</code> <code>UI</code> <p>Implementation of the UI interface.</p> required <code>token</code> <code>(str, Optional)</code> <p>authentication token to be used throughout communication. Defaults to None.</p> required Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef __init__(self, source: str):\n\"\"\"Create an instance of a communication object.\n    Args:\n        source (str): location of the communication source. Where messages are going to be sent.\n        ui (UI): Implementation of the UI interface.\n        token (str, Optional): authentication token to be used throughout communication. Defaults to None.\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.associate_cube","title":"<code>associate_cube(cube_uid, benchmark_uid, metadata={})</code>  <code>abstractmethod</code>","text":"<p>Create an MLCube-Benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>str</code> <p>MLCube UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> <code>{}</code> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef associate_cube(self, cube_uid: str, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create an MLCube-Benchmark association\n    Args:\n        cube_uid (str): MLCube UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.associate_dset","title":"<code>associate_dset(data_uid, benchmark_uid, metadata={})</code>  <code>abstractmethod</code>","text":"<p>Create a Dataset Benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>Registered dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> <code>{}</code> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef associate_dset(self, data_uid: int, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create a Dataset Benchmark association\n    Args:\n        data_uid (int): Registered dataset UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmark","title":"<code>get_benchmark(benchmark_uid)</code>  <code>abstractmethod</code>","text":"<p>Retrieves the benchmark specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>uid for the desired benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark specification</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_benchmark(self, benchmark_uid: int) -&gt; dict:\n\"\"\"Retrieves the benchmark specification file from the server\n    Args:\n        benchmark_uid (int): uid for the desired benchmark\n    Returns:\n        dict: benchmark specification\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmark_models","title":"<code>get_benchmark_models(benchmark_uid)</code>  <code>abstractmethod</code>","text":"<p>Retrieves all the models associated with a benchmark. reference model not included</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>list[int]: List of model UIDS</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_benchmark_models(self, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves all the models associated with a benchmark. reference model not included\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n    Returns:\n        list[int]: List of model UIDS\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmark_results","title":"<code>get_benchmark_results(benchmark_id)</code>  <code>abstractmethod</code>","text":"<p>Retrieves all results for a given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>int</code> <p>benchmark ID to retrieve results from</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result in the specified benchmark</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_benchmark_results(self, benchmark_id: int) -&gt; dict:\n\"\"\"Retrieves all results for a given benchmark\n    Args:\n        benchmark_id (int): benchmark ID to retrieve results from\n    Returns:\n        dict: dictionary with the contents of each result in the specified benchmark\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmarks","title":"<code>get_benchmarks()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all benchmarks in the platform.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: all benchmarks information.</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks in the platform.\n    Returns:\n        List[dict]: all benchmarks information.\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cube_metadata","title":"<code>get_cube_metadata(cube_uid)</code>  <code>abstractmethod</code>","text":"<p>Retrieves metadata about the specified cube</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>UID of the desired cube.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing url and hashes for the cube files</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_cube_metadata(self, cube_uid: int) -&gt; dict:\n\"\"\"Retrieves metadata about the specified cube\n    Args:\n        cube_uid (int): UID of the desired cube.\n    Returns:\n        dict: Dictionary containing url and hashes for the cube files\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cubes","title":"<code>get_cubes()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all MLCubes in the platform</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing the data of all MLCubes</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves all MLCubes in the platform\n    Returns:\n        List[dict]: List containing the data of all MLCubes\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cubes_associations","title":"<code>get_cubes_associations()</code>  <code>abstractmethod</code>","text":"<p>Get all cube associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_cubes_associations(self) -&gt; List[dict]:\n\"\"\"Get all cube associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_current_user","title":"<code>get_current_user()</code>  <code>abstractmethod</code>","text":"<p>Retrieve the currently-authenticated user information</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_current_user(self):\n\"\"\"Retrieve the currently-authenticated user information\"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_dataset","title":"<code>get_dataset(dset_uid)</code>  <code>abstractmethod</code>","text":"<p>Retrieves a specific dataset</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>str</code> <p>Dataset UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dataset metadata</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_dataset(self, dset_uid: str) -&gt; dict:\n\"\"\"Retrieves a specific dataset\n    Args:\n        dset_uid (str): Dataset UID\n    Returns:\n        dict: Dataset metadata\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_datasets","title":"<code>get_datasets()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all datasets in the platform</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of data from all datasets</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_datasets(self) -&gt; List[dict]:\n\"\"\"Retrieves all datasets in the platform\n    Returns:\n        List[dict]: List of data from all datasets\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_datasets_associations","title":"<code>get_datasets_associations()</code>  <code>abstractmethod</code>","text":"<p>Get all dataset associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_datasets_associations(self) -&gt; List[dict]:\n\"\"\"Get all dataset associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_result","title":"<code>get_result(result_uid)</code>  <code>abstractmethod</code>","text":"<p>Retrieves a specific result data</p> <p>Parameters:</p> Name Type Description Default <code>result_uid</code> <code>str</code> <p>Result UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Result metadata</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_result(self, result_uid: str) -&gt; dict:\n\"\"\"Retrieves a specific result data\n    Args:\n        result_uid (str): Result UID\n    Returns:\n        dict: Result metadata\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_results","title":"<code>get_results()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all results</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of results</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_results(self) -&gt; List[dict]:\n\"\"\"Retrieves all results\n    Returns:\n        List[dict]: List of results\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_benchmarks","title":"<code>get_user_benchmarks()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all benchmarks created by the user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: Benchmarks data</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_user_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks created by the user\n    Returns:\n        List[dict]: Benchmarks data\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_cubes","title":"<code>get_user_cubes()</code>  <code>abstractmethod</code>","text":"<p>Retrieves metadata from all cubes registered by the user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dictionaries containing the mlcubes registration information</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_user_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves metadata from all cubes registered by the user\n    Returns:\n        List[dict]: List of dictionaries containing the mlcubes registration information\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_datasets","title":"<code>get_user_datasets()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all datasets registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each dataset registration query</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_user_datasets(self) -&gt; dict:\n\"\"\"Retrieves all datasets registered by the user\n    Returns:\n        dict: dictionary with the contents of each dataset registration query\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_results","title":"<code>get_user_results()</code>  <code>abstractmethod</code>","text":"<p>Retrieves all results registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each dataset registration query</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef get_user_results(self) -&gt; dict:\n\"\"\"Retrieves all results registered by the user\n    Returns:\n        dict: dictionary with the contents of each dataset registration query\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.parse_url","title":"<code>parse_url(url)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Parse the source URL so that it can be used by the comms implementation. It should handle protocols and versioning to be able to communicate with the API.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>base URL</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>parsed URL with protocol and version</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef parse_url(self, url: str) -&gt; str:\n\"\"\"Parse the source URL so that it can be used by the comms implementation.\n    It should handle protocols and versioning to be able to communicate with the API.\n    Args:\n        url (str): base URL\n    Returns:\n        str: parsed URL with protocol and version\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.set_dataset_association_approval","title":"<code>set_dataset_association_approval(dataset_uid, benchmark_uid, status)</code>  <code>abstractmethod</code>","text":"<p>Approves a dataset association</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>str</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>str</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef set_dataset_association_approval(\nself, dataset_uid: str, benchmark_uid: str, status: str\n):\n\"\"\"Approves a dataset association\n    Args:\n        dataset_uid (str): Dataset UID\n        benchmark_uid (str): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.set_mlcube_association_approval","title":"<code>set_mlcube_association_approval(mlcube_uid, benchmark_uid, status)</code>  <code>abstractmethod</code>","text":"<p>Approves an mlcube association</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_uid</code> <code>str</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>str</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef set_mlcube_association_approval(\nself, mlcube_uid: str, benchmark_uid: str, status: str\n):\n\"\"\"Approves an mlcube association\n    Args:\n        mlcube_uid (str): Dataset UID\n        benchmark_uid (str): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.set_mlcube_association_priority","title":"<code>set_mlcube_association_priority(benchmark_uid, mlcube_uid, priority)</code>  <code>abstractmethod</code>","text":"<p>Sets the priority of an mlcube-benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_uid</code> <code>str</code> <p>MLCube UID</p> required <code>benchmark_uid</code> <code>str</code> <p>Benchmark UID</p> required <code>priority</code> <code>int</code> <p>priority value to set for the association</p> required Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef set_mlcube_association_priority(\nself, benchmark_uid: str, mlcube_uid: str, priority: int\n):\n\"\"\"Sets the priority of an mlcube-benchmark association\n    Args:\n        mlcube_uid (str): MLCube UID\n        benchmark_uid (str): Benchmark UID\n        priority (int): priority value to set for the association\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_benchmark","title":"<code>upload_benchmark(benchmark_dict)</code>  <code>abstractmethod</code>","text":"<p>Uploads a new benchmark to the server.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_dict</code> <code>dict</code> <p>benchmark_data to be uploaded</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>UID of newly created benchmark</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef upload_benchmark(self, benchmark_dict: dict) -&gt; int:\n\"\"\"Uploads a new benchmark to the server.\n    Args:\n        benchmark_dict (dict): benchmark_data to be uploaded\n    Returns:\n        int: UID of newly created benchmark\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_dataset","title":"<code>upload_dataset(reg_dict)</code>  <code>abstractmethod</code>","text":"<p>Uploads registration data to the server, under the sha name of the file.</p> <p>Parameters:</p> Name Type Description Default <code>reg_dict</code> <code>dict</code> <p>Dictionary containing registration information.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the created dataset registration.</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef upload_dataset(self, reg_dict: dict) -&gt; int:\n\"\"\"Uploads registration data to the server, under the sha name of the file.\n    Args:\n        reg_dict (dict): Dictionary containing registration information.\n    Returns:\n        int: id of the created dataset registration.\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_mlcube","title":"<code>upload_mlcube(mlcube_body)</code>  <code>abstractmethod</code>","text":"<p>Uploads an MLCube instance to the platform</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_body</code> <code>dict</code> <p>Dictionary containing all the relevant data for creating mlcubes</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the created mlcube instance on the platform</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef upload_mlcube(self, mlcube_body: dict) -&gt; int:\n\"\"\"Uploads an MLCube instance to the platform\n    Args:\n        mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes\n    Returns:\n        int: id of the created mlcube instance on the platform\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_result","title":"<code>upload_result(results_dict)</code>  <code>abstractmethod</code>","text":"<p>Uploads result to the server.</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>dict</code> <p>Dictionary containing results information.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the generated results entry</p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef upload_result(self, results_dict: dict) -&gt; int:\n\"\"\"Uploads result to the server.\n    Args:\n        results_dict (dict): Dictionary containing results information.\n    Returns:\n        int: id of the generated results entry\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/rest/","title":"Rest","text":""},{"location":"reference/comms/rest/#comms.rest.REST","title":"<code>REST</code>","text":"<p>             Bases: <code>Comms</code></p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>class REST(Comms):\ndef __init__(self, source: str):\nself.server_url = self.parse_url(source)\nself.cert = config.certificate\nif self.cert is None:\n# No certificate provided, default to normal verification\nself.cert = True\n@classmethod\ndef parse_url(cls, url: str) -&gt; str:\n\"\"\"Parse the source URL so that it can be used by the comms implementation.\n        It should handle protocols and versioning to be able to communicate with the API.\n        Args:\n            url (str): base URL\n        Returns:\n            str: parsed URL with protocol and version\n        \"\"\"\nurl_sections = url.split(\"://\")\napi_path = f\"/api/v{config.major_version}\"\n# Remove protocol if passed\nif len(url_sections) &gt; 1:\nurl = \"\".join(url_sections[1:])\nreturn f\"https://{url}{api_path}\"\ndef __auth_get(self, url, **kwargs):\nreturn self.__auth_req(url, requests.get, **kwargs)\ndef __auth_post(self, url, **kwargs):\nreturn self.__auth_req(url, requests.post, **kwargs)\ndef __auth_put(self, url, **kwargs):\nreturn self.__auth_req(url, requests.put, **kwargs)\ndef __auth_req(self, url, req_func, **kwargs):\ntoken = config.auth.access_token\nreturn self.__req(\nurl, req_func, headers={\"Authorization\": f\"Bearer {token}\"}, **kwargs\n)\ndef __req(self, url, req_func, **kwargs):\nlogging.debug(f\"Calling {req_func}: {url}\")\nif \"json\" in kwargs:\nlogging.debug(f\"Passing JSON contents: {kwargs['json']}\")\nkwargs[\"json\"] = sanitize_json(kwargs[\"json\"])\ntry:\nreturn req_func(url, verify=self.cert, **kwargs)\nexcept requests.exceptions.SSLError as e:\nlogging.error(f\"Couldn't connect to {self.server_url}: {e}\")\nraise CommunicationError(\n\"Couldn't connect to server through HTTPS. If running locally, \"\n\"remember to provide the server certificate through --certificate\"\n)\ndef __get_list(\nself,\nurl,\nnum_elements=None,\npage_size=config.default_page_size,\noffset=0,\nbinary_reduction=False,\n):\n\"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained.\n        If num_elements is None, then iterates until all elements have been retrieved.\n        If binary_reduction is enabled, errors are assumed to be related to response size. In that case,\n        the page_size is reduced by half until a successful response is obtained or until page_size can't be\n        reduced anymore.\n        Args:\n            url (str): The url to retrieve elements from\n            num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None.\n            page_size (int, optional): Starting page size. Defaults to config.default_page_size.\n            start_limit (int, optional): The starting position for element retrieval. Defaults to 0.\n            binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False.\n        Returns:\n            List[dict]: A list of dictionaries representing the retrieved elements.\n        \"\"\"\nel_list = []\nif num_elements is None:\nnum_elements = float(\"inf\")\nwhile len(el_list) &lt; num_elements:\npaginated_url = f\"{url}?limit={page_size}&amp;offset={offset}\"\nres = self.__auth_get(paginated_url)\nif res.status_code != 200:\nif not binary_reduction:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"there was an error retrieving the current list: {details}\"\n)\nlog_response_error(res, warn=True)\ndetails = format_errors_dict(res.json())\nif page_size &lt;= 1:\nraise CommunicationRetrievalError(\nf\"Could not retrieve list. Minimum page size achieved without success: {details}\"\n)\npage_size = page_size // 2\ncontinue\nelse:\ndata = res.json()\nel_list += data[\"results\"]\noffset += len(data[\"results\"])\nif data[\"next\"] is None:\nbreak\nif isinstance(num_elements, int):\nreturn el_list[:num_elements]\nreturn el_list\ndef __set_approval_status(self, url: str, status: str) -&gt; requests.Response:\n\"\"\"Sets the approval status of a resource\n        Args:\n            url (str): URL to the resource to update\n            status (str): approval status to set\n        Returns:\n            requests.Response: Response object returned by the update\n        \"\"\"\ndata = {\"approval_status\": status}\nres = self.__auth_put(url, json=data)\nreturn res\ndef get_current_user(self):\n\"\"\"Retrieve the currently-authenticated user information\"\"\"\nres = self.__auth_get(f\"{self.server_url}/me/\")\nreturn res.json()\ndef get_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks in the platform.\n        Returns:\n            List[dict]: all benchmarks information.\n        \"\"\"\nbmks = self.__get_list(f\"{self.server_url}/benchmarks/\")\nreturn bmks\ndef get_benchmark(self, benchmark_uid: int) -&gt; dict:\n\"\"\"Retrieves the benchmark specification file from the server\n        Args:\n            benchmark_uid (int): uid for the desired benchmark\n        Returns:\n            dict: benchmark specification\n        \"\"\"\nres = self.__auth_get(f\"{self.server_url}/benchmarks/{benchmark_uid}\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"the specified benchmark doesn't exist: {details}\"\n)\nreturn res.json()\ndef get_benchmark_models(self, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves all the models associated with a benchmark. reference model not included\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n        Returns:\n            list[int]: List of model UIDS\n        \"\"\"\nmodels = self.__get_list(f\"{self.server_url}/benchmarks/{benchmark_uid}/models\")\nmodel_uids = [model[\"id\"] for model in models]\nreturn model_uids\ndef get_user_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks created by the user\n        Returns:\n            List[dict]: Benchmarks data\n        \"\"\"\nbmks = self.__get_list(f\"{self.server_url}/me/benchmarks/\")\nreturn bmks\ndef get_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves all MLCubes in the platform\n        Returns:\n            List[dict]: List containing the data of all MLCubes\n        \"\"\"\ncubes = self.__get_list(f\"{self.server_url}/mlcubes/\")\nreturn cubes\ndef get_cube_metadata(self, cube_uid: int) -&gt; dict:\n\"\"\"Retrieves metadata about the specified cube\n        Args:\n            cube_uid (int): UID of the desired cube.\n        Returns:\n            dict: Dictionary containing url and hashes for the cube files\n        \"\"\"\nres = self.__auth_get(f\"{self.server_url}/mlcubes/{cube_uid}/\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"the specified cube doesn't exist {details}\"\n)\nreturn res.json()\ndef get_user_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves metadata from all cubes registered by the user\n        Returns:\n            List[dict]: List of dictionaries containing the mlcubes registration information\n        \"\"\"\ncubes = self.__get_list(f\"{self.server_url}/me/mlcubes/\")\nreturn cubes\ndef upload_benchmark(self, benchmark_dict: dict) -&gt; int:\n\"\"\"Uploads a new benchmark to the server.\n        Args:\n            benchmark_dict (dict): benchmark_data to be uploaded\n        Returns:\n            int: UID of newly created benchmark\n        \"\"\"\nres = self.__auth_post(f\"{self.server_url}/benchmarks/\", json=benchmark_dict)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"Could not upload benchmark: {details}\")\nreturn res.json()\ndef upload_mlcube(self, mlcube_body: dict) -&gt; int:\n\"\"\"Uploads an MLCube instance to the platform\n        Args:\n            mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes\n        Returns:\n            int: id of the created mlcube instance on the platform\n        \"\"\"\nres = self.__auth_post(f\"{self.server_url}/mlcubes/\", json=mlcube_body)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"Could not upload the mlcube: {details}\")\nreturn res.json()\ndef get_datasets(self) -&gt; List[dict]:\n\"\"\"Retrieves all datasets in the platform\n        Returns:\n            List[dict]: List of data from all datasets\n        \"\"\"\ndsets = self.__get_list(f\"{self.server_url}/datasets/\")\nreturn dsets\ndef get_dataset(self, dset_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific dataset\n        Args:\n            dset_uid (int): Dataset UID\n        Returns:\n            dict: Dataset metadata\n        \"\"\"\nres = self.__auth_get(f\"{self.server_url}/datasets/{dset_uid}/\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"Could not retrieve the specified dataset from server: {details}\"\n)\nreturn res.json()\ndef get_user_datasets(self) -&gt; dict:\n\"\"\"Retrieves all datasets registered by the user\n        Returns:\n            dict: dictionary with the contents of each dataset registration query\n        \"\"\"\ndsets = self.__get_list(f\"{self.server_url}/me/datasets/\")\nreturn dsets\ndef upload_dataset(self, reg_dict: dict) -&gt; int:\n\"\"\"Uploads registration data to the server, under the sha name of the file.\n        Args:\n            reg_dict (dict): Dictionary containing registration information.\n        Returns:\n            int: id of the created dataset registration.\n        \"\"\"\nres = self.__auth_post(f\"{self.server_url}/datasets/\", json=reg_dict)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(f\"Could not upload the dataset: {details}\")\nreturn res.json()\ndef get_results(self) -&gt; List[dict]:\n\"\"\"Retrieves all results\n        Returns:\n            List[dict]: List of results\n        \"\"\"\nres = self.__get_list(f\"{self.server_url}/results\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"Could not retrieve results: {details}\")\nreturn res.json()\ndef get_result(self, result_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific result data\n        Args:\n            result_uid (int): Result UID\n        Returns:\n            dict: Result metadata\n        \"\"\"\nres = self.__auth_get(f\"{self.server_url}/results/{result_uid}/\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"Could not retrieve the specified result: {details}\"\n)\nreturn res.json()\ndef get_user_results(self) -&gt; dict:\n\"\"\"Retrieves all results registered by the user\n        Returns:\n            dict: dictionary with the contents of each result registration query\n        \"\"\"\nresults = self.__get_list(f\"{self.server_url}/me/results/\")\nreturn results\ndef get_benchmark_results(self, benchmark_id: int) -&gt; dict:\n\"\"\"Retrieves all results for a given benchmark\n        Args:\n            benchmark_id (int): benchmark ID to retrieve results from\n        Returns:\n            dict: dictionary with the contents of each result in the specified benchmark\n        \"\"\"\nresults = self.__get_list(\nf\"{self.server_url}/benchmarks/{benchmark_id}/results\"\n)\nreturn results\ndef upload_result(self, results_dict: dict) -&gt; int:\n\"\"\"Uploads result to the server.\n        Args:\n            results_dict (dict): Dictionary containing results information.\n        Returns:\n            int: id of the generated results entry\n        \"\"\"\nres = self.__auth_post(f\"{self.server_url}/results/\", json=results_dict)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(f\"Could not upload the results: {details}\")\nreturn res.json()\ndef associate_dset(self, data_uid: int, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create a Dataset Benchmark association\n        Args:\n            data_uid (int): Registered dataset UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\ndata = {\n\"dataset\": data_uid,\n\"benchmark\": benchmark_uid,\n\"approval_status\": Status.PENDING.value,\n\"metadata\": metadata,\n}\nres = self.__auth_post(f\"{self.server_url}/datasets/benchmarks/\", json=data)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not associate dataset to benchmark: {details}\"\n)\ndef associate_cube(self, cube_uid: int, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create an MLCube-Benchmark association\n        Args:\n            cube_uid (int): MLCube UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\ndata = {\n\"approval_status\": Status.PENDING.value,\n\"model_mlcube\": cube_uid,\n\"benchmark\": benchmark_uid,\n\"metadata\": metadata,\n}\nres = self.__auth_post(f\"{self.server_url}/mlcubes/benchmarks/\", json=data)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not associate mlcube to benchmark: {details}\"\n)\ndef set_dataset_association_approval(\nself, benchmark_uid: int, dataset_uid: int, status: str\n):\n\"\"\"Approves a dataset association\n        Args:\n            dataset_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/datasets/{dataset_uid}/benchmarks/{benchmark_uid}/\"\nres = self.__set_approval_status(url, status)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not approve association between dataset {dataset_uid} and benchmark {benchmark_uid}: {details}\"\n)\ndef set_mlcube_association_approval(\nself, benchmark_uid: int, mlcube_uid: int, status: str\n):\n\"\"\"Approves an mlcube association\n        Args:\n            mlcube_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_uid}/benchmarks/{benchmark_uid}/\"\nres = self.__set_approval_status(url, status)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not approve association between mlcube {mlcube_uid} and benchmark {benchmark_uid}: {details}\"\n)\ndef get_datasets_associations(self) -&gt; List[dict]:\n\"\"\"Get all dataset associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nassocs = self.__get_list(f\"{self.server_url}/me/datasets/associations/\")\nreturn assocs\ndef get_cubes_associations(self) -&gt; List[dict]:\n\"\"\"Get all cube associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nassocs = self.__get_list(f\"{self.server_url}/me/mlcubes/associations/\")\nreturn assocs\ndef set_mlcube_association_priority(\nself, benchmark_uid: int, mlcube_uid: int, priority: int\n):\n\"\"\"Sets the priority of an mlcube-benchmark association\n        Args:\n            mlcube_uid (int): MLCube UID\n            benchmark_uid (int): Benchmark UID\n            priority (int): priority value to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_uid}/benchmarks/{benchmark_uid}/\"\ndata = {\"priority\": priority}\nres = self.__auth_put(url, json=data)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not set the priority of mlcube {mlcube_uid} within the benchmark {benchmark_uid}: {details}\"\n)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.__get_list","title":"<code>__get_list(url, num_elements=None, page_size=config.default_page_size, offset=0, binary_reduction=False)</code>","text":"<p>Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to retrieve elements from</p> required <code>num_elements</code> <code>int</code> <p>The desired number of elements to be retrieved. Defaults to None.</p> <code>None</code> <code>page_size</code> <code>int</code> <p>Starting page size. Defaults to config.default_page_size.</p> <code>config.default_page_size</code> <code>start_limit</code> <code>int</code> <p>The starting position for element retrieval. Defaults to 0.</p> required <code>binary_reduction</code> <code>bool</code> <p>Wether to handle errors by halfing the page size. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>List[dict]: A list of dictionaries representing the retrieved elements.</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def __get_list(\nself,\nurl,\nnum_elements=None,\npage_size=config.default_page_size,\noffset=0,\nbinary_reduction=False,\n):\n\"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained.\n    If num_elements is None, then iterates until all elements have been retrieved.\n    If binary_reduction is enabled, errors are assumed to be related to response size. In that case,\n    the page_size is reduced by half until a successful response is obtained or until page_size can't be\n    reduced anymore.\n    Args:\n        url (str): The url to retrieve elements from\n        num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None.\n        page_size (int, optional): Starting page size. Defaults to config.default_page_size.\n        start_limit (int, optional): The starting position for element retrieval. Defaults to 0.\n        binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False.\n    Returns:\n        List[dict]: A list of dictionaries representing the retrieved elements.\n    \"\"\"\nel_list = []\nif num_elements is None:\nnum_elements = float(\"inf\")\nwhile len(el_list) &lt; num_elements:\npaginated_url = f\"{url}?limit={page_size}&amp;offset={offset}\"\nres = self.__auth_get(paginated_url)\nif res.status_code != 200:\nif not binary_reduction:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"there was an error retrieving the current list: {details}\"\n)\nlog_response_error(res, warn=True)\ndetails = format_errors_dict(res.json())\nif page_size &lt;= 1:\nraise CommunicationRetrievalError(\nf\"Could not retrieve list. Minimum page size achieved without success: {details}\"\n)\npage_size = page_size // 2\ncontinue\nelse:\ndata = res.json()\nel_list += data[\"results\"]\noffset += len(data[\"results\"])\nif data[\"next\"] is None:\nbreak\nif isinstance(num_elements, int):\nreturn el_list[:num_elements]\nreturn el_list\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.__set_approval_status","title":"<code>__set_approval_status(url, status)</code>","text":"<p>Sets the approval status of a resource</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to the resource to update</p> required <code>status</code> <code>str</code> <p>approval status to set</p> required <p>Returns:</p> Type Description <code>requests.Response</code> <p>requests.Response: Response object returned by the update</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def __set_approval_status(self, url: str, status: str) -&gt; requests.Response:\n\"\"\"Sets the approval status of a resource\n    Args:\n        url (str): URL to the resource to update\n        status (str): approval status to set\n    Returns:\n        requests.Response: Response object returned by the update\n    \"\"\"\ndata = {\"approval_status\": status}\nres = self.__auth_put(url, json=data)\nreturn res\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_cube","title":"<code>associate_cube(cube_uid, benchmark_uid, metadata={})</code>","text":"<p>Create an MLCube-Benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>MLCube UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> <code>{}</code> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_cube(self, cube_uid: int, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create an MLCube-Benchmark association\n    Args:\n        cube_uid (int): MLCube UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\ndata = {\n\"approval_status\": Status.PENDING.value,\n\"model_mlcube\": cube_uid,\n\"benchmark\": benchmark_uid,\n\"metadata\": metadata,\n}\nres = self.__auth_post(f\"{self.server_url}/mlcubes/benchmarks/\", json=data)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not associate mlcube to benchmark: {details}\"\n)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_dset","title":"<code>associate_dset(data_uid, benchmark_uid, metadata={})</code>","text":"<p>Create a Dataset Benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>Registered dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> <code>{}</code> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_dset(self, data_uid: int, benchmark_uid: int, metadata: dict = {}):\n\"\"\"Create a Dataset Benchmark association\n    Args:\n        data_uid (int): Registered dataset UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\ndata = {\n\"dataset\": data_uid,\n\"benchmark\": benchmark_uid,\n\"approval_status\": Status.PENDING.value,\n\"metadata\": metadata,\n}\nres = self.__auth_post(f\"{self.server_url}/datasets/benchmarks/\", json=data)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not associate dataset to benchmark: {details}\"\n)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark","title":"<code>get_benchmark(benchmark_uid)</code>","text":"<p>Retrieves the benchmark specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>uid for the desired benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark(self, benchmark_uid: int) -&gt; dict:\n\"\"\"Retrieves the benchmark specification file from the server\n    Args:\n        benchmark_uid (int): uid for the desired benchmark\n    Returns:\n        dict: benchmark specification\n    \"\"\"\nres = self.__auth_get(f\"{self.server_url}/benchmarks/{benchmark_uid}\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"the specified benchmark doesn't exist: {details}\"\n)\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_models","title":"<code>get_benchmark_models(benchmark_uid)</code>","text":"<p>Retrieves all the models associated with a benchmark. reference model not included</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>list[int]: List of model UIDS</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark_models(self, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves all the models associated with a benchmark. reference model not included\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n    Returns:\n        list[int]: List of model UIDS\n    \"\"\"\nmodels = self.__get_list(f\"{self.server_url}/benchmarks/{benchmark_uid}/models\")\nmodel_uids = [model[\"id\"] for model in models]\nreturn model_uids\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_results","title":"<code>get_benchmark_results(benchmark_id)</code>","text":"<p>Retrieves all results for a given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>int</code> <p>benchmark ID to retrieve results from</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result in the specified benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark_results(self, benchmark_id: int) -&gt; dict:\n\"\"\"Retrieves all results for a given benchmark\n    Args:\n        benchmark_id (int): benchmark ID to retrieve results from\n    Returns:\n        dict: dictionary with the contents of each result in the specified benchmark\n    \"\"\"\nresults = self.__get_list(\nf\"{self.server_url}/benchmarks/{benchmark_id}/results\"\n)\nreturn results\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmarks","title":"<code>get_benchmarks()</code>","text":"<p>Retrieves all benchmarks in the platform.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: all benchmarks information.</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks in the platform.\n    Returns:\n        List[dict]: all benchmarks information.\n    \"\"\"\nbmks = self.__get_list(f\"{self.server_url}/benchmarks/\")\nreturn bmks\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube_metadata","title":"<code>get_cube_metadata(cube_uid)</code>","text":"<p>Retrieves metadata about the specified cube</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>UID of the desired cube.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing url and hashes for the cube files</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_cube_metadata(self, cube_uid: int) -&gt; dict:\n\"\"\"Retrieves metadata about the specified cube\n    Args:\n        cube_uid (int): UID of the desired cube.\n    Returns:\n        dict: Dictionary containing url and hashes for the cube files\n    \"\"\"\nres = self.__auth_get(f\"{self.server_url}/mlcubes/{cube_uid}/\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"the specified cube doesn't exist {details}\"\n)\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_cubes","title":"<code>get_cubes()</code>","text":"<p>Retrieves all MLCubes in the platform</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing the data of all MLCubes</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves all MLCubes in the platform\n    Returns:\n        List[dict]: List containing the data of all MLCubes\n    \"\"\"\ncubes = self.__get_list(f\"{self.server_url}/mlcubes/\")\nreturn cubes\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_cubes_associations","title":"<code>get_cubes_associations()</code>","text":"<p>Get all cube associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_cubes_associations(self) -&gt; List[dict]:\n\"\"\"Get all cube associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nassocs = self.__get_list(f\"{self.server_url}/me/mlcubes/associations/\")\nreturn assocs\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_current_user","title":"<code>get_current_user()</code>","text":"<p>Retrieve the currently-authenticated user information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_current_user(self):\n\"\"\"Retrieve the currently-authenticated user information\"\"\"\nres = self.__auth_get(f\"{self.server_url}/me/\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_dataset","title":"<code>get_dataset(dset_uid)</code>","text":"<p>Retrieves a specific dataset</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>int</code> <p>Dataset UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dataset metadata</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_dataset(self, dset_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific dataset\n    Args:\n        dset_uid (int): Dataset UID\n    Returns:\n        dict: Dataset metadata\n    \"\"\"\nres = self.__auth_get(f\"{self.server_url}/datasets/{dset_uid}/\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"Could not retrieve the specified dataset from server: {details}\"\n)\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_datasets","title":"<code>get_datasets()</code>","text":"<p>Retrieves all datasets in the platform</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of data from all datasets</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_datasets(self) -&gt; List[dict]:\n\"\"\"Retrieves all datasets in the platform\n    Returns:\n        List[dict]: List of data from all datasets\n    \"\"\"\ndsets = self.__get_list(f\"{self.server_url}/datasets/\")\nreturn dsets\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_datasets_associations","title":"<code>get_datasets_associations()</code>","text":"<p>Get all dataset associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_datasets_associations(self) -&gt; List[dict]:\n\"\"\"Get all dataset associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nassocs = self.__get_list(f\"{self.server_url}/me/datasets/associations/\")\nreturn assocs\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_result","title":"<code>get_result(result_uid)</code>","text":"<p>Retrieves a specific result data</p> <p>Parameters:</p> Name Type Description Default <code>result_uid</code> <code>int</code> <p>Result UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Result metadata</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_result(self, result_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific result data\n    Args:\n        result_uid (int): Result UID\n    Returns:\n        dict: Result metadata\n    \"\"\"\nres = self.__auth_get(f\"{self.server_url}/results/{result_uid}/\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(\nf\"Could not retrieve the specified result: {details}\"\n)\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_results","title":"<code>get_results()</code>","text":"<p>Retrieves all results</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of results</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_results(self) -&gt; List[dict]:\n\"\"\"Retrieves all results\n    Returns:\n        List[dict]: List of results\n    \"\"\"\nres = self.__get_list(f\"{self.server_url}/results\")\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"Could not retrieve results: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_benchmarks","title":"<code>get_user_benchmarks()</code>","text":"<p>Retrieves all benchmarks created by the user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: Benchmarks data</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_benchmarks(self) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks created by the user\n    Returns:\n        List[dict]: Benchmarks data\n    \"\"\"\nbmks = self.__get_list(f\"{self.server_url}/me/benchmarks/\")\nreturn bmks\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_cubes","title":"<code>get_user_cubes()</code>","text":"<p>Retrieves metadata from all cubes registered by the user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dictionaries containing the mlcubes registration information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_cubes(self) -&gt; List[dict]:\n\"\"\"Retrieves metadata from all cubes registered by the user\n    Returns:\n        List[dict]: List of dictionaries containing the mlcubes registration information\n    \"\"\"\ncubes = self.__get_list(f\"{self.server_url}/me/mlcubes/\")\nreturn cubes\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_datasets","title":"<code>get_user_datasets()</code>","text":"<p>Retrieves all datasets registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each dataset registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_datasets(self) -&gt; dict:\n\"\"\"Retrieves all datasets registered by the user\n    Returns:\n        dict: dictionary with the contents of each dataset registration query\n    \"\"\"\ndsets = self.__get_list(f\"{self.server_url}/me/datasets/\")\nreturn dsets\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_results","title":"<code>get_user_results()</code>","text":"<p>Retrieves all results registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_results(self) -&gt; dict:\n\"\"\"Retrieves all results registered by the user\n    Returns:\n        dict: dictionary with the contents of each result registration query\n    \"\"\"\nresults = self.__get_list(f\"{self.server_url}/me/results/\")\nreturn results\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.parse_url","title":"<code>parse_url(url)</code>  <code>classmethod</code>","text":"<p>Parse the source URL so that it can be used by the comms implementation. It should handle protocols and versioning to be able to communicate with the API.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>base URL</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>parsed URL with protocol and version</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>@classmethod\ndef parse_url(cls, url: str) -&gt; str:\n\"\"\"Parse the source URL so that it can be used by the comms implementation.\n    It should handle protocols and versioning to be able to communicate with the API.\n    Args:\n        url (str): base URL\n    Returns:\n        str: parsed URL with protocol and version\n    \"\"\"\nurl_sections = url.split(\"://\")\napi_path = f\"/api/v{config.major_version}\"\n# Remove protocol if passed\nif len(url_sections) &gt; 1:\nurl = \"\".join(url_sections[1:])\nreturn f\"https://{url}{api_path}\"\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.set_dataset_association_approval","title":"<code>set_dataset_association_approval(benchmark_uid, dataset_uid, status)</code>","text":"<p>Approves a dataset association</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def set_dataset_association_approval(\nself, benchmark_uid: int, dataset_uid: int, status: str\n):\n\"\"\"Approves a dataset association\n    Args:\n        dataset_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/datasets/{dataset_uid}/benchmarks/{benchmark_uid}/\"\nres = self.__set_approval_status(url, status)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not approve association between dataset {dataset_uid} and benchmark {benchmark_uid}: {details}\"\n)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.set_mlcube_association_approval","title":"<code>set_mlcube_association_approval(benchmark_uid, mlcube_uid, status)</code>","text":"<p>Approves an mlcube association</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def set_mlcube_association_approval(\nself, benchmark_uid: int, mlcube_uid: int, status: str\n):\n\"\"\"Approves an mlcube association\n    Args:\n        mlcube_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_uid}/benchmarks/{benchmark_uid}/\"\nres = self.__set_approval_status(url, status)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not approve association between mlcube {mlcube_uid} and benchmark {benchmark_uid}: {details}\"\n)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.set_mlcube_association_priority","title":"<code>set_mlcube_association_priority(benchmark_uid, mlcube_uid, priority)</code>","text":"<p>Sets the priority of an mlcube-benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_uid</code> <code>int</code> <p>MLCube UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>priority</code> <code>int</code> <p>priority value to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def set_mlcube_association_priority(\nself, benchmark_uid: int, mlcube_uid: int, priority: int\n):\n\"\"\"Sets the priority of an mlcube-benchmark association\n    Args:\n        mlcube_uid (int): MLCube UID\n        benchmark_uid (int): Benchmark UID\n        priority (int): priority value to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_uid}/benchmarks/{benchmark_uid}/\"\ndata = {\"priority\": priority}\nres = self.__auth_put(url, json=data)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(\nf\"Could not set the priority of mlcube {mlcube_uid} within the benchmark {benchmark_uid}: {details}\"\n)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_benchmark","title":"<code>upload_benchmark(benchmark_dict)</code>","text":"<p>Uploads a new benchmark to the server.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_dict</code> <code>dict</code> <p>benchmark_data to be uploaded</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>UID of newly created benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_benchmark(self, benchmark_dict: dict) -&gt; int:\n\"\"\"Uploads a new benchmark to the server.\n    Args:\n        benchmark_dict (dict): benchmark_data to be uploaded\n    Returns:\n        int: UID of newly created benchmark\n    \"\"\"\nres = self.__auth_post(f\"{self.server_url}/benchmarks/\", json=benchmark_dict)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"Could not upload benchmark: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_dataset","title":"<code>upload_dataset(reg_dict)</code>","text":"<p>Uploads registration data to the server, under the sha name of the file.</p> <p>Parameters:</p> Name Type Description Default <code>reg_dict</code> <code>dict</code> <p>Dictionary containing registration information.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the created dataset registration.</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_dataset(self, reg_dict: dict) -&gt; int:\n\"\"\"Uploads registration data to the server, under the sha name of the file.\n    Args:\n        reg_dict (dict): Dictionary containing registration information.\n    Returns:\n        int: id of the created dataset registration.\n    \"\"\"\nres = self.__auth_post(f\"{self.server_url}/datasets/\", json=reg_dict)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(f\"Could not upload the dataset: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_mlcube","title":"<code>upload_mlcube(mlcube_body)</code>","text":"<p>Uploads an MLCube instance to the platform</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_body</code> <code>dict</code> <p>Dictionary containing all the relevant data for creating mlcubes</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the created mlcube instance on the platform</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_mlcube(self, mlcube_body: dict) -&gt; int:\n\"\"\"Uploads an MLCube instance to the platform\n    Args:\n        mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes\n    Returns:\n        int: id of the created mlcube instance on the platform\n    \"\"\"\nres = self.__auth_post(f\"{self.server_url}/mlcubes/\", json=mlcube_body)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"Could not upload the mlcube: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_result","title":"<code>upload_result(results_dict)</code>","text":"<p>Uploads result to the server.</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>dict</code> <p>Dictionary containing results information.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the generated results entry</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_result(self, results_dict: dict) -&gt; int:\n\"\"\"Uploads result to the server.\n    Args:\n        results_dict (dict): Dictionary containing results information.\n    Returns:\n        int: id of the generated results entry\n    \"\"\"\nres = self.__auth_post(f\"{self.server_url}/results/\", json=results_dict)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(f\"Could not upload the results: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/auth/auth0/","title":"Auth0","text":""},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0","title":"<code>Auth0</code>","text":"<p>             Bases: <code>Auth</code></p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>class Auth0(Auth):\ndef __init__(self):\nself.domain = config.auth_domain\nself.client_id = config.auth_client_id\nself.audience = config.auth_audience\ndef login(self, email):\n\"\"\"Retrieves and stores an access token/refresh token pair from the auth0\n        backend using the device authorization flow.\n        Args:\n            email (str): user email. This will be used to validate that the received\n                         id_token contains the same email address.\n        \"\"\"\ndevice_code_response = self.__request_device_code()\ndevice_code = device_code_response[\"device_code\"]\nuser_code = device_code_response[\"user_code\"]\nverification_uri_complete = device_code_response[\"verification_uri_complete\"]\ninterval = device_code_response[\"interval\"]\nconfig.ui.print(\n\"\\nPlease go to the following link to complete your login request:\\n\"\nf\"\\t{verification_uri_complete}\\n\\n\"\n\"Make sure that you will be presented with the following code:\\n\"\nf\"\\t{user_code}\\n\\n\"\n)\nconfig.ui.print_warning(\n\"Keep this terminal open until you complete your login request. \"\n\"The command will exit on its own once you complete the request. \"\n\"If you wish to stop the login request anyway, press Ctrl+C.\"\n)\ntoken_response, token_issued_at = self.__get_device_access_token(\ndevice_code, interval\n)\naccess_token = token_response[\"access_token\"]\nid_token = token_response[\"id_token\"]\nrefresh_token = token_response[\"refresh_token\"]\ntoken_expires_in = token_response[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nself.__check_token_email(id_token_payload, email)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\n)\ndef __request_device_code(self):\n\"\"\"Get a device code from the auth0 backend to be used for the authorization process\"\"\"\nurl = f\"https://{self.domain}/oauth/device/code\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"client_id\": self.client_id,\n\"audience\": self.audience,\n\"scope\": \"offline_access openid email\",\n}\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Login\")\nreturn res.json()\ndef __get_device_access_token(self, device_code, polling_interval):\n\"\"\"Get the access token from the auth0 backend associated with\n        the device code requested before. This function will keep polling\n        the access token until the user completes the browser flow part\n        of the authorization process.\n        Args:\n            device_code (str): A temporary device code requested by `__request_device_code`\n            polling_interval (float): number of seconds to wait between each two polling requests\n        Returns:\n            json_res (dict): the response of the successful request, containg the access/refresh tokens pair\n            token_issued_at (float): the timestamp when the access token was issued\n        \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n\"device_code\": device_code,\n\"client_id\": self.client_id,\n}\nwhile True:\ntime.sleep(polling_interval)\ntoken_issued_at = time.time()\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code == 200:\njson_res = res.json()\nreturn json_res, token_issued_at\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\nerror = json_res.get(\"error\", None)\nif error not in [\"slow_down\", \"authorization_pending\"]:\nself.__raise_errors(res, \"Login\")\ndef __check_token_email(self, id_token_payload, email):\n\"\"\"Checks if the email provided by the user in the terminal matches the\n        email found in the recieved id token.\"\"\"\nemail_in_token = id_token_payload[\"email\"]\nif email.lower() != email_in_token:\nraise CommunicationError(\n\"The email provided in the terminal does not match the one provided during login\"\n)\ndef logout(self):\n\"\"\"Logs out the user by revoking their refresh token and deleting the\n        stored tokens.\"\"\"\ncreds = read_credentials()\nrefresh_token = creds[\"refresh_token\"]\nurl = f\"https://{self.domain}/oauth/revoke\"\nheaders = {\"content-type\": \"application/json\"}\nbody = {\n\"client_id\": self.client_id,\n\"token\": refresh_token,\n}\nres = requests.post(url=url, headers=headers, json=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Logout\")\ndelete_credentials()\n@property\ndef access_token(self):\n\"\"\"Reads and returns an access token of the currently logged\n        in user to be used for authorizing requests to the MedPerf server.\n        Refresh the token if necessary.\n        Returns:\n            access_token (str): the access token\n        \"\"\"\ncreds = read_credentials()\naccess_token = creds[\"access_token\"]\nrefresh_token = creds[\"refresh_token\"]\ntoken_expires_in = creds[\"token_expires_in\"]\ntoken_issued_at = creds[\"token_issued_at\"]\nif (\ntime.time()\n&gt; token_issued_at + token_expires_in - config.token_expiration_leeway\n):\naccess_token = self.__refresh_access_token(refresh_token)\nreturn access_token\ndef __refresh_access_token(self, refresh_token):\n\"\"\"Retrieve and store a new access token using a refresh token.\n        A new refresh token will also be retrieved and stored.\n        Args:\n            refresh_token (str): the refresh token\n        Returns:\n            access_token (str): the new access token\n        \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"refresh_token\",\n\"client_id\": self.client_id,\n\"refresh_token\": refresh_token,\n}\ntoken_issued_at = time.time()\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Token refresh\")\njson_res = res.json()\naccess_token = json_res[\"access_token\"]\nid_token = json_res[\"id_token\"]\nrefresh_token = json_res[\"refresh_token\"]\ntoken_expires_in = json_res[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_expires_in,\ntoken_issued_at,\n)\nreturn access_token\ndef __raise_errors(self, res, action):\n\"\"\"log the failed request's response and raise errors.\n        Args:\n            res (requests.Response): the response of a failed request\n            action (str): a string for more informative error display\n            to the user.\n        \"\"\"\nlog_response_error(res)\nif res.status_code == 429:\nraise CommunicationError(\"Too many requests. Try again later.\")\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\ndescription = json_res.get(\"error_description\", \"\")\nmsg = f\"{action} failed.\"\nif description:\nmsg += f\" {description}\"\nraise CommunicationError(msg)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.access_token","title":"<code>access_token</code>  <code>property</code>","text":"<p>Reads and returns an access token of the currently logged in user to be used for authorizing requests to the MedPerf server. Refresh the token if necessary.</p> <p>Returns:</p> Name Type Description <code>access_token</code> <code>str</code> <p>the access token</p>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__check_token_email","title":"<code>__check_token_email(id_token_payload, email)</code>","text":"<p>Checks if the email provided by the user in the terminal matches the email found in the recieved id token.</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __check_token_email(self, id_token_payload, email):\n\"\"\"Checks if the email provided by the user in the terminal matches the\n    email found in the recieved id token.\"\"\"\nemail_in_token = id_token_payload[\"email\"]\nif email.lower() != email_in_token:\nraise CommunicationError(\n\"The email provided in the terminal does not match the one provided during login\"\n)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__get_device_access_token","title":"<code>__get_device_access_token(device_code, polling_interval)</code>","text":"<p>Get the access token from the auth0 backend associated with the device code requested before. This function will keep polling the access token until the user completes the browser flow part of the authorization process.</p> <p>Parameters:</p> Name Type Description Default <code>device_code</code> <code>str</code> <p>A temporary device code requested by <code>__request_device_code</code></p> required <code>polling_interval</code> <code>float</code> <p>number of seconds to wait between each two polling requests</p> required <p>Returns:</p> Name Type Description <code>json_res</code> <code>dict</code> <p>the response of the successful request, containg the access/refresh tokens pair</p> <code>token_issued_at</code> <code>float</code> <p>the timestamp when the access token was issued</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __get_device_access_token(self, device_code, polling_interval):\n\"\"\"Get the access token from the auth0 backend associated with\n    the device code requested before. This function will keep polling\n    the access token until the user completes the browser flow part\n    of the authorization process.\n    Args:\n        device_code (str): A temporary device code requested by `__request_device_code`\n        polling_interval (float): number of seconds to wait between each two polling requests\n    Returns:\n        json_res (dict): the response of the successful request, containg the access/refresh tokens pair\n        token_issued_at (float): the timestamp when the access token was issued\n    \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n\"device_code\": device_code,\n\"client_id\": self.client_id,\n}\nwhile True:\ntime.sleep(polling_interval)\ntoken_issued_at = time.time()\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code == 200:\njson_res = res.json()\nreturn json_res, token_issued_at\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\nerror = json_res.get(\"error\", None)\nif error not in [\"slow_down\", \"authorization_pending\"]:\nself.__raise_errors(res, \"Login\")\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__raise_errors","title":"<code>__raise_errors(res, action)</code>","text":"<p>log the failed request's response and raise errors.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>requests.Response</code> <p>the response of a failed request</p> required <code>action</code> <code>str</code> <p>a string for more informative error display</p> required Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __raise_errors(self, res, action):\n\"\"\"log the failed request's response and raise errors.\n    Args:\n        res (requests.Response): the response of a failed request\n        action (str): a string for more informative error display\n        to the user.\n    \"\"\"\nlog_response_error(res)\nif res.status_code == 429:\nraise CommunicationError(\"Too many requests. Try again later.\")\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\ndescription = json_res.get(\"error_description\", \"\")\nmsg = f\"{action} failed.\"\nif description:\nmsg += f\" {description}\"\nraise CommunicationError(msg)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__refresh_access_token","title":"<code>__refresh_access_token(refresh_token)</code>","text":"<p>Retrieve and store a new access token using a refresh token. A new refresh token will also be retrieved and stored.</p> <p>Parameters:</p> Name Type Description Default <code>refresh_token</code> <code>str</code> <p>the refresh token</p> required <p>Returns:</p> Name Type Description <code>access_token</code> <code>str</code> <p>the new access token</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __refresh_access_token(self, refresh_token):\n\"\"\"Retrieve and store a new access token using a refresh token.\n    A new refresh token will also be retrieved and stored.\n    Args:\n        refresh_token (str): the refresh token\n    Returns:\n        access_token (str): the new access token\n    \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"refresh_token\",\n\"client_id\": self.client_id,\n\"refresh_token\": refresh_token,\n}\ntoken_issued_at = time.time()\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Token refresh\")\njson_res = res.json()\naccess_token = json_res[\"access_token\"]\nid_token = json_res[\"id_token\"]\nrefresh_token = json_res[\"refresh_token\"]\ntoken_expires_in = json_res[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_expires_in,\ntoken_issued_at,\n)\nreturn access_token\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__request_device_code","title":"<code>__request_device_code()</code>","text":"<p>Get a device code from the auth0 backend to be used for the authorization process</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __request_device_code(self):\n\"\"\"Get a device code from the auth0 backend to be used for the authorization process\"\"\"\nurl = f\"https://{self.domain}/oauth/device/code\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"client_id\": self.client_id,\n\"audience\": self.audience,\n\"scope\": \"offline_access openid email\",\n}\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Login\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.login","title":"<code>login(email)</code>","text":"<p>Retrieves and stores an access token/refresh token pair from the auth0 backend using the device authorization flow.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email. This will be used to validate that the received          id_token contains the same email address.</p> required Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def login(self, email):\n\"\"\"Retrieves and stores an access token/refresh token pair from the auth0\n    backend using the device authorization flow.\n    Args:\n        email (str): user email. This will be used to validate that the received\n                     id_token contains the same email address.\n    \"\"\"\ndevice_code_response = self.__request_device_code()\ndevice_code = device_code_response[\"device_code\"]\nuser_code = device_code_response[\"user_code\"]\nverification_uri_complete = device_code_response[\"verification_uri_complete\"]\ninterval = device_code_response[\"interval\"]\nconfig.ui.print(\n\"\\nPlease go to the following link to complete your login request:\\n\"\nf\"\\t{verification_uri_complete}\\n\\n\"\n\"Make sure that you will be presented with the following code:\\n\"\nf\"\\t{user_code}\\n\\n\"\n)\nconfig.ui.print_warning(\n\"Keep this terminal open until you complete your login request. \"\n\"The command will exit on its own once you complete the request. \"\n\"If you wish to stop the login request anyway, press Ctrl+C.\"\n)\ntoken_response, token_issued_at = self.__get_device_access_token(\ndevice_code, interval\n)\naccess_token = token_response[\"access_token\"]\nid_token = token_response[\"id_token\"]\nrefresh_token = token_response[\"refresh_token\"]\ntoken_expires_in = token_response[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nself.__check_token_email(id_token_payload, email)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\n)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.logout","title":"<code>logout()</code>","text":"<p>Logs out the user by revoking their refresh token and deleting the stored tokens.</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def logout(self):\n\"\"\"Logs out the user by revoking their refresh token and deleting the\n    stored tokens.\"\"\"\ncreds = read_credentials()\nrefresh_token = creds[\"refresh_token\"]\nurl = f\"https://{self.domain}/oauth/revoke\"\nheaders = {\"content-type\": \"application/json\"}\nbody = {\n\"client_id\": self.client_id,\n\"token\": refresh_token,\n}\nres = requests.post(url=url, headers=headers, json=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Logout\")\ndelete_credentials()\n</code></pre>"},{"location":"reference/comms/auth/interface/","title":"Interface","text":""},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth","title":"<code>Auth</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>class Auth(ABC):\n@abstractmethod\ndef __init__(self):\n\"\"\"Initialize the class\"\"\"\n@abstractmethod\ndef login(self, email):\n\"\"\"Log in a user\"\"\"\n@abstractmethod\ndef logout(self):\n\"\"\"Log out a user\"\"\"\n@property\n@abstractmethod\ndef access_token(self):\n\"\"\"An access token to authorize requests to the MedPerf server\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.access_token","title":"<code>access_token</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>An access token to authorize requests to the MedPerf server</p>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Initialize the class</p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n\"\"\"Initialize the class\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.login","title":"<code>login(email)</code>  <code>abstractmethod</code>","text":"<p>Log in a user</p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>@abstractmethod\ndef login(self, email):\n\"\"\"Log in a user\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.logout","title":"<code>logout()</code>  <code>abstractmethod</code>","text":"<p>Log out a user</p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>@abstractmethod\ndef logout(self):\n\"\"\"Log out a user\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/local/","title":"Local","text":""},{"location":"reference/comms/auth/local/#comms.auth.local.Local","title":"<code>Local</code>","text":"<p>             Bases: <code>Auth</code></p> Source code in <code>cli/medperf/comms/auth/local.py</code> <pre><code>class Local(Auth):\ndef __init__(self):\nwith open(config.local_tokens_path) as f:\nself.tokens = json.load(f)\ndef login(self, email):\n\"\"\"Retrieves and stores an access token from a local store json file.\n        Args:\n            email (str): user email.\n        \"\"\"\ntry:\naccess_token = self.tokens[email]\nexcept KeyError:\nraise InvalidArgumentError(\n\"The provided email does not exist for testing. \"\n\"Make sure you activated the right profile.\"\n)\nrefresh_token = \"refresh token\"\nid_token_payload = {\"email\": email}\ntoken_issued_at = 0\ntoken_expires_in = 10**10\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\n)\ndef logout(self):\n\"\"\"Logs out the user by deleting the stored tokens.\"\"\"\ndelete_credentials()\n@property\ndef access_token(self):\n\"\"\"Reads and returns an access token of the currently logged\n        in user to be used for authorizing requests to the MedPerf server.\n        Returns:\n            access_token (str): the access token\n        \"\"\"\ncreds = read_credentials()\naccess_token = creds[\"access_token\"]\nreturn access_token\n</code></pre>"},{"location":"reference/comms/auth/local/#comms.auth.local.Local.access_token","title":"<code>access_token</code>  <code>property</code>","text":"<p>Reads and returns an access token of the currently logged in user to be used for authorizing requests to the MedPerf server.</p> <p>Returns:</p> Name Type Description <code>access_token</code> <code>str</code> <p>the access token</p>"},{"location":"reference/comms/auth/local/#comms.auth.local.Local.login","title":"<code>login(email)</code>","text":"<p>Retrieves and stores an access token from a local store json file.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email.</p> required Source code in <code>cli/medperf/comms/auth/local.py</code> <pre><code>def login(self, email):\n\"\"\"Retrieves and stores an access token from a local store json file.\n    Args:\n        email (str): user email.\n    \"\"\"\ntry:\naccess_token = self.tokens[email]\nexcept KeyError:\nraise InvalidArgumentError(\n\"The provided email does not exist for testing. \"\n\"Make sure you activated the right profile.\"\n)\nrefresh_token = \"refresh token\"\nid_token_payload = {\"email\": email}\ntoken_issued_at = 0\ntoken_expires_in = 10**10\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\n)\n</code></pre>"},{"location":"reference/comms/auth/local/#comms.auth.local.Local.logout","title":"<code>logout()</code>","text":"<p>Logs out the user by deleting the stored tokens.</p> Source code in <code>cli/medperf/comms/auth/local.py</code> <pre><code>def logout(self):\n\"\"\"Logs out the user by deleting the stored tokens.\"\"\"\ndelete_credentials()\n</code></pre>"},{"location":"reference/comms/auth/token_verifier/","title":"Token verifier","text":"<p>This module defines a wrapper around the existing token verifier in auth0-python library. The library is designed to cache public keys in memory. Since our client is ephemeral, we wrapped the library's <code>JwksFetcher</code> to cache keys in the filesystem storage, and wrapped the library's signature verifier to use this new <code>JwksFetcher</code></p>"},{"location":"reference/comms/entity_resources/resources/","title":"Resources","text":"<p>This module downloads files from the internet. It provides a set of functions to download common files that are necessary for workflow executions and are not on the MedPerf server. An example of such files is model weights of a Model MLCube.</p> <p>This module takes care of validating the integrity of the downloaded file if a hash was specified when requesting the file. It also returns the hash of the downloaded file, which can be the original specified hash or the calculated hash of the freshly downloaded file if no hash was specified.</p> <p>Additionally, to avoid unnecessary downloads, an existing file will not be re-downloaded.</p>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_benchmark_demo_dataset","title":"<code>get_benchmark_demo_dataset(url, expected_hash=None)</code>","text":"<p>Downloads and extracts a demo dataset. If the hash is provided, the file's integrity will be checked upon download.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the compressed demo dataset file can be downloaded.</p> required <code>expected_hash</code> <code>str</code> <p>expected hash of the downloaded file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output_path</code> <code>str</code> <p>location where the uncompressed demo dataset is stored locally.</p> <code>hash_value</code> <code>str</code> <p>The hash of the downloaded tarball file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_benchmark_demo_dataset(url: str, expected_hash: str = None) -&gt; str:\n\"\"\"Downloads and extracts a demo dataset. If the hash is provided,\n    the file's integrity will be checked upon download.\n    Args:\n        url (str): URL where the compressed demo dataset file can be downloaded.\n        expected_hash (str, optional): expected hash of the downloaded file\n    Returns:\n        output_path (str): location where the uncompressed demo dataset is stored locally.\n        hash_value (str): The hash of the downloaded tarball file\n    \"\"\"\n# TODO: at some point maybe it is better to download demo datasets in\n# their benchmark folder. Doing this, we should then modify\n# the compatibility test command and remove the option of directly passing\n# demo datasets. This would look cleaner.\n# Possible cons: if multiple benchmarks use the same demo dataset.\ndemo_storage = storage_path(config.demo_data_storage)\nif expected_hash:\n# If the folder exists, return\ndemo_dataset_folder = os.path.join(demo_storage, expected_hash)\nif os.path.exists(demo_dataset_folder):\nreturn demo_dataset_folder, expected_hash\n# make sure files are uncompressed while in tmp storage, to avoid any clutter\n# objects if uncompression fails for some reason.\ntmp_output_folder = generate_tmp_path()\noutput_tarball_path = os.path.join(tmp_output_folder, config.tarball_filename)\nhash_value = download_resource(url, output_tarball_path, expected_hash)\nuntar(output_tarball_path)\ndemo_dataset_folder = os.path.join(demo_storage, hash_value)\nif os.path.exists(demo_dataset_folder):\n# handle the possibility of having clutter uncompressed files\nremove_path(demo_dataset_folder)\nos.rename(tmp_output_folder, demo_dataset_folder)\nreturn demo_dataset_folder, hash_value\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube","title":"<code>get_cube(url, cube_path, expected_hash=None)</code>","text":"<p>Downloads and writes an mlcube.yaml file. If the hash is provided, the file's integrity will be checked upon download.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the mlcube.yaml file can be downloaded.</p> required <code>cube_path</code> <code>str</code> <p>Cube location.</p> required <code>expected_hash</code> <code>str</code> <p>expected hash of the downloaded file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output_path</code> <code>str</code> <p>location where the mlcube.yaml file is stored locally.</p> <code>hash_value</code> <code>str</code> <p>The hash of the downloaded file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube(url: str, cube_path: str, expected_hash: str = None) -&gt; str:\n\"\"\"Downloads and writes an mlcube.yaml file. If the hash is provided,\n    the file's integrity will be checked upon download.\n    Args:\n        url (str): URL where the mlcube.yaml file can be downloaded.\n        cube_path (str): Cube location.\n        expected_hash (str, optional): expected hash of the downloaded file\n    Returns:\n        output_path (str): location where the mlcube.yaml file is stored locally.\n        hash_value (str): The hash of the downloaded file\n    \"\"\"\noutput_path = os.path.join(cube_path, config.cube_filename)\nif os.path.exists(output_path):\nreturn output_path, expected_hash\nhash_value = download_resource(url, output_path, expected_hash)\nreturn output_path, hash_value\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube_additional","title":"<code>get_cube_additional(url, cube_path, expected_tarball_hash=None)</code>","text":"<p>Retrieves additional files of an MLCube. The additional files will be in a compressed tarball file. The function will additionally extract this file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the additional_files.tar.gz file can be downloaded.</p> required <code>cube_path</code> <code>str</code> <p>Cube location.</p> required <code>expected_tarball_hash</code> <code>str</code> <p>expected hash of tarball file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tarball_hash</code> <code>str</code> <p>The hash of the downloaded tarball file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube_additional(\nurl: str,\ncube_path: str,\nexpected_tarball_hash: str = None,\n) -&gt; str:\n\"\"\"Retrieves additional files of an MLCube. The additional files\n    will be in a compressed tarball file. The function will additionally\n    extract this file.\n    Args:\n        url (str): URL where the additional_files.tar.gz file can be downloaded.\n        cube_path (str): Cube location.\n        expected_tarball_hash (str, optional): expected hash of tarball file\n    Returns:\n        tarball_hash (str): The hash of the downloaded tarball file\n    \"\"\"\nadditional_files_folder = os.path.join(cube_path, config.additional_path)\nif os.path.exists(additional_files_folder):\nreturn expected_tarball_hash\n# make sure files are uncompressed while in tmp storage, to avoid any clutter\n# objects if uncompression fails for some reason.\ntmp_output_folder = generate_tmp_path()\noutput_tarball_path = os.path.join(tmp_output_folder, config.tarball_filename)\ntarball_hash = download_resource(url, output_tarball_path, expected_tarball_hash)\nuntar(output_tarball_path)\nparent_folder = os.path.dirname(os.path.normpath(additional_files_folder))\nos.makedirs(parent_folder, exist_ok=True)\nos.rename(tmp_output_folder, additional_files_folder)\nreturn tarball_hash\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube_image","title":"<code>get_cube_image(url, cube_path, hash_value=None)</code>","text":"<p>Retrieves and stores the image file from the server. Stores images on a shared location, and retrieves a cached image by hash if found locally. Creates a symbolic link to the cube storage.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the image file can be downloaded.</p> required <code>cube_path</code> <code>str</code> <p>Path to cube.</p> required <code>hash_value</code> <code>(str, Optional)</code> <p>File hash to store under shared storage. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>image_cube_file</code> <code>str</code> <p>Location where the image file is stored locally.</p> <code>hash_value</code> <code>str</code> <p>The hash of the downloaded file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube_image(url: str, cube_path: str, hash_value: str = None) -&gt; str:\n\"\"\"Retrieves and stores the image file from the server. Stores images\n    on a shared location, and retrieves a cached image by hash if found locally.\n    Creates a symbolic link to the cube storage.\n    Args:\n        url (str): URL where the image file can be downloaded.\n        cube_path (str): Path to cube.\n        hash_value (str, Optional): File hash to store under shared storage. Defaults to None.\n    Returns:\n        image_cube_file: Location where the image file is stored locally.\n        hash_value (str): The hash of the downloaded file\n    \"\"\"\nimage_path = config.image_path\nimage_name = get_cube_image_name(cube_path)\nimage_cube_path = os.path.join(cube_path, image_path)\nos.makedirs(image_cube_path, exist_ok=True)\nimage_cube_file = os.path.join(image_cube_path, image_name)\nif os.path.exists(image_cube_file):\n# Remove existing links\nos.unlink(image_cube_file)\nimgs_storage = base_storage_path(config.images_storage)\nif not hash_value:\n# No hash provided, we need to download the file first\ntmp_output_path = generate_tmp_path()\nhash_value = download_resource(url, tmp_output_path)\nimg_storage = os.path.join(imgs_storage, hash_value)\nshutil.move(tmp_output_path, img_storage)\nelse:\nimg_storage = os.path.join(imgs_storage, hash_value)\nif not os.path.exists(img_storage):\n# If image doesn't exist locally, download it normally\ndownload_resource(url, img_storage, hash_value)\n# Create a symbolic link to individual cube storage\nos.symlink(img_storage, image_cube_file)\nreturn image_cube_file, hash_value\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube_params","title":"<code>get_cube_params(url, cube_path, expected_hash=None)</code>","text":"<p>Downloads and writes a cube parameters file. If the hash is provided, the file's integrity will be checked upon download.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the parameters.yaml file can be downloaded.</p> required <code>cube_path</code> <code>str</code> <p>Cube location.</p> required <code>expected_hash</code> <code>str</code> <p>expected hash of the downloaded file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output_path</code> <code>str</code> <p>location where the parameters file is stored locally.</p> <code>hash_value</code> <code>str</code> <p>The hash of the downloaded file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube_params(url: str, cube_path: str, expected_hash: str = None) -&gt; str:\n\"\"\"Downloads and writes a cube parameters file. If the hash is provided,\n    the file's integrity will be checked upon download.\n    Args:\n        url (str): URL where the parameters.yaml file can be downloaded.\n        cube_path (str): Cube location.\n        expected_hash (str, optional): expected hash of the downloaded file\n    Returns:\n        output_path (str): location where the parameters file is stored locally.\n        hash_value (str): The hash of the downloaded file\n    \"\"\"\noutput_path = os.path.join(cube_path, config.workspace_path, config.params_filename)\nif os.path.exists(output_path):\nreturn output_path, expected_hash\nhash_value = download_resource(url, output_path, expected_hash)\nreturn output_path, hash_value\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/","title":"Utils","text":""},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.__parse_resource","title":"<code>__parse_resource(resource)</code>","text":"<p>Parses a resource string and returns its identifier and the source class it can be downloaded from. The function iterates over all supported sources and checks which one accepts this resource. A resource is a string that should match a certain pattern to be downloaded by a certain resource.</p> <p>If the resource pattern does not correspond to any supported source, the function raises an <code>InvalidArgumentError</code></p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The resource string. Must be in the form : required Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def __parse_resource(resource: str):\n\"\"\"Parses a resource string and returns its identifier and the source class\n    it can be downloaded from.\n    The function iterates over all supported sources and checks which one accepts\n    this resource. A resource is a string that should match a certain pattern to be\n    downloaded by a certain resource.\n    If the resource pattern does not correspond to any supported source, the\n    function raises an `InvalidArgumentError`\n    Args:\n        resource (str): The resource string. Must be in the form &lt;source_prefix&gt;:&lt;resource_identifier&gt;\n        or a url. The later case will be interpreted as a direct download link.\n    \"\"\"\nfor source_class in supported_sources:\nresource_identifier = source_class.validate_resource(resource)\nif resource_identifier:\nreturn source_class, resource_identifier\n# In this case the input format is not compatible with any source\nmsg = f\"\"\"Invalid resource input: {resource}. A Resource must be a url or\n    in the following format: '&lt;source_prefix&gt;:&lt;resource_identifier&gt;'. Run\n    `medperf mlcube submit --help` for more details.\"\"\"\nraise InvalidArgumentError(msg)\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.download_resource","title":"<code>download_resource(resource, output_path, expected_hash=None)</code>","text":"<p>Downloads a resource/file from the internet. Passing a hash is optional. If hash is provided, the downloaded file's hash will be checked and an error will be raised if it is incorrect.</p> <p>Upon success, the function returns the hash of the downloaded file.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The resource string. Must be in the form : required <code>output_path</code> <code>str</code> <p>The path to download the resource to</p> required <code>expected_hash</code> <code>(optional, str)</code> <p>The expected hash of the file to be downloaded</p> <code>None</code> <p>Returns:</p> Type Description <p>The hash of the downloaded file (or existing file)</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def download_resource(\nresource: str, output_path: str, expected_hash: Optional[str] = None\n):\n\"\"\"Downloads a resource/file from the internet. Passing a hash is optional.\n    If hash is provided, the downloaded file's hash will be checked and an error\n    will be raised if it is incorrect.\n    Upon success, the function returns the hash of the downloaded file.\n    Args:\n        resource (str): The resource string. Must be in the form &lt;source_prefix&gt;:&lt;resource_identifier&gt;\n        or a url.\n        output_path (str): The path to download the resource to\n        expected_hash (optional, str): The expected hash of the file to be downloaded\n    Returns:\n        The hash of the downloaded file (or existing file)\n    \"\"\"\ntmp_output_path = tmp_download_resource(resource)\ncalculated_hash = verify_or_get_hash(tmp_output_path, expected_hash)\nto_permanent_path(tmp_output_path, output_path)\nreturn calculated_hash\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.tmp_download_resource","title":"<code>tmp_download_resource(resource)</code>","text":"<p>Downloads a resource to the temporary storage.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The resource string. Must be in the form : required <p>Returns:</p> Name Type Description <code>tmp_output_path</code> <code>str</code> <p>The location where the resource was downloaded</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def tmp_download_resource(resource):\n\"\"\"Downloads a resource to the temporary storage.\n    Args:\n        resource (str): The resource string. Must be in the form &lt;source_prefix&gt;:&lt;resource_identifier&gt;\n        or a url.\n    Returns:\n        tmp_output_path (str): The location where the resource was downloaded\n    \"\"\"\ntmp_output_path = generate_tmp_path()\nsource_class, resource_identifier = __parse_resource(resource)\nsource = source_class()\nsource.authenticate()\nsource.download(resource_identifier, tmp_output_path)\nreturn tmp_output_path\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.to_permanent_path","title":"<code>to_permanent_path(tmp_output_path, output_path)</code>","text":"<p>Writes a file from the temporary storage to the desired output path.</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def to_permanent_path(tmp_output_path, output_path):\n\"\"\"Writes a file from the temporary storage to the desired output path.\"\"\"\noutput_folder = os.path.dirname(os.path.abspath(output_path))\nos.makedirs(output_folder, exist_ok=True)\nos.rename(tmp_output_path, output_path)\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.verify_or_get_hash","title":"<code>verify_or_get_hash(tmp_output_path, expected_hash)</code>","text":"<p>Checks if the downloaded file matches the passed expected hash if provided. The function returns the calculated hash.</p> <p>Parameters:</p> Name Type Description Default <code>tmp_output_path</code> <code>str</code> <p>path to the asset that will be checked</p> required <code>expected_hash</code> <code>str</code> <p>expected hash of the asset</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Calculated hash of the asset. Only returns if the hashes match</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def verify_or_get_hash(tmp_output_path: str, expected_hash: str) -&gt; str:\n\"\"\"Checks if the downloaded file matches the passed expected hash\n    if provided. The function returns the calculated hash.\n    Args:\n        tmp_output_path (str): path to the asset that will be checked\n        expected_hash (str): expected hash of the asset\n    Returns:\n        str: Calculated hash of the asset. Only returns if the hashes match\n    \"\"\"\ncalculated_hash = get_file_hash(tmp_output_path)\nverify_hash(calculated_hash, expected_hash)\nreturn calculated_hash\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/","title":"Direct","text":""},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource","title":"<code>DirectLinkSource</code>","text":"<p>             Bases: <code>BaseSource</code></p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>class DirectLinkSource(BaseSource):\nprefix = \"direct:\"\n@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n        `direct:&lt;URL&gt;` or only a URL.\n        Args:\n            resource (str): the resource string\n        Returns:\n            (str|None): The URL if the pattern matches, else None\n        \"\"\"\nprefix = cls.prefix\nif value.startswith(prefix):\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif validators.url(value):\nreturn value\ndef __init__(self):\npass\ndef authenticate(self):\npass\ndef __download_once(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file by streaming its contents. source:\n        https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n        \"\"\"\nwith requests.get(resource_identifier, stream=True) as res:\nif res.status_code != 200:\nlog_response_error(res)\nmsg = (\n\"There was a problem retrieving the specified file at \"\n+ resource_identifier\n)\nraise CommunicationRetrievalError(msg)\nwith open(output_path, \"wb\") as f:\nfor chunk in res.iter_content(chunk_size=config.ddl_stream_chunk_size):\n# NOTE: if the response is chunk-encoded, this may not work\n# check whether this is common.\nf.write(chunk)\ndef download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file with multiple attempts. This is\n        done due to facing transient network failure from some direct download\n        link servers.\"\"\"\nattempt = 0\nwhile attempt &lt; config.ddl_max_redownload_attempts:\ntry:\nself.__download_once(resource_identifier, output_path)\nreturn\nexcept CommunicationRetrievalError:\nif os.path.exists(output_path):\nremove_path(output_path)\nattempt += 1\nraise CommunicationRetrievalError(f\"Could not download {resource_identifier}\")\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource.__download_once","title":"<code>__download_once(resource_identifier, output_path)</code>","text":"<p>Downloads a direct-download-link file by streaming its contents. source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests</p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>def __download_once(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file by streaming its contents. source:\n    https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n    \"\"\"\nwith requests.get(resource_identifier, stream=True) as res:\nif res.status_code != 200:\nlog_response_error(res)\nmsg = (\n\"There was a problem retrieving the specified file at \"\n+ resource_identifier\n)\nraise CommunicationRetrievalError(msg)\nwith open(output_path, \"wb\") as f:\nfor chunk in res.iter_content(chunk_size=config.ddl_stream_chunk_size):\n# NOTE: if the response is chunk-encoded, this may not work\n# check whether this is common.\nf.write(chunk)\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource.download","title":"<code>download(resource_identifier, output_path)</code>","text":"<p>Downloads a direct-download-link file with multiple attempts. This is done due to facing transient network failure from some direct download link servers.</p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>def download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file with multiple attempts. This is\n    done due to facing transient network failure from some direct download\n    link servers.\"\"\"\nattempt = 0\nwhile attempt &lt; config.ddl_max_redownload_attempts:\ntry:\nself.__download_once(resource_identifier, output_path)\nreturn\nexcept CommunicationRetrievalError:\nif os.path.exists(output_path):\nremove_path(output_path)\nattempt += 1\nraise CommunicationRetrievalError(f\"Could not download {resource_identifier}\")\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource.validate_resource","title":"<code>validate_resource(value)</code>  <code>classmethod</code>","text":"<p>This class expects a resource string of the form <code>direct:&lt;URL&gt;</code> or only a URL.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>the resource string</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The URL if the pattern matches, else None</p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n    `direct:&lt;URL&gt;` or only a URL.\n    Args:\n        resource (str): the resource string\n    Returns:\n        (str|None): The URL if the pattern matches, else None\n    \"\"\"\nprefix = cls.prefix\nif value.startswith(prefix):\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif validators.url(value):\nreturn value\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/","title":"Source","text":""},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource","title":"<code>BaseSource</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>class BaseSource(ABC):\n@classmethod\n@abstractmethod\ndef validate_resource(cls, value: str):\n\"\"\"Checks if an input resource can be downloaded by this class\"\"\"\n@abstractmethod\ndef __init__(self):\n\"\"\"Initialize\"\"\"\n@abstractmethod\ndef authenticate(self):\n\"\"\"Authenticates with the source server, if needed.\"\"\"\n@abstractmethod\ndef download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads the requested resource to the specified location\n        Args:\n            resource_identifier (str): The identifier that is used to download\n            the resource (e.g. URL, asset ID, ...) It is the parsed output\n            by `validate_resource`\n            output_path (str): The path to download the resource to\n        \"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Initialize</p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n\"\"\"Initialize\"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.authenticate","title":"<code>authenticate()</code>  <code>abstractmethod</code>","text":"<p>Authenticates with the source server, if needed.</p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@abstractmethod\ndef authenticate(self):\n\"\"\"Authenticates with the source server, if needed.\"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.download","title":"<code>download(resource_identifier, output_path)</code>  <code>abstractmethod</code>","text":"<p>Downloads the requested resource to the specified location</p> <p>Parameters:</p> Name Type Description Default <code>resource_identifier</code> <code>str</code> <p>The identifier that is used to download</p> required <code>output_path</code> <code>str</code> <p>The path to download the resource to</p> required Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@abstractmethod\ndef download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads the requested resource to the specified location\n    Args:\n        resource_identifier (str): The identifier that is used to download\n        the resource (e.g. URL, asset ID, ...) It is the parsed output\n        by `validate_resource`\n        output_path (str): The path to download the resource to\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.validate_resource","title":"<code>validate_resource(value)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Checks if an input resource can be downloaded by this class</p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@classmethod\n@abstractmethod\ndef validate_resource(cls, value: str):\n\"\"\"Checks if an input resource can be downloaded by this class\"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/synapse/","title":"Synapse","text":""},{"location":"reference/comms/entity_resources/sources/synapse/#comms.entity_resources.sources.synapse.SynapseSource","title":"<code>SynapseSource</code>","text":"<p>             Bases: <code>BaseSource</code></p> Source code in <code>cli/medperf/comms/entity_resources/sources/synapse.py</code> <pre><code>class SynapseSource(BaseSource):\nprefix = \"synapse:\"\n@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n        `synapse:&lt;synapse_id&gt;`, where &lt;synapse_id&gt; is in the form `syn&lt;Integer&gt;`.\n        Args:\n            resource (str): the resource string\n        Returns:\n            (str|None): The synapse ID if the pattern matches, else None\n        \"\"\"\nprefix = cls.prefix\nif not value.startswith(prefix):\nreturn\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif re.match(r\"syn\\d+$\", value):\nreturn value\ndef __init__(self):\nself.client = synapseclient.Synapse()\ndef authenticate(self):\ntry:\nself.client.login(silent=True)\nexcept SynapseNoCredentialsError:\nmsg = \"There was an attempt to download resources from the Synapse \"\nmsg += \"platform, but couldn't find Synapse credentials.\"\nmsg += \"\\nDid you run 'medperf auth synapse_login' before?\"\nraise CommunicationAuthenticationError(msg)\ndef download(self, resource_identifier: str, output_path: str):\n# we can specify target folder only. File name depends on how it was stored\ndownload_location = os.path.dirname(output_path)\nos.makedirs(download_location, exist_ok=True)\ntry:\nresource_file = self.client.get(\nresource_identifier, downloadLocation=download_location\n)\nexcept (SynapseHTTPError, SynapseUnmetAccessRestrictions) as e:\nraise CommunicationRetrievalError(str(e))\nresource_path = os.path.join(download_location, resource_file.name)\n# synapseclient may only throw a warning in some cases\n# (e.g. read permissions but no download permissions)\nif not os.path.exists(resource_path):\nraise CommunicationRetrievalError(\n\"There was a problem retrieving a file from Synapse\"\n)\nshutil.move(resource_path, output_path)\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/synapse/#comms.entity_resources.sources.synapse.SynapseSource.validate_resource","title":"<code>validate_resource(value)</code>  <code>classmethod</code>","text":"<p>This class expects a resource string of the form <code>synapse:&lt;synapse_id&gt;</code>, where  is in the form <code>syn&lt;Integer&gt;</code>. <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>the resource string</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The synapse ID if the pattern matches, else None</p> Source code in <code>cli/medperf/comms/entity_resources/sources/synapse.py</code> <pre><code>@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n    `synapse:&lt;synapse_id&gt;`, where &lt;synapse_id&gt; is in the form `syn&lt;Integer&gt;`.\n    Args:\n        resource (str): the resource string\n    Returns:\n        (str|None): The synapse ID if the pattern matches, else None\n    \"\"\"\nprefix = cls.prefix\nif not value.startswith(prefix):\nreturn\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif re.match(r\"syn\\d+$\", value):\nreturn value\n</code></pre>"},{"location":"reference/entities/benchmark/","title":"Benchmark","text":""},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark","title":"<code>Benchmark</code>","text":"<p>             Bases: <code>Entity</code>, <code>Uploadable</code>, <code>MedperfSchema</code>, <code>ApprovableSchema</code>, <code>DeployableSchema</code></p> <p>Class representing a Benchmark</p> <p>a benchmark is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A Benchmark instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them.</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>class Benchmark(Entity, Uploadable, MedperfSchema, ApprovableSchema, DeployableSchema):\n\"\"\"\n    Class representing a Benchmark\n    a benchmark is a bundle of assets that enables quantitative\n    measurement of the performance of AI models for a specific\n    clinical problem. A Benchmark instance contains information\n    regarding how to prepare datasets for execution, as well as\n    what models to run and how to evaluate them.\n    \"\"\"\ndescription: Optional[str] = Field(None, max_length=20)\ndocs_url: Optional[HttpUrl]\ndemo_dataset_tarball_url: Optional[str]\ndemo_dataset_tarball_hash: Optional[str]\ndemo_dataset_generated_uid: Optional[str]\ndata_preparation_mlcube: int\nreference_model_mlcube: int\ndata_evaluator_mlcube: int\nmodels: List[int] = None\nmetadata: dict = {}\nuser_metadata: dict = {}\nis_active: bool = True\n@validator(\"models\", pre=True, always=True)\ndef set_default_models_value(cls, value, values, **kwargs):\nif not value:\n# Empty or None value assigned\nreturn [values[\"reference_model_mlcube\"]]\nreturn value\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a new benchmark instance\n        Args:\n            bmk_desc (Union[dict, BenchmarkModel]): Benchmark instance description\n        \"\"\"\nsuper().__init__(*args, **kwargs)\nself.generated_uid = f\"p{self.data_preparation_mlcube}m{self.reference_model_mlcube}e{self.data_evaluator_mlcube}\"\npath = storage_path(config.benchmarks_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\nself.path = path\n@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Benchmark\"]:\n\"\"\"Gets and creates instances of all retrievable benchmarks\n        Args:\n            local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n            filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n        Returns:\n            List[Benchmark]: a list of Benchmark instances.\n        \"\"\"\nlogging.info(\"Retrieving all benchmarks\")\nbenchmarks = []\nif not local_only:\nbenchmarks = cls.__remote_all(filters=filters)\nremote_uids = set([bmk.id for bmk in benchmarks])\nlocal_benchmarks = cls.__local_all()\nbenchmarks += [bmk for bmk in local_benchmarks if bmk.id not in remote_uids]\nreturn benchmarks\n@classmethod\ndef __remote_all(cls, filters: dict) -&gt; List[\"Benchmark\"]:\nbenchmarks = []\ntry:\ncomms_fn = cls.__remote_prefilter(filters)\nbmks_meta = comms_fn()\nfor bmk_meta in bmks_meta:\n# Loading all related models for all benchmarks could be expensive.\n# Most probably not necessary when getting all benchmarks.\n# If associated models for a benchmark are needed then use Benchmark.get()\nbmk_meta[\"models\"] = [bmk_meta[\"reference_model_mlcube\"]]\nbenchmarks = [cls(**meta) for meta in bmks_meta]\nexcept CommunicationRetrievalError:\nmsg = \"Couldn't retrieve all benchmarks from the server\"\nlogging.warning(msg)\nreturn benchmarks\n@classmethod\ndef __remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_benchmarks\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_benchmarks\nreturn comms_fn\n@classmethod\ndef __local_all(cls) -&gt; List[\"Benchmark\"]:\nbenchmarks = []\nbmks_storage = storage_path(config.benchmarks_storage)\ntry:\nuids = next(os.walk(bmks_storage))[1]\nexcept StopIteration:\nmsg = \"Couldn't iterate over benchmarks directory\"\nlogging.warning(msg)\nraise MedperfException(msg)\nfor uid in uids:\nmeta = cls.__get_local_dict(uid)\nbenchmark = cls(**meta)\nbenchmarks.append(benchmark)\nreturn benchmarks\n@classmethod\ndef get(\ncls, benchmark_uid: Union[str, int], local_only: bool = False\n) -&gt; \"Benchmark\":\n\"\"\"Retrieves and creates a Benchmark instance from the server.\n        If benchmark already exists in the platform then retrieve that\n        version.\n        Args:\n            benchmark_uid (str): UID of the benchmark.\n            comms (Comms): Instance of a communication interface.\n        Returns:\n            Benchmark: a Benchmark instance with the retrieved data.\n        \"\"\"\nif not str(benchmark_uid).isdigit() or local_only:\nreturn cls.__local_get(benchmark_uid)\ntry:\nreturn cls.__remote_get(benchmark_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting Benchmark {benchmark_uid} from comms failed\")\nlogging.info(f\"Looking for benchmark {benchmark_uid} locally\")\nreturn cls.__local_get(benchmark_uid)\n@classmethod\ndef __remote_get(cls, benchmark_uid: int) -&gt; \"Benchmark\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            dset_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nlogging.debug(f\"Retrieving benchmark {benchmark_uid} remotely\")\nbenchmark_dict = config.comms.get_benchmark(benchmark_uid)\nref_model = benchmark_dict[\"reference_model_mlcube\"]\nadd_models = cls.get_models_uids(benchmark_uid)\nbenchmark_dict[\"models\"] = [ref_model] + add_models\nbenchmark = cls(**benchmark_dict)\nbenchmark.write()\nreturn benchmark\n@classmethod\ndef __local_get(cls, benchmark_uid: Union[str, int]) -&gt; \"Benchmark\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            dset_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nlogging.debug(f\"Retrieving benchmark {benchmark_uid} locally\")\nbenchmark_dict = cls.__get_local_dict(benchmark_uid)\nbenchmark = cls(**benchmark_dict)\nreturn benchmark\n@classmethod\ndef __get_local_dict(cls, benchmark_uid) -&gt; dict:\n\"\"\"Retrieves a local benchmark information\n        Args:\n            benchmark_uid (str): uid of the local benchmark\n        Returns:\n            dict: information of the benchmark\n        \"\"\"\nlogging.info(f\"Retrieving benchmark {benchmark_uid} from local storage\")\nstorage = storage_path(config.benchmarks_storage)\nbmk_storage = os.path.join(storage, str(benchmark_uid))\nbmk_file = os.path.join(bmk_storage, config.benchmarks_filename)\nif not os.path.exists(bmk_file):\nraise InvalidArgumentError(\"No benchmark with the given uid could be found\")\nwith open(bmk_file, \"r\") as f:\ndata = yaml.safe_load(f)\nreturn data\n@classmethod\ndef get_models_uids(cls, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the benchmark\n        Args:\n            benchmark_uid (int): UID of the benchmark.\n            comms (Comms): Instance of the communications interface.\n        Returns:\n            List[int]: List of mlcube uids\n        \"\"\"\nreturn config.comms.get_benchmark_models(benchmark_uid)\ndef todict(self) -&gt; dict:\n\"\"\"Dictionary representation of the benchmark instance\n        Returns:\n        dict: Dictionary containing benchmark information\n        \"\"\"\nreturn self.extended_dict()\ndef write(self) -&gt; str:\n\"\"\"Writes the benchmark into disk\n        Args:\n            filename (str, optional): name of the file. Defaults to config.benchmarks_filename.\n        Returns:\n            str: path to the created benchmark file\n        \"\"\"\ndata = self.todict()\nbmk_file = os.path.join(self.path, config.benchmarks_filename)\nif not os.path.exists(bmk_file):\nos.makedirs(self.path, exist_ok=True)\nwith open(bmk_file, \"w\") as f:\nyaml.dump(data, f)\nreturn bmk_file\ndef upload(self):\n\"\"\"Uploads a benchmark to the server\n        Args:\n            comms (Comms): communications entity to submit through\n        \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test benchmarks.\")\nbody = self.todict()\nupdated_body = config.comms.upload_benchmark(body)\nupdated_body[\"models\"] = body[\"models\"]\nreturn updated_body\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Description\": self.description,\n\"Documentation\": self.docs_url,\n\"Created At\": self.created_at,\n\"Data Preparation MLCube\": int(self.data_preparation_mlcube),\n\"Reference Model MLCube\": int(self.reference_model_mlcube),\n\"Associated Models\": \",\".join(map(str, self.models)),\n\"Data Evaluator MLCube\": int(self.data_evaluator_mlcube),\n\"State\": self.state,\n\"Approval Status\": self.approval_status,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__get_local_dict","title":"<code>__get_local_dict(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves a local benchmark information</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>str</code> <p>uid of the local benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>information of the benchmark</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef __get_local_dict(cls, benchmark_uid) -&gt; dict:\n\"\"\"Retrieves a local benchmark information\n    Args:\n        benchmark_uid (str): uid of the local benchmark\n    Returns:\n        dict: information of the benchmark\n    \"\"\"\nlogging.info(f\"Retrieving benchmark {benchmark_uid} from local storage\")\nstorage = storage_path(config.benchmarks_storage)\nbmk_storage = os.path.join(storage, str(benchmark_uid))\nbmk_file = os.path.join(bmk_storage, config.benchmarks_filename)\nif not os.path.exists(bmk_file):\nraise InvalidArgumentError(\"No benchmark with the given uid could be found\")\nwith open(bmk_file, \"r\") as f:\ndata = yaml.safe_load(f)\nreturn data\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a new benchmark instance</p> <p>Parameters:</p> Name Type Description Default <code>bmk_desc</code> <code>Union[dict, BenchmarkModel]</code> <p>Benchmark instance description</p> required Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a new benchmark instance\n    Args:\n        bmk_desc (Union[dict, BenchmarkModel]): Benchmark instance description\n    \"\"\"\nsuper().__init__(*args, **kwargs)\nself.generated_uid = f\"p{self.data_preparation_mlcube}m{self.reference_model_mlcube}e{self.data_evaluator_mlcube}\"\npath = storage_path(config.benchmarks_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\nself.path = path\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__local_get","title":"<code>__local_get(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Benchmark</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef __local_get(cls, benchmark_uid: Union[str, int]) -&gt; \"Benchmark\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        dset_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nlogging.debug(f\"Retrieving benchmark {benchmark_uid} locally\")\nbenchmark_dict = cls.__get_local_dict(benchmark_uid)\nbenchmark = cls(**benchmark_dict)\nreturn benchmark\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__remote_get","title":"<code>__remote_get(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Benchmark</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef __remote_get(cls, benchmark_uid: int) -&gt; \"Benchmark\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        dset_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nlogging.debug(f\"Retrieving benchmark {benchmark_uid} remotely\")\nbenchmark_dict = config.comms.get_benchmark(benchmark_uid)\nref_model = benchmark_dict[\"reference_model_mlcube\"]\nadd_models = cls.get_models_uids(benchmark_uid)\nbenchmark_dict[\"models\"] = [ref_model] + add_models\nbenchmark = cls(**benchmark_dict)\nbenchmark.write()\nreturn benchmark\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__remote_prefilter","title":"<code>__remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef __remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_benchmarks\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_benchmarks\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.all","title":"<code>all(local_only=False, filters={})</code>  <code>classmethod</code>","text":"<p>Gets and creates instances of all retrievable benchmarks</p> <p>Parameters:</p> Name Type Description Default <code>local_only</code> <code>bool</code> <p>Wether to retrieve only local entities. Defaults to False.</p> <code>False</code> <code>filters</code> <code>dict</code> <p>key-value pairs specifying filters to apply to the list of entities.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Benchmark]</code> <p>List[Benchmark]: a list of Benchmark instances.</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Benchmark\"]:\n\"\"\"Gets and creates instances of all retrievable benchmarks\n    Args:\n        local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n        filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n    Returns:\n        List[Benchmark]: a list of Benchmark instances.\n    \"\"\"\nlogging.info(\"Retrieving all benchmarks\")\nbenchmarks = []\nif not local_only:\nbenchmarks = cls.__remote_all(filters=filters)\nremote_uids = set([bmk.id for bmk in benchmarks])\nlocal_benchmarks = cls.__local_all()\nbenchmarks += [bmk for bmk in local_benchmarks if bmk.id not in remote_uids]\nreturn benchmarks\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get","title":"<code>get(benchmark_uid, local_only=False)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>str</code> <p>UID of the benchmark.</p> required <code>comms</code> <code>Comms</code> <p>Instance of a communication interface.</p> required <p>Returns:</p> Name Type Description <code>Benchmark</code> <code>Benchmark</code> <p>a Benchmark instance with the retrieved data.</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef get(\ncls, benchmark_uid: Union[str, int], local_only: bool = False\n) -&gt; \"Benchmark\":\n\"\"\"Retrieves and creates a Benchmark instance from the server.\n    If benchmark already exists in the platform then retrieve that\n    version.\n    Args:\n        benchmark_uid (str): UID of the benchmark.\n        comms (Comms): Instance of a communication interface.\n    Returns:\n        Benchmark: a Benchmark instance with the retrieved data.\n    \"\"\"\nif not str(benchmark_uid).isdigit() or local_only:\nreturn cls.__local_get(benchmark_uid)\ntry:\nreturn cls.__remote_get(benchmark_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting Benchmark {benchmark_uid} from comms failed\")\nlogging.info(f\"Looking for benchmark {benchmark_uid} locally\")\nreturn cls.__local_get(benchmark_uid)\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get_models_uids","title":"<code>get_models_uids(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of models associated to the benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark.</p> required <code>comms</code> <code>Comms</code> <p>Instance of the communications interface.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of mlcube uids</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef get_models_uids(cls, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the benchmark\n    Args:\n        benchmark_uid (int): UID of the benchmark.\n        comms (Comms): Instance of the communications interface.\n    Returns:\n        List[int]: List of mlcube uids\n    \"\"\"\nreturn config.comms.get_benchmark_models(benchmark_uid)\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.todict","title":"<code>todict()</code>","text":"<p>Dictionary representation of the benchmark instance</p> <p>dict: Dictionary containing benchmark information</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>def todict(self) -&gt; dict:\n\"\"\"Dictionary representation of the benchmark instance\n    Returns:\n    dict: Dictionary containing benchmark information\n    \"\"\"\nreturn self.extended_dict()\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.upload","title":"<code>upload()</code>","text":"<p>Uploads a benchmark to the server</p> <p>Parameters:</p> Name Type Description Default <code>comms</code> <code>Comms</code> <p>communications entity to submit through</p> required Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>def upload(self):\n\"\"\"Uploads a benchmark to the server\n    Args:\n        comms (Comms): communications entity to submit through\n    \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test benchmarks.\")\nbody = self.todict()\nupdated_body = config.comms.upload_benchmark(body)\nupdated_body[\"models\"] = body[\"models\"]\nreturn updated_body\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.write","title":"<code>write()</code>","text":"<p>Writes the benchmark into disk</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>name of the file. Defaults to config.benchmarks_filename.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>path to the created benchmark file</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>def write(self) -&gt; str:\n\"\"\"Writes the benchmark into disk\n    Args:\n        filename (str, optional): name of the file. Defaults to config.benchmarks_filename.\n    Returns:\n        str: path to the created benchmark file\n    \"\"\"\ndata = self.todict()\nbmk_file = os.path.join(self.path, config.benchmarks_filename)\nif not os.path.exists(bmk_file):\nos.makedirs(self.path, exist_ok=True)\nwith open(bmk_file, \"w\") as f:\nyaml.dump(data, f)\nreturn bmk_file\n</code></pre>"},{"location":"reference/entities/cube/","title":"Cube","text":""},{"location":"reference/entities/cube/#entities.cube.Cube","title":"<code>Cube</code>","text":"<p>             Bases: <code>Entity</code>, <code>Uploadable</code>, <code>MedperfSchema</code>, <code>DeployableSchema</code></p> <p>Class representing an MLCube Container</p> <p>Medperf platform uses the MLCube container for components such as Dataset Preparation, Evaluation, and the Registered Models. MLCube containers are software containers (e.g., Docker and Singularity) with standard metadata and a consistent file-system level interface.</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>class Cube(Entity, Uploadable, MedperfSchema, DeployableSchema):\n\"\"\"\n    Class representing an MLCube Container\n    Medperf platform uses the MLCube container for components such as\n    Dataset Preparation, Evaluation, and the Registered Models. MLCube\n    containers are software containers (e.g., Docker and Singularity)\n    with standard metadata and a consistent file-system level interface.\n    \"\"\"\ngit_mlcube_url: str\nmlcube_hash: Optional[str]\ngit_parameters_url: Optional[str]\nparameters_hash: Optional[str]\nimage_tarball_url: Optional[str]\nimage_tarball_hash: Optional[str]\nimage_hash: Optional[str]\nadditional_files_tarball_url: Optional[str] = Field(None, alias=\"tarball_url\")\nadditional_files_tarball_hash: Optional[str] = Field(None, alias=\"tarball_hash\")\nmetadata: dict = {}\nuser_metadata: dict = {}\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a Cube instance\n        Args:\n            cube_desc (Union[dict, CubeModel]): MLCube Instance description\n        \"\"\"\nsuper().__init__(*args, **kwargs)\nself.generated_uid = self.name\npath = storage_path(config.cubes_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\n# NOTE: maybe have these as @property, to have the same entity reusable\n#       before and after submission\nself.path = path\nself.cube_path = os.path.join(path, config.cube_filename)\nself.params_path = None\nif self.git_parameters_url:\nself.params_path = os.path.join(path, config.params_filename)\n@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Cube\"]:\n\"\"\"Class method for retrieving all retrievable MLCubes\n        Args:\n            local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n            filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n        Returns:\n            List[Cube]: List containing all cubes\n        \"\"\"\nlogging.info(\"Retrieving all cubes\")\ncubes = []\nif not local_only:\ncubes = cls.__remote_all(filters=filters)\nremote_uids = set([cube.id for cube in cubes])\nlocal_cubes = cls.__local_all()\ncubes += [cube for cube in local_cubes if cube.id not in remote_uids]\nreturn cubes\n@classmethod\ndef __remote_all(cls, filters: dict) -&gt; List[\"Cube\"]:\ncubes = []\ntry:\ncomms_fn = cls.__remote_prefilter(filters)\ncubes_meta = comms_fn()\ncubes = [cls(**meta) for meta in cubes_meta]\nexcept CommunicationRetrievalError:\nmsg = \"Couldn't retrieve all cubes from the server\"\nlogging.warning(msg)\nreturn cubes\n@classmethod\ndef __remote_prefilter(cls, filters: dict):\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_cubes\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_cubes\nreturn comms_fn\n@classmethod\ndef __local_all(cls) -&gt; List[\"Cube\"]:\ncubes = []\ncubes_storage = storage_path(config.cubes_storage)\ntry:\nuids = next(os.walk(cubes_storage))[1]\nexcept StopIteration:\nmsg = \"Couldn't iterate over cubes directory\"\nlogging.warning(msg)\nraise MedperfException(msg)\nfor uid in uids:\nmeta = cls.__get_local_dict(uid)\ncube = cls(**meta)\ncubes.append(cube)\nreturn cubes\n@classmethod\ndef get(cls, cube_uid: Union[str, int], local_only: bool = False) -&gt; \"Cube\":\n\"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists\n        inside the user's computer then retrieves it from there.\n        Args:\n            cube_uid (str): UID of the cube.\n        Returns:\n            Cube : a Cube instance with the retrieved data.\n        \"\"\"\nif not str(cube_uid).isdigit() or local_only:\ncube = cls.__local_get(cube_uid)\nelse:\ntry:\ncube = cls.__remote_get(cube_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting MLCube {cube_uid} from comms failed\")\nlogging.info(f\"Retrieving MLCube {cube_uid} from local storage\")\ncube = cls.__local_get(cube_uid)\nif not cube.is_valid:\nraise InvalidEntityError(\"The requested MLCube is marked as INVALID.\")\ncube.download()\nreturn cube\n@classmethod\ndef __remote_get(cls, cube_uid: int) -&gt; \"Cube\":\nlogging.debug(f\"Retrieving mlcube {cube_uid} remotely\")\nmeta = config.comms.get_cube_metadata(cube_uid)\ncube = cls(**meta)\ncube.write()\nreturn cube\n@classmethod\ndef __local_get(cls, cube_uid: Union[str, int]) -&gt; \"Cube\":\nlogging.debug(f\"Retrieving cube {cube_uid} locally\")\nlocal_meta = cls.__get_local_dict(cube_uid)\ncube = cls(**local_meta)\nreturn cube\ndef download_mlcube(self):\nurl = self.git_mlcube_url\npath, file_hash = resources.get_cube(url, self.path, self.mlcube_hash)\nself.cube_path = path\nself.mlcube_hash = file_hash\ndef download_parameters(self):\nurl = self.git_parameters_url\nif url:\npath, file_hash = resources.get_cube_params(\nurl, self.path, self.parameters_hash\n)\nself.params_path = path\nself.parameters_hash = file_hash\ndef download_additional(self):\nurl = self.additional_files_tarball_url\nif url:\nfile_hash = resources.get_cube_additional(\nurl, self.path, self.additional_files_tarball_hash\n)\nself.additional_files_tarball_hash = file_hash\ndef download_image(self):\nurl = self.image_tarball_url\ntarball_hash = self.image_tarball_hash\nimg_hash = self.image_hash\nif url:\n_, local_hash = resources.get_cube_image(url, self.path, tarball_hash)\nself.image_tarball_hash = local_hash\nelse:\n# Retrieve image from image registry\nlogging.debug(f\"Retrieving {self.id} image\")\ncmd = f\"mlcube configure --mlcube={self.cube_path}\"\nwith pexpect.spawn(cmd, timeout=config.mlcube_configure_timeout) as proc:\nproc_out = proc.read()\nif proc.exitstatus != 0:\nraise ExecutionError(\n\"There was an error while retrieving the MLCube image\"\n)\nlogging.debug(proc_out)\n# Retrieve image hash from MLCube\nlogging.debug(f\"Retrieving {self.id} image hash\")\ntmp_out_yaml = generate_tmp_path()\ncmd = f\"mlcube inspect --mlcube={self.cube_path} --format=yaml\"\ncmd += f\" --output-file {tmp_out_yaml}\"\nwith pexpect.spawn(cmd, timeout=config.mlcube_inspect_timeout) as proc:\nproc_stdout = proc.read()\nlogging.debug(proc_stdout)\nif proc.exitstatus != 0:\nraise ExecutionError(\n\"There was an error while inspecting the image hash\"\n)\nwith open(tmp_out_yaml) as f:\nmlcube_details = yaml.safe_load(f)\nremove_path(tmp_out_yaml)\nlocal_hash = mlcube_details[\"hash\"]\nverify_hash(local_hash, img_hash)\nself.image_hash = local_hash\ndef download(self):\n\"\"\"Downloads the required elements for an mlcube to run locally.\"\"\"\ntry:\nself.download_mlcube()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} manifest file: {e}\")\ntry:\nself.download_parameters()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} parameters file: {e}\")\ntry:\nself.download_additional()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} additional files: {e}\")\ntry:\nself.download_image()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} image file: {e}\")\ndef run(\nself,\ntask: str,\noutput_logs: str = None,\nstring_params: Dict[str, str] = {},\ntimeout: int = None,\n**kwargs,\n):\n\"\"\"Executes a given task on the cube instance\n        Args:\n            task (str): task to run\n            string_params (Dict[str], optional): Extra parameters that can't be passed as normal function args.\n                                                 Defaults to {}.\n            timeout (int, optional): timeout for the task in seconds. Defaults to None.\n            kwargs (dict): additional arguments that are passed directly to the mlcube command\n        \"\"\"\nkwargs.update(string_params)\n# TODO: re-use `loglevel=critical` or figure out a clean MLCube logging\ncmd = \"mlcube run\"\ncmd += f\" --mlcube={self.cube_path} --task={task} --platform={config.platform} --network=none\"\nif config.gpus is not None:\ncmd += f\" --gpus={config.gpus}\"\nfor k, v in kwargs.items():\ncmd_arg = f'{k}=\"{v}\"'\ncmd = \" \".join([cmd, cmd_arg])\nlogging.info(f\"Running MLCube command: {cmd}\")\nproc = pexpect.spawn(cmd, timeout=timeout)\nproc_out = combine_proc_sp_text(proc)\nproc.close()\nif output_logs is None:\nlogging.debug(proc_out)\nelse:\nwith open(output_logs, \"w\") as f:\nf.write(proc_out)\nif proc.exitstatus != 0:\nraise ExecutionError(\"There was an error while executing the cube\")\nlogging.debug(list_files(config.storage))\nreturn proc\ndef get_default_output(self, task: str, out_key: str, param_key: str = None) -&gt; str:\n\"\"\"Returns the output parameter specified in the mlcube.yaml file\n        Args:\n            task (str): the task of interest\n            out_key (str): key used to identify the desired output in the yaml file\n            param_key (str): key inside the parameters file that completes the output path. Defaults to None.\n        Returns:\n            str: the path as specified in the mlcube.yaml file for the desired\n                output for the desired task. Defaults to None if out_key not found\n        \"\"\"\nwith open(self.cube_path, \"r\") as f:\ncube = yaml.safe_load(f)\nout_params = cube[\"tasks\"][task][\"parameters\"][\"outputs\"]\nif out_key not in out_params:\nreturn None\nout_path = cube[\"tasks\"][task][\"parameters\"][\"outputs\"][out_key]\nif isinstance(out_path, dict):\n# output is specified as a dict with type and default values\nout_path = out_path[\"default\"]\ncube_loc = str(Path(self.cube_path).parent)\nout_path = os.path.join(cube_loc, \"workspace\", out_path)\nif self.params_path is not None and param_key is not None:\nwith open(self.params_path, \"r\") as f:\nparams = yaml.safe_load(f)\nout_path = os.path.join(out_path, params[param_key])\nreturn out_path\ndef todict(self) -&gt; Dict:\nreturn self.extended_dict()\ndef write(self):\ncube_loc = str(Path(self.cube_path).parent)\nmeta_file = os.path.join(cube_loc, config.cube_metadata_filename)\nos.makedirs(cube_loc, exist_ok=True)\nwith open(meta_file, \"w\") as f:\nyaml.dump(self.todict(), f)\nreturn meta_file\ndef upload(self):\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test mlcubes.\")\ncube_dict = self.todict()\nupdated_cube_dict = config.comms.upload_mlcube(cube_dict)\nreturn updated_cube_dict\n@classmethod\ndef __get_local_dict(cls, uid):\ncubes_storage = storage_path(config.cubes_storage)\nmeta_file = os.path.join(cubes_storage, str(uid), config.cube_metadata_filename)\nif not os.path.exists(meta_file):\nraise InvalidArgumentError(\n\"The requested mlcube information could not be found locally\"\n)\nwith open(meta_file, \"r\") as f:\nmeta = yaml.safe_load(f)\nreturn meta\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Config File\": self.git_mlcube_url,\n\"State\": self.state,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a Cube instance</p> <p>Parameters:</p> Name Type Description Default <code>cube_desc</code> <code>Union[dict, CubeModel]</code> <p>MLCube Instance description</p> required Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a Cube instance\n    Args:\n        cube_desc (Union[dict, CubeModel]): MLCube Instance description\n    \"\"\"\nsuper().__init__(*args, **kwargs)\nself.generated_uid = self.name\npath = storage_path(config.cubes_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\n# NOTE: maybe have these as @property, to have the same entity reusable\n#       before and after submission\nself.path = path\nself.cube_path = os.path.join(path, config.cube_filename)\nself.params_path = None\nif self.git_parameters_url:\nself.params_path = os.path.join(path, config.params_filename)\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.__remote_prefilter","title":"<code>__remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>@classmethod\ndef __remote_prefilter(cls, filters: dict):\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_cubes\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_cubes\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.all","title":"<code>all(local_only=False, filters={})</code>  <code>classmethod</code>","text":"<p>Class method for retrieving all retrievable MLCubes</p> <p>Parameters:</p> Name Type Description Default <code>local_only</code> <code>bool</code> <p>Wether to retrieve only local entities. Defaults to False.</p> <code>False</code> <code>filters</code> <code>dict</code> <p>key-value pairs specifying filters to apply to the list of entities.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Cube]</code> <p>List[Cube]: List containing all cubes</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Cube\"]:\n\"\"\"Class method for retrieving all retrievable MLCubes\n    Args:\n        local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n        filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n    Returns:\n        List[Cube]: List containing all cubes\n    \"\"\"\nlogging.info(\"Retrieving all cubes\")\ncubes = []\nif not local_only:\ncubes = cls.__remote_all(filters=filters)\nremote_uids = set([cube.id for cube in cubes])\nlocal_cubes = cls.__local_all()\ncubes += [cube for cube in local_cubes if cube.id not in remote_uids]\nreturn cubes\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.download","title":"<code>download()</code>","text":"<p>Downloads the required elements for an mlcube to run locally.</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>def download(self):\n\"\"\"Downloads the required elements for an mlcube to run locally.\"\"\"\ntry:\nself.download_mlcube()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} manifest file: {e}\")\ntry:\nself.download_parameters()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} parameters file: {e}\")\ntry:\nself.download_additional()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} additional files: {e}\")\ntry:\nself.download_image()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"MLCube {self.name} image file: {e}\")\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.get","title":"<code>get(cube_uid, local_only=False)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>str</code> <p>UID of the cube.</p> required <p>Returns:</p> Name Type Description <code>Cube</code> <code>Cube</code> <p>a Cube instance with the retrieved data.</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>@classmethod\ndef get(cls, cube_uid: Union[str, int], local_only: bool = False) -&gt; \"Cube\":\n\"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists\n    inside the user's computer then retrieves it from there.\n    Args:\n        cube_uid (str): UID of the cube.\n    Returns:\n        Cube : a Cube instance with the retrieved data.\n    \"\"\"\nif not str(cube_uid).isdigit() or local_only:\ncube = cls.__local_get(cube_uid)\nelse:\ntry:\ncube = cls.__remote_get(cube_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting MLCube {cube_uid} from comms failed\")\nlogging.info(f\"Retrieving MLCube {cube_uid} from local storage\")\ncube = cls.__local_get(cube_uid)\nif not cube.is_valid:\nraise InvalidEntityError(\"The requested MLCube is marked as INVALID.\")\ncube.download()\nreturn cube\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.get_default_output","title":"<code>get_default_output(task, out_key, param_key=None)</code>","text":"<p>Returns the output parameter specified in the mlcube.yaml file</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>the task of interest</p> required <code>out_key</code> <code>str</code> <p>key used to identify the desired output in the yaml file</p> required <code>param_key</code> <code>str</code> <p>key inside the parameters file that completes the output path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>def get_default_output(self, task: str, out_key: str, param_key: str = None) -&gt; str:\n\"\"\"Returns the output parameter specified in the mlcube.yaml file\n    Args:\n        task (str): the task of interest\n        out_key (str): key used to identify the desired output in the yaml file\n        param_key (str): key inside the parameters file that completes the output path. Defaults to None.\n    Returns:\n        str: the path as specified in the mlcube.yaml file for the desired\n            output for the desired task. Defaults to None if out_key not found\n    \"\"\"\nwith open(self.cube_path, \"r\") as f:\ncube = yaml.safe_load(f)\nout_params = cube[\"tasks\"][task][\"parameters\"][\"outputs\"]\nif out_key not in out_params:\nreturn None\nout_path = cube[\"tasks\"][task][\"parameters\"][\"outputs\"][out_key]\nif isinstance(out_path, dict):\n# output is specified as a dict with type and default values\nout_path = out_path[\"default\"]\ncube_loc = str(Path(self.cube_path).parent)\nout_path = os.path.join(cube_loc, \"workspace\", out_path)\nif self.params_path is not None and param_key is not None:\nwith open(self.params_path, \"r\") as f:\nparams = yaml.safe_load(f)\nout_path = os.path.join(out_path, params[param_key])\nreturn out_path\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.run","title":"<code>run(task, output_logs=None, string_params={}, timeout=None, **kwargs)</code>","text":"<p>Executes a given task on the cube instance</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>task to run</p> required <code>string_params</code> <code>Dict[str]</code> <p>Extra parameters that can't be passed as normal function args.                                  Defaults to {}.</p> <code>{}</code> <code>timeout</code> <code>int</code> <p>timeout for the task in seconds. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>additional arguments that are passed directly to the mlcube command</p> <code>{}</code> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>def run(\nself,\ntask: str,\noutput_logs: str = None,\nstring_params: Dict[str, str] = {},\ntimeout: int = None,\n**kwargs,\n):\n\"\"\"Executes a given task on the cube instance\n    Args:\n        task (str): task to run\n        string_params (Dict[str], optional): Extra parameters that can't be passed as normal function args.\n                                             Defaults to {}.\n        timeout (int, optional): timeout for the task in seconds. Defaults to None.\n        kwargs (dict): additional arguments that are passed directly to the mlcube command\n    \"\"\"\nkwargs.update(string_params)\n# TODO: re-use `loglevel=critical` or figure out a clean MLCube logging\ncmd = \"mlcube run\"\ncmd += f\" --mlcube={self.cube_path} --task={task} --platform={config.platform} --network=none\"\nif config.gpus is not None:\ncmd += f\" --gpus={config.gpus}\"\nfor k, v in kwargs.items():\ncmd_arg = f'{k}=\"{v}\"'\ncmd = \" \".join([cmd, cmd_arg])\nlogging.info(f\"Running MLCube command: {cmd}\")\nproc = pexpect.spawn(cmd, timeout=timeout)\nproc_out = combine_proc_sp_text(proc)\nproc.close()\nif output_logs is None:\nlogging.debug(proc_out)\nelse:\nwith open(output_logs, \"w\") as f:\nf.write(proc_out)\nif proc.exitstatus != 0:\nraise ExecutionError(\"There was an error while executing the cube\")\nlogging.debug(list_files(config.storage))\nreturn proc\n</code></pre>"},{"location":"reference/entities/dataset/","title":"Dataset","text":""},{"location":"reference/entities/dataset/#entities.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>Entity</code>, <code>Uploadable</code>, <code>MedperfSchema</code>, <code>DeployableSchema</code></p> <p>Class representing a Dataset</p> <p>Datasets are stored locally in the Data Owner's machine. They contain information regarding the prepared dataset, such as name and description, general statistics and an UID generated by hashing the contents of the data preparation output.</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>class Dataset(Entity, Uploadable, MedperfSchema, DeployableSchema):\n\"\"\"\n    Class representing a Dataset\n    Datasets are stored locally in the Data Owner's machine. They contain\n    information regarding the prepared dataset, such as name and description,\n    general statistics and an UID generated by hashing the contents of the\n    data preparation output.\n    \"\"\"\ndescription: Optional[str] = Field(None, max_length=20)\nlocation: str = Field(..., max_length=20)\ninput_data_hash: str\ngenerated_uid: str\ndata_preparation_mlcube: Union[int, str]\nsplit_seed: Optional[int]\ngenerated_metadata: dict = Field(..., alias=\"metadata\")\nstatus: Status = None\nuser_metadata: dict = {}\n@validator(\"status\", pre=True, always=True)\ndef default_status(cls, v, *, values, **kwargs):\ndefault = Status.PENDING\nif values[\"id\"] is not None:\ndefault = Status.APPROVED\nif v is None:\nreturn default\nreturn Status(v)\n@validator(\"data_preparation_mlcube\", pre=True, always=True)\ndef check_data_preparation_mlcube(cls, v, *, values, **kwargs):\nif not isinstance(v, int) and not values[\"for_test\"]:\nraise ValueError(\n\"data_preparation_mlcube must be an integer if not running a compatibility test\"\n)\nreturn v\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\npath = storage_path(config.data_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\nself.path = path\nself.data_path = os.path.join(self.path, \"data\")\nself.labels_path = os.path.join(self.path, \"labels\")\ndef todict(self):\nreturn self.extended_dict()\n@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Dataset\"]:\n\"\"\"Gets and creates instances of all the locally prepared datasets\n        Args:\n            local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n            filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n        Returns:\n            List[Dataset]: a list of Dataset instances.\n        \"\"\"\nlogging.info(\"Retrieving all datasets\")\ndsets = []\nif not local_only:\ndsets = cls.__remote_all(filters=filters)\nremote_uids = set([dset.id for dset in dsets])\nlocal_dsets = cls.__local_all()\ndsets += [dset for dset in local_dsets if dset.id not in remote_uids]\nreturn dsets\n@classmethod\ndef __remote_all(cls, filters: dict) -&gt; List[\"Dataset\"]:\ndsets = []\ntry:\ncomms_fn = cls.__remote_prefilter(filters)\ndsets_meta = comms_fn()\ndsets = [cls(**meta) for meta in dsets_meta]\nexcept CommunicationRetrievalError:\nmsg = \"Couldn't retrieve all datasets from the server\"\nlogging.warning(msg)\nreturn dsets\n@classmethod\ndef __remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_datasets\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_datasets\nreturn comms_fn\n@classmethod\ndef __local_all(cls) -&gt; List[\"Dataset\"]:\ndsets = []\ndata_storage = storage_path(config.data_storage)\ntry:\nuids = next(os.walk(data_storage))[1]\nexcept StopIteration:\nmsg = \"Couldn't iterate over the dataset directory\"\nlogging.warning(msg)\nraise MedperfException(msg)\nfor uid in uids:\nlocal_meta = cls.__get_local_dict(uid)\ndset = cls(**local_meta)\ndsets.append(dset)\nreturn dsets\n@classmethod\ndef get(cls, dset_uid: Union[str, int], local_only: bool = False) -&gt; \"Dataset\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            dset_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nif not str(dset_uid).isdigit() or local_only:\nreturn cls.__local_get(dset_uid)\ntry:\nreturn cls.__remote_get(dset_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting Dataset {dset_uid} from comms failed\")\nlogging.info(f\"Looking for dataset {dset_uid} locally\")\nreturn cls.__local_get(dset_uid)\n@classmethod\ndef __remote_get(cls, dset_uid: int) -&gt; \"Dataset\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            dset_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nlogging.debug(f\"Retrieving dataset {dset_uid} remotely\")\nmeta = config.comms.get_dataset(dset_uid)\ndataset = cls(**meta)\ndataset.write()\nreturn dataset\n@classmethod\ndef __local_get(cls, dset_uid: Union[str, int]) -&gt; \"Dataset\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            dset_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nlogging.debug(f\"Retrieving dataset {dset_uid} locally\")\nlocal_meta = cls.__get_local_dict(dset_uid)\ndataset = cls(**local_meta)\nreturn dataset\ndef write(self):\nlogging.info(f\"Updating registration information for dataset: {self.id}\")\nlogging.debug(f\"registration information: {self.todict()}\")\nregfile = os.path.join(self.path, config.reg_file)\nos.makedirs(self.path, exist_ok=True)\nwith open(regfile, \"w\") as f:\nyaml.dump(self.todict(), f)\nreturn regfile\ndef upload(self):\n\"\"\"Uploads the registration information to the comms.\n        Args:\n            comms (Comms): Instance of the comms interface.\n        \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test datasets.\")\ndataset_dict = self.todict()\nupdated_dataset_dict = config.comms.upload_dataset(dataset_dict)\nupdated_dataset_dict[\"status\"] = dataset_dict[\"status\"]\nreturn updated_dataset_dict\n@classmethod\ndef __get_local_dict(cls, data_uid):\ndataset_path = os.path.join(storage_path(config.data_storage), str(data_uid))\nregfile = os.path.join(dataset_path, config.reg_file)\nif not os.path.exists(regfile):\nraise InvalidArgumentError(\n\"The requested dataset information could not be found locally\"\n)\nwith open(regfile, \"r\") as f:\nreg = yaml.safe_load(f)\nreturn reg\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Description\": self.description,\n\"Location\": self.location,\n\"Data Preparation Cube UID\": self.data_preparation_mlcube,\n\"Generated Hash\": self.generated_uid,\n\"Status\": self.status,\n\"State\": self.state,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.__local_get","title":"<code>__local_get(dset_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@classmethod\ndef __local_get(cls, dset_uid: Union[str, int]) -&gt; \"Dataset\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        dset_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nlogging.debug(f\"Retrieving dataset {dset_uid} locally\")\nlocal_meta = cls.__get_local_dict(dset_uid)\ndataset = cls(**local_meta)\nreturn dataset\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.__remote_get","title":"<code>__remote_get(dset_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@classmethod\ndef __remote_get(cls, dset_uid: int) -&gt; \"Dataset\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        dset_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nlogging.debug(f\"Retrieving dataset {dset_uid} remotely\")\nmeta = config.comms.get_dataset(dset_uid)\ndataset = cls(**meta)\ndataset.write()\nreturn dataset\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.__remote_prefilter","title":"<code>__remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@classmethod\ndef __remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_datasets\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_datasets\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.all","title":"<code>all(local_only=False, filters={})</code>  <code>classmethod</code>","text":"<p>Gets and creates instances of all the locally prepared datasets</p> <p>Parameters:</p> Name Type Description Default <code>local_only</code> <code>bool</code> <p>Wether to retrieve only local entities. Defaults to False.</p> <code>False</code> <code>filters</code> <code>dict</code> <p>key-value pairs specifying filters to apply to the list of entities.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dataset]</code> <p>List[Dataset]: a list of Dataset instances.</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Dataset\"]:\n\"\"\"Gets and creates instances of all the locally prepared datasets\n    Args:\n        local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n        filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n    Returns:\n        List[Dataset]: a list of Dataset instances.\n    \"\"\"\nlogging.info(\"Retrieving all datasets\")\ndsets = []\nif not local_only:\ndsets = cls.__remote_all(filters=filters)\nremote_uids = set([dset.id for dset in dsets])\nlocal_dsets = cls.__local_all()\ndsets += [dset for dset in local_dsets if dset.id not in remote_uids]\nreturn dsets\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.get","title":"<code>get(dset_uid, local_only=False)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@classmethod\ndef get(cls, dset_uid: Union[str, int], local_only: bool = False) -&gt; \"Dataset\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        dset_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nif not str(dset_uid).isdigit() or local_only:\nreturn cls.__local_get(dset_uid)\ntry:\nreturn cls.__remote_get(dset_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting Dataset {dset_uid} from comms failed\")\nlogging.info(f\"Looking for dataset {dset_uid} locally\")\nreturn cls.__local_get(dset_uid)\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.upload","title":"<code>upload()</code>","text":"<p>Uploads the registration information to the comms.</p> <p>Parameters:</p> Name Type Description Default <code>comms</code> <code>Comms</code> <p>Instance of the comms interface.</p> required Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>def upload(self):\n\"\"\"Uploads the registration information to the comms.\n    Args:\n        comms (Comms): Instance of the comms interface.\n    \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test datasets.\")\ndataset_dict = self.todict()\nupdated_dataset_dict = config.comms.upload_dataset(dataset_dict)\nupdated_dataset_dict[\"status\"] = dataset_dict[\"status\"]\nreturn updated_dataset_dict\n</code></pre>"},{"location":"reference/entities/interface/","title":"Interface","text":""},{"location":"reference/entities/interface/#entities.interface.Entity","title":"<code>Entity</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>class Entity(ABC):\n@abstractmethod\ndef all(\ncls, local_only: bool = False, comms_func: callable = None\n) -&gt; List[\"Entity\"]:\n\"\"\"Gets a list of all instances of the respective entity.\n        Wether the list is local or remote depends on the implementation.\n        Args:\n            local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n            comms_func (callable, optional): Function to use to retrieve remote entities.\n                If not provided, will use the default entrypoint.\n        Returns:\n            List[Entity]: a list of entities.\n        \"\"\"\n@abstractmethod\ndef get(cls, uid: Union[str, int]) -&gt; \"Entity\":\n\"\"\"Gets an instance of the respective entity.\n        Wether this requires only local read or remote calls depends\n        on the implementation.\n        Args:\n            uid (str): Unique Identifier to retrieve the entity\n        Returns:\n            Entity: Entity Instance associated to the UID\n        \"\"\"\n@abstractmethod\ndef todict(self) -&gt; Dict:\n\"\"\"Dictionary representation of the entity\n        Returns:\n            Dict: Dictionary containing information about the entity\n        \"\"\"\n@abstractmethod\ndef write(self) -&gt; str:\n\"\"\"Writes the entity to the local storage\n        Returns:\n            str: Path to the stored entity\n        \"\"\"\n@abstractmethod\ndef display_dict(self) -&gt; dict:\n\"\"\"Returns a dictionary of entity properties that can be displayed\n        to a user interface using a verbose name of the property rather than\n        the internal names\n        Returns:\n            dict: the display dictionary\n        \"\"\"\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.all","title":"<code>all(local_only=False, comms_func=None)</code>  <code>abstractmethod</code>","text":"<p>Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation.</p> <p>Parameters:</p> Name Type Description Default <code>local_only</code> <code>bool</code> <p>Wether to retrieve only local entities. Defaults to False.</p> <code>False</code> <code>comms_func</code> <code>callable</code> <p>Function to use to retrieve remote entities. If not provided, will use the default entrypoint.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Entity]</code> <p>List[Entity]: a list of entities.</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@abstractmethod\ndef all(\ncls, local_only: bool = False, comms_func: callable = None\n) -&gt; List[\"Entity\"]:\n\"\"\"Gets a list of all instances of the respective entity.\n    Wether the list is local or remote depends on the implementation.\n    Args:\n        local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n        comms_func (callable, optional): Function to use to retrieve remote entities.\n            If not provided, will use the default entrypoint.\n    Returns:\n        List[Entity]: a list of entities.\n    \"\"\"\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.display_dict","title":"<code>display_dict()</code>  <code>abstractmethod</code>","text":"<p>Returns a dictionary of entity properties that can be displayed to a user interface using a verbose name of the property rather than the internal names</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the display dictionary</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@abstractmethod\ndef display_dict(self) -&gt; dict:\n\"\"\"Returns a dictionary of entity properties that can be displayed\n    to a user interface using a verbose name of the property rather than\n    the internal names\n    Returns:\n        dict: the display dictionary\n    \"\"\"\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.get","title":"<code>get(uid)</code>  <code>abstractmethod</code>","text":"<p>Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique Identifier to retrieve the entity</p> required <p>Returns:</p> Name Type Description <code>Entity</code> <code>Entity</code> <p>Entity Instance associated to the UID</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@abstractmethod\ndef get(cls, uid: Union[str, int]) -&gt; \"Entity\":\n\"\"\"Gets an instance of the respective entity.\n    Wether this requires only local read or remote calls depends\n    on the implementation.\n    Args:\n        uid (str): Unique Identifier to retrieve the entity\n    Returns:\n        Entity: Entity Instance associated to the UID\n    \"\"\"\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.todict","title":"<code>todict()</code>  <code>abstractmethod</code>","text":"<p>Dictionary representation of the entity</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary containing information about the entity</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@abstractmethod\ndef todict(self) -&gt; Dict:\n\"\"\"Dictionary representation of the entity\n    Returns:\n        Dict: Dictionary containing information about the entity\n    \"\"\"\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.write","title":"<code>write()</code>  <code>abstractmethod</code>","text":"<p>Writes the entity to the local storage</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the stored entity</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@abstractmethod\ndef write(self) -&gt; str:\n\"\"\"Writes the entity to the local storage\n    Returns:\n        str: Path to the stored entity\n    \"\"\"\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Uploadable","title":"<code>Uploadable</code>","text":"Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>class Uploadable:\n@abstractmethod\ndef upload(self) -&gt; Dict:\n\"\"\"Upload the entity-related information to the communication's interface\n        Returns:\n            Dict: Dictionary with the updated entity information\n        \"\"\"\n@property\ndef identifier(self):\nreturn self.id or self.generated_uid\n@property\ndef is_registered(self):\nreturn self.id is not None\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Uploadable.upload","title":"<code>upload()</code>  <code>abstractmethod</code>","text":"<p>Upload the entity-related information to the communication's interface</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary with the updated entity information</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@abstractmethod\ndef upload(self) -&gt; Dict:\n\"\"\"Upload the entity-related information to the communication's interface\n    Returns:\n        Dict: Dictionary with the updated entity information\n    \"\"\"\n</code></pre>"},{"location":"reference/entities/report/","title":"Report","text":""},{"location":"reference/entities/report/#entities.report.TestReport","title":"<code>TestReport</code>","text":"<p>             Bases: <code>Entity</code>, <code>MedperfBaseSchema</code></p> <p>Class representing a compatibility test report entry</p> <p>A test report is comprised of the components of a test execution: - data used, which can be:     - a demo dataset url and its hash, or     - a raw data path and its labels path, or     - a prepared dataset uid - Data preparation cube if the data used was not already prepared - model cube - evaluator cube - results</p> Source code in <code>cli/medperf/entities/report.py</code> <pre><code>class TestReport(Entity, MedperfBaseSchema):\n\"\"\"\n    Class representing a compatibility test report entry\n    A test report is comprised of the components of a test execution:\n    - data used, which can be:\n        - a demo dataset url and its hash, or\n        - a raw data path and its labels path, or\n        - a prepared dataset uid\n    - Data preparation cube if the data used was not already prepared\n    - model cube\n    - evaluator cube\n    - results\n    \"\"\"\ndemo_dataset_url: Optional[str]\ndemo_dataset_hash: Optional[str]\ndata_path: Optional[str]\nlabels_path: Optional[str]\nprepared_data_hash: Optional[str]\ndata_preparation_mlcube: Optional[Union[int, str]]\nmodel: Union[int, str]\ndata_evaluator_mlcube: Union[int, str]\nresults: Optional[dict]\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.generated_uid = self.__generate_uid()\npath = storage_path(config.test_storage)\nself.path = os.path.join(path, self.generated_uid)\ndef __generate_uid(self):\n\"\"\"A helper that generates a unique hash for a test report.\"\"\"\nparams = self.todict()\ndel params[\"results\"]\nparams = str(params)\nreturn hashlib.sha256(params.encode()).hexdigest()\ndef set_results(self, results):\nself.results = results\n@classmethod\ndef all(\ncls, local_only: bool = False, mine_only: bool = False\n) -&gt; List[\"TestReport\"]:\n\"\"\"Gets and creates instances of test reports.\n        Arguments are only specified for compatibility with\n        `Entity.List` and `Entity.View`, but they don't contribute to\n        the logic.\n        Returns:\n            List[TestReport]: List containing all test reports\n        \"\"\"\nlogging.info(\"Retrieving all reports\")\nreports = []\ntest_storage = storage_path(config.test_storage)\ntry:\nuids = next(os.walk(test_storage))[1]\nexcept StopIteration:\nmsg = \"Couldn't iterate over the tests directory\"\nlogging.warning(msg)\nraise RuntimeError(msg)\nfor uid in uids:\nlocal_meta = cls.__get_local_dict(uid)\nreport = cls(**local_meta)\nreports.append(report)\nreturn reports\n@classmethod\ndef get(cls, report_uid: str) -&gt; \"TestReport\":\n\"\"\"Retrieves and creates a TestReport instance obtained the user's machine\n        Args:\n            report_uid (str): UID of the TestReport instance\n        Returns:\n            TestReport: Specified TestReport instance\n        \"\"\"\nlogging.debug(f\"Retrieving report {report_uid}\")\nreport_dict = cls.__get_local_dict(report_uid)\nreport = cls(**report_dict)\nreport.write()\nreturn report\ndef todict(self):\nreturn self.extended_dict()\ndef write(self):\nreport_file = os.path.join(self.path, config.test_report_file)\nos.makedirs(self.path, exist_ok=True)\nwith open(report_file, \"w\") as f:\nyaml.dump(self.todict(), f)\nreturn report_file\n@classmethod\ndef __get_local_dict(cls, local_uid):\nreport_path = os.path.join(storage_path(config.test_storage), str(local_uid))\nreport_file = os.path.join(report_path, config.test_report_file)\nif not os.path.exists(report_file):\nraise InvalidArgumentError(\nf\"The requested report {local_uid} could not be retrieved\"\n)\nwith open(report_file, \"r\") as f:\nreport_info = yaml.safe_load(f)\nreturn report_info\ndef display_dict(self):\nif self.data_path:\ndata_source = f\"{self.data_path}\"[:27] + \"...\"\nelif self.demo_dataset_url:\ndata_source = f\"{self.demo_dataset_url}\"[:27] + \"...\"\nelse:\ndata_source = f\"{self.prepared_data_hash}\"\nreturn {\n\"UID\": self.generated_uid,\n\"Data Source\": data_source,\n\"Model\": self.model\nif isinstance(self.model, int)\nelse self.model[:27] + \"...\",\n\"Evaluator\": self.data_evaluator_mlcube\nif isinstance(self.data_evaluator_mlcube, int)\nelse self.data_evaluator_mlcube[:27] + \"...\",\n}\n</code></pre>"},{"location":"reference/entities/report/#entities.report.TestReport.__generate_uid","title":"<code>__generate_uid()</code>","text":"<p>A helper that generates a unique hash for a test report.</p> Source code in <code>cli/medperf/entities/report.py</code> <pre><code>def __generate_uid(self):\n\"\"\"A helper that generates a unique hash for a test report.\"\"\"\nparams = self.todict()\ndel params[\"results\"]\nparams = str(params)\nreturn hashlib.sha256(params.encode()).hexdigest()\n</code></pre>"},{"location":"reference/entities/report/#entities.report.TestReport.all","title":"<code>all(local_only=False, mine_only=False)</code>  <code>classmethod</code>","text":"<p>Gets and creates instances of test reports. Arguments are only specified for compatibility with <code>Entity.List</code> and <code>Entity.View</code>, but they don't contribute to the logic.</p> <p>Returns:</p> Type Description <code>List[TestReport]</code> <p>List[TestReport]: List containing all test reports</p> Source code in <code>cli/medperf/entities/report.py</code> <pre><code>@classmethod\ndef all(\ncls, local_only: bool = False, mine_only: bool = False\n) -&gt; List[\"TestReport\"]:\n\"\"\"Gets and creates instances of test reports.\n    Arguments are only specified for compatibility with\n    `Entity.List` and `Entity.View`, but they don't contribute to\n    the logic.\n    Returns:\n        List[TestReport]: List containing all test reports\n    \"\"\"\nlogging.info(\"Retrieving all reports\")\nreports = []\ntest_storage = storage_path(config.test_storage)\ntry:\nuids = next(os.walk(test_storage))[1]\nexcept StopIteration:\nmsg = \"Couldn't iterate over the tests directory\"\nlogging.warning(msg)\nraise RuntimeError(msg)\nfor uid in uids:\nlocal_meta = cls.__get_local_dict(uid)\nreport = cls(**local_meta)\nreports.append(report)\nreturn reports\n</code></pre>"},{"location":"reference/entities/report/#entities.report.TestReport.get","title":"<code>get(report_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a TestReport instance obtained the user's machine</p> <p>Parameters:</p> Name Type Description Default <code>report_uid</code> <code>str</code> <p>UID of the TestReport instance</p> required <p>Returns:</p> Name Type Description <code>TestReport</code> <code>TestReport</code> <p>Specified TestReport instance</p> Source code in <code>cli/medperf/entities/report.py</code> <pre><code>@classmethod\ndef get(cls, report_uid: str) -&gt; \"TestReport\":\n\"\"\"Retrieves and creates a TestReport instance obtained the user's machine\n    Args:\n        report_uid (str): UID of the TestReport instance\n    Returns:\n        TestReport: Specified TestReport instance\n    \"\"\"\nlogging.debug(f\"Retrieving report {report_uid}\")\nreport_dict = cls.__get_local_dict(report_uid)\nreport = cls(**report_dict)\nreport.write()\nreturn report\n</code></pre>"},{"location":"reference/entities/result/","title":"Result","text":""},{"location":"reference/entities/result/#entities.result.Result","title":"<code>Result</code>","text":"<p>             Bases: <code>Entity</code>, <code>Uploadable</code>, <code>MedperfSchema</code>, <code>ApprovableSchema</code></p> <p>Class representing a Result entry</p> <p>Results are obtained after successfully running a benchmark execution flow. They contain information regarding the components involved in obtaining metrics results, as well as the results themselves. This class provides methods for working with benchmark results and how to upload them to the backend.</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>class Result(Entity, Uploadable, MedperfSchema, ApprovableSchema):\n\"\"\"\n    Class representing a Result entry\n    Results are obtained after successfully running a benchmark\n    execution flow. They contain information regarding the\n    components involved in obtaining metrics results, as well as the\n    results themselves. This class provides methods for working with\n    benchmark results and how to upload them to the backend.\n    \"\"\"\nbenchmark: int\nmodel: int\ndataset: int\nresults: dict\nmetadata: dict = {}\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a new result instance\"\"\"\nsuper().__init__(*args, **kwargs)\nself.generated_uid = f\"b{self.benchmark}m{self.model}d{self.dataset}\"\npath = storage_path(config.results_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\nself.path = path\n@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Result\"]:\n\"\"\"Gets and creates instances of all the user's results\n        Args:\n            local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n            filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n        Returns:\n            List[Result]: List containing all results\n        \"\"\"\nlogging.info(\"Retrieving all results\")\nresults = []\nif not local_only:\nresults = cls.__remote_all(filters=filters)\nremote_uids = set([result.id for result in results])\nlocal_results = cls.__local_all()\nresults += [res for res in local_results if res.id not in remote_uids]\nreturn results\n@classmethod\ndef __remote_all(cls, filters: dict) -&gt; List[\"Result\"]:\nresults = []\ntry:\ncomms_fn = cls.__remote_prefilter(filters)\nresults_meta = comms_fn()\nresults = [cls(**meta) for meta in results_meta]\nexcept CommunicationRetrievalError:\nmsg = \"Couldn't retrieve all results from the server\"\nlogging.warning(msg)\nreturn results\n@classmethod\ndef __remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_results\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_results\nif \"benchmark\" in filters and filters[\"benchmark\"] is not None:\nbmk = filters[\"benchmark\"]\ndef get_benchmark_results():\n# Decorate the benchmark results remote function so it has the same signature\n# as all the comms_fns\nreturn config.comms.get_benchmark_results(bmk)\ncomms_fn = get_benchmark_results\nreturn comms_fn\n@classmethod\ndef __local_all(cls) -&gt; List[\"Result\"]:\nresults = []\nresults_storage = storage_path(config.results_storage)\ntry:\nuids = next(os.walk(results_storage))[1]\nexcept StopIteration:\nmsg = \"Couldn't iterate over the dataset directory\"\nlogging.warning(msg)\nraise RuntimeError(msg)\nfor uid in uids:\nlocal_meta = cls.__get_local_dict(uid)\nresult = cls(**local_meta)\nresults.append(result)\nreturn results\n@classmethod\ndef get(cls, result_uid: Union[str, int], local_only: bool = False) -&gt; \"Result\":\n\"\"\"Retrieves and creates a Result instance obtained from the platform.\n        If the result instance already exists in the user's machine, it loads\n        the local instance\n        Args:\n            result_uid (str): UID of the Result instance\n        Returns:\n            Result: Specified Result instance\n        \"\"\"\nif not str(result_uid).isdigit() or local_only:\nreturn cls.__local_get(result_uid)\ntry:\nreturn cls.__remote_get(result_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting Result {result_uid} from comms failed\")\nlogging.info(f\"Looking for result {result_uid} locally\")\nreturn cls.__local_get(result_uid)\n@classmethod\ndef __remote_get(cls, result_uid: int) -&gt; \"Result\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            result_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nlogging.debug(f\"Retrieving result {result_uid} remotely\")\nmeta = config.comms.get_result(result_uid)\nresult = cls(**meta)\nresult.write()\nreturn result\n@classmethod\ndef __local_get(cls, result_uid: Union[str, int]) -&gt; \"Result\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n        If the dataset is present in the user's machine then it retrieves it from there.\n        Args:\n            result_uid (str): server UID of the dataset\n        Returns:\n            Dataset: Specified Dataset Instance\n        \"\"\"\nlogging.debug(f\"Retrieving result {result_uid} locally\")\nlocal_meta = cls.__get_local_dict(result_uid)\nresult = cls(**local_meta)\nreturn result\ndef todict(self):\nreturn self.extended_dict()\ndef upload(self):\n\"\"\"Uploads the results to the comms\n        Args:\n            comms (Comms): Instance of the communications interface.\n        \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test results.\")\nresults_info = self.todict()\nupdated_results_info = config.comms.upload_result(results_info)\nreturn updated_results_info\ndef write(self):\nresult_file = os.path.join(self.path, config.results_info_file)\nos.makedirs(self.path, exist_ok=True)\nwith open(result_file, \"w\") as f:\nyaml.dump(self.todict(), f)\nreturn result_file\n@classmethod\ndef __get_local_dict(cls, local_uid):\nresult_path = os.path.join(storage_path(config.results_storage), str(local_uid))\nresult_file = os.path.join(result_path, config.results_info_file)\nif not os.path.exists(result_file):\nraise InvalidArgumentError(\nf\"The requested result {local_uid} could not be retrieved\"\n)\nwith open(result_file, \"r\") as f:\nresults_info = yaml.safe_load(f)\nreturn results_info\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Benchmark\": self.benchmark,\n\"Model\": self.model,\n\"Dataset\": self.dataset,\n\"Partial\": self.metadata[\"partial\"],\n\"Approval Status\": self.approval_status,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a new result instance</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a new result instance\"\"\"\nsuper().__init__(*args, **kwargs)\nself.generated_uid = f\"b{self.benchmark}m{self.model}d{self.dataset}\"\npath = storage_path(config.results_storage)\nif self.id:\npath = os.path.join(path, str(self.id))\nelse:\npath = os.path.join(path, self.generated_uid)\nself.path = path\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.__local_get","title":"<code>__local_get(result_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>result_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Result</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>@classmethod\ndef __local_get(cls, result_uid: Union[str, int]) -&gt; \"Result\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        result_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nlogging.debug(f\"Retrieving result {result_uid} locally\")\nlocal_meta = cls.__get_local_dict(result_uid)\nresult = cls(**local_meta)\nreturn result\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.__remote_get","title":"<code>__remote_get(result_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>result_uid</code> <code>str</code> <p>server UID of the dataset</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Result</code> <p>Specified Dataset Instance</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>@classmethod\ndef __remote_get(cls, result_uid: int) -&gt; \"Result\":\n\"\"\"Retrieves and creates a Dataset instance from the comms instance.\n    If the dataset is present in the user's machine then it retrieves it from there.\n    Args:\n        result_uid (str): server UID of the dataset\n    Returns:\n        Dataset: Specified Dataset Instance\n    \"\"\"\nlogging.debug(f\"Retrieving result {result_uid} remotely\")\nmeta = config.comms.get_result(result_uid)\nresult = cls(**meta)\nresult.write()\nreturn result\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.__remote_prefilter","title":"<code>__remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>@classmethod\ndef __remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_results\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_results\nif \"benchmark\" in filters and filters[\"benchmark\"] is not None:\nbmk = filters[\"benchmark\"]\ndef get_benchmark_results():\n# Decorate the benchmark results remote function so it has the same signature\n# as all the comms_fns\nreturn config.comms.get_benchmark_results(bmk)\ncomms_fn = get_benchmark_results\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.all","title":"<code>all(local_only=False, filters={})</code>  <code>classmethod</code>","text":"<p>Gets and creates instances of all the user's results</p> <p>Parameters:</p> Name Type Description Default <code>local_only</code> <code>bool</code> <p>Wether to retrieve only local entities. Defaults to False.</p> <code>False</code> <code>filters</code> <code>dict</code> <p>key-value pairs specifying filters to apply to the list of entities.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Result]</code> <p>List[Result]: List containing all results</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>@classmethod\ndef all(cls, local_only: bool = False, filters: dict = {}) -&gt; List[\"Result\"]:\n\"\"\"Gets and creates instances of all the user's results\n    Args:\n        local_only (bool, optional): Wether to retrieve only local entities. Defaults to False.\n        filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n    Returns:\n        List[Result]: List containing all results\n    \"\"\"\nlogging.info(\"Retrieving all results\")\nresults = []\nif not local_only:\nresults = cls.__remote_all(filters=filters)\nremote_uids = set([result.id for result in results])\nlocal_results = cls.__local_all()\nresults += [res for res in local_results if res.id not in remote_uids]\nreturn results\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.get","title":"<code>get(result_uid, local_only=False)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance</p> <p>Parameters:</p> Name Type Description Default <code>result_uid</code> <code>str</code> <p>UID of the Result instance</p> required <p>Returns:</p> Name Type Description <code>Result</code> <code>Result</code> <p>Specified Result instance</p> Source code in <code>cli/medperf/entities/result.py</code> <pre><code>@classmethod\ndef get(cls, result_uid: Union[str, int], local_only: bool = False) -&gt; \"Result\":\n\"\"\"Retrieves and creates a Result instance obtained from the platform.\n    If the result instance already exists in the user's machine, it loads\n    the local instance\n    Args:\n        result_uid (str): UID of the Result instance\n    Returns:\n        Result: Specified Result instance\n    \"\"\"\nif not str(result_uid).isdigit() or local_only:\nreturn cls.__local_get(result_uid)\ntry:\nreturn cls.__remote_get(result_uid)\nexcept CommunicationRetrievalError:\nlogging.warning(f\"Getting Result {result_uid} from comms failed\")\nlogging.info(f\"Looking for result {result_uid} locally\")\nreturn cls.__local_get(result_uid)\n</code></pre>"},{"location":"reference/entities/result/#entities.result.Result.upload","title":"<code>upload()</code>","text":"<p>Uploads the results to the comms</p> <p>Parameters:</p> Name Type Description Default <code>comms</code> <code>Comms</code> <p>Instance of the communications interface.</p> required Source code in <code>cli/medperf/entities/result.py</code> <pre><code>def upload(self):\n\"\"\"Uploads the results to the comms\n    Args:\n        comms (Comms): Instance of the communications interface.\n    \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\"Cannot upload test results.\")\nresults_info = self.todict()\nupdated_results_info = config.comms.upload_result(results_info)\nreturn updated_results_info\n</code></pre>"},{"location":"reference/entities/schemas/","title":"Schemas","text":""},{"location":"reference/entities/schemas/#entities.schemas.MedperfBaseSchema","title":"<code>MedperfBaseSchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>class MedperfBaseSchema(BaseModel):\ndef __init__(self, *args, **kwargs):\n\"\"\"Override the ValidationError procedure so we can\n        format the error message in our desired way\n        \"\"\"\ntry:\nsuper().__init__(*args, **kwargs)\nexcept ValidationError as e:\nerrors_dict = defaultdict(list)\nfor error in e.errors():\nfield = error[\"loc\"]\nmsg = error[\"msg\"]\nerrors_dict[field].append(msg)\nerror_msg = \"Field Validation Error:\"\nerror_msg += format_errors_dict(errors_dict)\nraise MedperfException(error_msg)\ndef dict(self, *args, **kwargs) -&gt; dict:\n\"\"\"Overrides dictionary implementation so it filters out\n        fields not defined in the pydantic model\n        Returns:\n            dict: filtered dictionary\n        \"\"\"\nfields = self.__fields__\nvalid_fields = []\n# Gather all the field names, both original an alias names\nfor field_name, field_item in fields.items():\nvalid_fields.append(field_name)\nvalid_fields.append(field_item.alias)\n# Remove duplicates\nvalid_fields = set(valid_fields)\nmodel_dict = super().dict(*args, **kwargs)\nout_dict = {k: v for k, v in model_dict.items() if k in valid_fields}\nreturn out_dict\ndef extended_dict(self) -&gt; dict:\n\"\"\"Dictionary containing both original and alias fields\n        Returns:\n            dict: Extended dictionary representation\n        \"\"\"\nog_dict = self.dict()\nalias_dict = self.dict(by_alias=True)\nog_dict.update(alias_dict)\nfor k, v in og_dict.items():\nif v is None:\nog_dict[k] = \"\"\nif isinstance(v, HttpUrl):\nog_dict[k] = str(v)\nreturn og_dict\n@validator(\"*\", pre=True)\ndef empty_str_to_none(cls, v):\nif v == \"\":\nreturn None\nreturn v\nclass Config:\nallow_population_by_field_name = True\nextra = \"allow\"\nuse_enum_values = True\n</code></pre>"},{"location":"reference/entities/schemas/#entities.schemas.MedperfBaseSchema.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Override the ValidationError procedure so we can format the error message in our desired way</p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Override the ValidationError procedure so we can\n    format the error message in our desired way\n    \"\"\"\ntry:\nsuper().__init__(*args, **kwargs)\nexcept ValidationError as e:\nerrors_dict = defaultdict(list)\nfor error in e.errors():\nfield = error[\"loc\"]\nmsg = error[\"msg\"]\nerrors_dict[field].append(msg)\nerror_msg = \"Field Validation Error:\"\nerror_msg += format_errors_dict(errors_dict)\nraise MedperfException(error_msg)\n</code></pre>"},{"location":"reference/entities/schemas/#entities.schemas.MedperfBaseSchema.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Overrides dictionary implementation so it filters out fields not defined in the pydantic model</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>filtered dictionary</p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; dict:\n\"\"\"Overrides dictionary implementation so it filters out\n    fields not defined in the pydantic model\n    Returns:\n        dict: filtered dictionary\n    \"\"\"\nfields = self.__fields__\nvalid_fields = []\n# Gather all the field names, both original an alias names\nfor field_name, field_item in fields.items():\nvalid_fields.append(field_name)\nvalid_fields.append(field_item.alias)\n# Remove duplicates\nvalid_fields = set(valid_fields)\nmodel_dict = super().dict(*args, **kwargs)\nout_dict = {k: v for k, v in model_dict.items() if k in valid_fields}\nreturn out_dict\n</code></pre>"},{"location":"reference/entities/schemas/#entities.schemas.MedperfBaseSchema.extended_dict","title":"<code>extended_dict()</code>","text":"<p>Dictionary containing both original and alias fields</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Extended dictionary representation</p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>def extended_dict(self) -&gt; dict:\n\"\"\"Dictionary containing both original and alias fields\n    Returns:\n        dict: Extended dictionary representation\n    \"\"\"\nog_dict = self.dict()\nalias_dict = self.dict(by_alias=True)\nog_dict.update(alias_dict)\nfor k, v in og_dict.items():\nif v is None:\nog_dict[k] = \"\"\nif isinstance(v, HttpUrl):\nog_dict[k] = str(v)\nreturn og_dict\n</code></pre>"},{"location":"reference/ui/cli/","title":"Cli","text":""},{"location":"reference/ui/cli/#ui.cli.CLI","title":"<code>CLI</code>","text":"<p>             Bases: <code>UI</code></p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>class CLI(UI):\ndef __init__(self):\nself.spinner = yaspin(color=\"green\")\nself.is_interactive = False\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message on the command line\n        Args:\n            msg (str): message to print\n        \"\"\"\nself.__print(msg)\ndef print_error(self, msg: str):\n\"\"\"Display an error message on the command line\n        Args:\n            msg (str): error message to display\n        \"\"\"\nmsg = f\"\u274c {msg}\"\nmsg = typer.style(msg, fg=typer.colors.RED, bold=True)\nself.__print(msg)\ndef print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\n        Args:\n            msg (str): warning message to display\n        \"\"\"\nmsg = typer.style(msg, fg=typer.colors.YELLOW, bold=True)\nself.__print(msg)\ndef __print(self, msg: str = \"\"):\nif self.is_interactive:\nself.spinner.write(msg)\nelse:\ntyper.echo(msg)\ndef start_interactive(self):\n\"\"\"Start an interactive session where messages can be overwritten\n        and animations can be displayed\"\"\"\nself.is_interactive = True\nself.spinner.start()\ndef stop_interactive(self):\n\"\"\"Stop an interactive session\"\"\"\nself.is_interactive = False\nself.spinner.stop()\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session.\n        Yields:\n            CLI: Yields the current CLI instance with an interactive session initialized\n        \"\"\"\nself.start_interactive()\ntry:\nyield self\nfinally:\nself.stop_interactive()\n@property\ndef text(self):\nreturn self.spinner.text\n@text.setter\ndef text(self, msg: str = \"\"):\n\"\"\"Displays a message that overwrites previous messages if they\n        were created during an interactive ui session.\n        If not on interactive session already, then it calls the ui print function\n        Args:\n            msg (str): message to display\n        \"\"\"\nif not self.is_interactive:\nself.print(msg)\nself.spinner.text = msg\ndef prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\nreturn input(msg)\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\nreturn getpass(msg)\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a highlighted message\n        Args:\n            msg (str): message to print\n        \"\"\"\nmsg = typer.style(msg, fg=typer.colors.GREEN)\nself.__print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.hidden_prompt","title":"<code>hidden_prompt(msg)</code>","text":"<p>Displays a prompt to the user and waits for an aswer. User input is not displayed</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\nreturn getpass(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.interactive","title":"<code>interactive()</code>","text":"<p>Context managed interactive session.</p> <p>Yields:</p> Name Type Description <code>CLI</code> <p>Yields the current CLI instance with an interactive session initialized</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session.\n    Yields:\n        CLI: Yields the current CLI instance with an interactive session initialized\n    \"\"\"\nself.start_interactive()\ntry:\nyield self\nfinally:\nself.stop_interactive()\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print","title":"<code>print(msg='')</code>","text":"<p>Display a message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print(self, msg: str = \"\"):\n\"\"\"Display a message on the command line\n    Args:\n        msg (str): message to print\n    \"\"\"\nself.__print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_error","title":"<code>print_error(msg)</code>","text":"<p>Display an error message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>error message to display</p> required Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_error(self, msg: str):\n\"\"\"Display an error message on the command line\n    Args:\n        msg (str): error message to display\n    \"\"\"\nmsg = f\"\u274c {msg}\"\nmsg = typer.style(msg, fg=typer.colors.RED, bold=True)\nself.__print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_highlight","title":"<code>print_highlight(msg='')</code>","text":"<p>Display a highlighted message</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_highlight(self, msg: str = \"\"):\n\"\"\"Display a highlighted message\n    Args:\n        msg (str): message to print\n    \"\"\"\nmsg = typer.style(msg, fg=typer.colors.GREEN)\nself.__print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_warning","title":"<code>print_warning(msg)</code>","text":"<p>Display a warning message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>warning message to display</p> required Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\n    Args:\n        msg (str): warning message to display\n    \"\"\"\nmsg = typer.style(msg, fg=typer.colors.YELLOW, bold=True)\nself.__print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.prompt","title":"<code>prompt(msg)</code>","text":"<p>Displays a prompt to the user and waits for an answer</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\nreturn input(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.start_interactive","title":"<code>start_interactive()</code>","text":"<p>Start an interactive session where messages can be overwritten and animations can be displayed</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def start_interactive(self):\n\"\"\"Start an interactive session where messages can be overwritten\n    and animations can be displayed\"\"\"\nself.is_interactive = True\nself.spinner.start()\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.stop_interactive","title":"<code>stop_interactive()</code>","text":"<p>Stop an interactive session</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def stop_interactive(self):\n\"\"\"Stop an interactive session\"\"\"\nself.is_interactive = False\nself.spinner.stop()\n</code></pre>"},{"location":"reference/ui/factory/","title":"Factory","text":""},{"location":"reference/ui/interface/","title":"Interface","text":""},{"location":"reference/ui/interface/#ui.interface.UI","title":"<code>UI</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>class UI(ABC):\n@abstractmethod\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message to the interface. If on interactive session overrides\n        previous message\n        \"\"\"\n@abstractmethod\ndef print_error(self, msg: str):\n\"\"\"Display an error message to the interface\"\"\"\ndef print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\"\"\"\n@abstractmethod\ndef start_interactive(self):\n\"\"\"Initialize an interactive session for animations or overriding messages.\n        If the UI doesn't support this, the function can be left empty.\n        \"\"\"\n@abstractmethod\ndef stop_interactive(self):\n\"\"\"Terminate an interactive session.\n        If the UI doesn't support this, the function can be left empty.\n        \"\"\"\n@abstractmethod\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session. Expected to yield the same instance\"\"\"\n@abstractmethod\ndef text(self, msg: str):\n\"\"\"Displays a messages that overwrites previous messages if they were created\n        during an interactive session.\n        If not supported or not on an interactive session, it is expected to fallback\n        to the UI print function.\n        Args:\n            msg (str): message to display\n        \"\"\"\n@abstractmethod\ndef prompt(msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\"\"\"\n@abstractmethod\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\n@abstractmethod\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a message on the command line with green color\n        Args:\n            msg (str): message to print\n        \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.hidden_prompt","title":"<code>hidden_prompt(msg)</code>  <code>abstractmethod</code>","text":"<p>Displays a prompt to the user and waits for an aswer. User input is not displayed</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.interactive","title":"<code>interactive()</code>  <code>abstractmethod</code>","text":"<p>Context managed interactive session. Expected to yield the same instance</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session. Expected to yield the same instance\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print","title":"<code>print(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a message to the interface. If on interactive session overrides previous message</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message to the interface. If on interactive session overrides\n    previous message\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_error","title":"<code>print_error(msg)</code>  <code>abstractmethod</code>","text":"<p>Display an error message to the interface</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_error(self, msg: str):\n\"\"\"Display an error message to the interface\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_highlight","title":"<code>print_highlight(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a message on the command line with green color</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a message on the command line with green color\n    Args:\n        msg (str): message to print\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_warning","title":"<code>print_warning(msg)</code>","text":"<p>Display a warning message on the command line</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>def print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.prompt","title":"<code>prompt(msg)</code>  <code>abstractmethod</code>","text":"<p>Displays a prompt to the user and waits for an answer</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef prompt(msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.start_interactive","title":"<code>start_interactive()</code>  <code>abstractmethod</code>","text":"<p>Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty.</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef start_interactive(self):\n\"\"\"Initialize an interactive session for animations or overriding messages.\n    If the UI doesn't support this, the function can be left empty.\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.stop_interactive","title":"<code>stop_interactive()</code>  <code>abstractmethod</code>","text":"<p>Terminate an interactive session. If the UI doesn't support this, the function can be left empty.</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef stop_interactive(self):\n\"\"\"Terminate an interactive session.\n    If the UI doesn't support this, the function can be left empty.\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.text","title":"<code>text(msg)</code>  <code>abstractmethod</code>","text":"<p>Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to display</p> required Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef text(self, msg: str):\n\"\"\"Displays a messages that overwrites previous messages if they were created\n    during an interactive session.\n    If not supported or not on an interactive session, it is expected to fallback\n    to the UI print function.\n    Args:\n        msg (str): message to display\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/stdin/","title":"Stdin","text":""},{"location":"reference/ui/stdin/#ui.stdin.StdIn","title":"<code>StdIn</code>","text":"<p>             Bases: <code>UI</code></p> <p>Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating execution with class-like objects. Using only basic IO methods ensures that piping from the command-line. Should not be used in normal execution, as hidden prompts and interactive prints will not work as expected.</p> Source code in <code>cli/medperf/ui/stdin.py</code> <pre><code>class StdIn(UI):\n\"\"\"\n    Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating\n    execution with class-like objects. Using only basic IO methods ensures that\n    piping from the command-line. Should not be used in normal execution, as\n    hidden prompts and interactive prints will not work as expected.\n    \"\"\"\ndef print(self, msg: str = \"\"):\nreturn print(msg)\ndef print_error(self, msg: str):\nreturn self.print(msg)\ndef start_interactive(self):\npass\ndef stop_interactive(self):\npass\n@contextmanager\ndef interactive(self):\nyield self\n@property\ndef text(self):\nreturn \"\"\n@text.setter\ndef text(self, msg: str = \"\"):\nreturn\ndef prompt(self, msg: str) -&gt; str:\nreturn input(msg)\ndef hidden_prompt(self, msg: str) -&gt; str:\nreturn self.prompt(msg)\n</code></pre>"}]}